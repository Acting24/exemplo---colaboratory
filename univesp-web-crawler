{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMrKw8dCQVxYbNZXz5cJ8mR"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "titulo_principal"
   },
   "source": [
    "# ğŸ•·ï¸ Web Crawler da Univesp com AnÃ¡lise de FrequÃªncia de Palavras\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/SEU_USUARIO/SEU_REPOSITORIO)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## ğŸ“‹ DescriÃ§Ã£o do Projeto\n",
    "\n",
    "Este notebook implementa um **web crawler** que visita sistematicamente as pÃ¡ginas da Univesp, seguindo hyperlinks e analisando a frequÃªncia de palavras encontradas. O projeto foi desenvolvido como parte do **Desafio Semana 4** e demonstra tÃ©cnicas de:\n",
    "\n",
    "- ğŸŒ **Web Scraping** e crawling sistemÃ¡tico\n",
    "- ğŸ“Š **AnÃ¡lise de texto** e processamento de linguagem natural\n",
    "- ğŸ“ˆ **VisualizaÃ§Ã£o de dados** com grÃ¡ficos interativos\n",
    "- ğŸ **ProgramaÃ§Ã£o Python** com bibliotecas especializadas\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Objetivos\n",
    "1. Implementar um crawler que navega automaticamente pelo site da Univesp\n",
    "2. Extrair e limpar texto de pÃ¡ginas HTML\n",
    "3. Analisar frequÃªncia de palavras com filtros inteligentes\n",
    "4. Gerar visualizaÃ§Ãµes e relatÃ³rios detalhados\n",
    "5. Criar uma ferramenta reutilizÃ¡vel e bem documentada\n",
    "\n",
    "### ğŸš€ Como Usar Este Notebook\n",
    "1. **Execute as cÃ©lulas sequencialmente** usando `Shift + Enter`\n",
    "2. **Ajuste os parÃ¢metros** na seÃ§Ã£o de configuraÃ§Ã£o conforme necessÃ¡rio\n",
    "3. **Visualize os resultados** nas seÃ§Ãµes de anÃ¡lise e grÃ¡ficos\n",
    "4. **Baixe os dados** gerados na seÃ§Ã£o final\n",
    "\n",
    "### ğŸ“– Estrutura do Notebook\n",
    "- **SeÃ§Ã£o 1:** ConfiguraÃ§Ã£o e InstalaÃ§Ã£o de DependÃªncias\n",
    "- **SeÃ§Ã£o 2:** ImplementaÃ§Ã£o do Web Crawler\n",
    "- **SeÃ§Ã£o 3:** Processamento e AnÃ¡lise de Texto\n",
    "- **SeÃ§Ã£o 4:** VisualizaÃ§Ãµes e GrÃ¡ficos\n",
    "- **SeÃ§Ã£o 5:** RelatÃ³rios e ExportaÃ§Ã£o de Dados\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Desenvolvido para o Desafio Semana 4 - Univesp  \n",
    "**Data:** 2025  \n",
    "**VersÃ£o:** 1.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_instalacao"
   },
   "source": [
    "# 1ï¸âƒ£ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o de DependÃªncias\n",
    "\n",
    "Nesta seÃ§Ã£o, vamos instalar e importar todas as bibliotecas necessÃ¡rias para o funcionamento do web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ“¦ InstalaÃ§Ã£o de bibliotecas necessÃ¡rias\n",
    "!pip install requests beautifulsoup4 matplotlib seaborn wordcloud plotly pandas --quiet\n",
    "\n",
    "print(\"âœ… Todas as dependÃªncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# ğŸ“š ImportaÃ§Ã£o de bibliotecas\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "import unicodedata\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas para web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Bibliotecas para anÃ¡lise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualizaÃ§Ã£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ConfiguraÃ§Ãµes visuais\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ… Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"ğŸ Python executando no Google Colab\")\n",
    "print(f\"â° Notebook iniciado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_configuracao"
   },
   "source": [
    "## âš™ï¸ ConfiguraÃ§Ã£o de ParÃ¢metros\n",
    "\n",
    "Aqui vocÃª pode ajustar os parÃ¢metros do crawler conforme suas necessidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuracao_parametros"
   },
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ CONFIGURAÃ‡Ã•ES DO WEB CRAWLER\n",
    "# ================================\n",
    "\n",
    "# URL base da Univesp\n",
    "BASE_URL = \"https://univesp.br\"\n",
    "\n",
    "# NÃºmero mÃ¡ximo de pÃ¡ginas a visitar\n",
    "MAX_PAGES = 20  # Reduzido para o Colab (pode aumentar se necessÃ¡rio)\n",
    "\n",
    "# Delay entre requisiÃ§Ãµes (em segundos)\n",
    "DELAY = 1.0  # Seja respeitoso com o servidor!\n",
    "\n",
    "# Timeout para requisiÃ§Ãµes (em segundos)\n",
    "TIMEOUT = 10\n",
    "\n",
    "# Headers para simular um navegador real\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Lista de stopwords em portuguÃªs\n",
    "STOPWORDS = {\n",
    "    'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'atÃ©', \n",
    "    'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', \n",
    "    'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', \n",
    "    'esse', 'esses', 'esta', 'estÃ£o', 'estas', 'estamos', 'estar', 'este', 'estes', \n",
    "    'eu', 'foi', 'for', 'foram', 'hÃ¡', 'isso', 'isto', 'jÃ¡', 'mais', 'mas', 'me', \n",
    "    'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'nÃ£o', 'no', 'nos', \n",
    "    'nÃ³s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', \n",
    "    'qual', 'quando', 'que', 'quem', 'sÃ£o', 'se', 'sem', 'ser', 'seu', 'seus', \n",
    "    'sÃ³', 'sua', 'suas', 'tambÃ©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', \n",
    "    'tuas', 'um', 'uma', 'vocÃª', 'vocÃªs', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',\n",
    "    'pode', 'podem', 'vai', 'vÃ£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',\n",
    "    'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'lÃ¡', 'agora', 'entÃ£o',\n",
    "    'sobre', 'apÃ³s', 'durante', 'antes', 'depois', 'enquanto', 'desde', 'serÃ¡',\n",
    "    'serÃ£o', 'estÃ¡', 'estÃ£o', 'foi', 'foram', 'terÃ¡', 'terÃ£o', 'atÃ©', 'atravÃ©s',\n",
    "    'alÃ©m', 'tambÃ©m', 'porÃ©m', 'contudo', 'todavia', 'entretanto', 'portanto',\n",
    "    'assim', 'entÃ£o', 'logo', 'pois', 'porque', 'porquÃª', 'quando', 'quanto',\n",
    "    'qualquer', 'quaisquer', 'algum', 'alguns', 'alguma', 'algumas', 'nenhum',\n",
    "    'nenhuma', 'outro', 'outra', 'outros', 'outras', 'mesmo', 'mesma', 'mesmos',\n",
    "    'mesmas', 'tanto', 'tanta', 'tantos', 'tantas', 'quanto', 'quanta', 'quantos',\n",
    "    'quantas', 'tal', 'tais', 'cada', 'qualquer', 'seja', 'sejam', 'fosse',\n",
    "    'fossem', 'sendo', 'tendo', 'havendo', 'haver', 'ter', 'fazer', 'dizer',\n",
    "    'dar', 'ficar', 'ir', 'vir', 'sair', 'chegar', 'voltar', 'passar', 'levar'\n",
    "}\n",
    "\n",
    "print(f\"âš™ï¸ ConfiguraÃ§Ãµes do Crawler:\")\n",
    "print(f\"   ğŸŒ URL Base: {BASE_URL}\")\n",
    "print(f\"   ğŸ“„ MÃ¡ximo de pÃ¡ginas: {MAX_PAGES}\")\n",
    "print(f\"   â±ï¸ Delay entre requisiÃ§Ãµes: {DELAY}s\")\n",
    "print(f\"   ğŸš« Stopwords: {len(STOPWORDS)} palavras filtradas\")\n",
    "print(f\"\\nâœ… ConfiguraÃ§Ã£o concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_crawler"
   },
   "source": [
    "# 2ï¸âƒ£ ImplementaÃ§Ã£o do Web Crawler\n",
    "\n",
    "Vamos criar nossa classe principal do web crawler com todas as funcionalidades necessÃ¡rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crawler_class"
   },
   "outputs": [],
   "source": [
    "class UnivespWebCrawler:\n",
    "    \"\"\"\n",
    "    ğŸ•·ï¸ Web Crawler da Univesp com AnÃ¡lise de FrequÃªncia de Palavras\n",
    "    \n",
    "    Esta classe implementa um crawler completo que:\n",
    "    - Visita pÃ¡ginas da Univesp sistematicamente\n",
    "    - Extrai e processa texto de cada pÃ¡gina\n",
    "    - Analisa frequÃªncia de palavras\n",
    "    - Gera estatÃ­sticas detalhadas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=BASE_URL, max_pages=MAX_PAGES, delay=DELAY):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.visited_urls = set()\n",
    "        self.failed_urls = set()\n",
    "        self.word_frequency = Counter()\n",
    "        self.page_word_counts = defaultdict(Counter)\n",
    "        self.crawl_data = []\n",
    "        \n",
    "        # Configurar sessÃ£o HTTP com retry\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Verifica se a URL Ã© vÃ¡lida e pertence ao domÃ­nio da Univesp\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return (parsed.netloc.endswith('univesp.br') and \n",
    "                   parsed.scheme in ['http', 'https'] and\n",
    "                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Faz o download do conteÃºdo de uma pÃ¡gina\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ”— Visitando: {url}\")\n",
    "            response = self.session.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Garantir codificaÃ§Ã£o UTF-8\n",
    "            if response.encoding is None or response.encoding.lower() not in ['utf-8', 'utf8']:\n",
    "                response.encoding = 'utf-8'\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro ao acessar {url}: {e}\")\n",
    "            self.failed_urls.add(url)\n",
    "            return None\n",
    "    \n",
    "    def extract_text_and_links(self, html_content, base_url):\n",
    "        \"\"\"Extrai texto limpo e links de uma pÃ¡gina HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Remove scripts, estilos e elementos desnecessÃ¡rios\n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extrai texto\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Limpa o texto\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Extrai links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(base_url, link['href'])\n",
    "                if self.is_valid_url(absolute_url):\n",
    "                    links.append(absolute_url)\n",
    "            \n",
    "            return clean_text, list(set(links))  # Remove duplicatas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro ao processar HTML: {e}\")\n",
    "            return \"\", []\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Processa texto para contar palavras\"\"\"\n",
    "        if not text:\n",
    "            return Counter()\n",
    "        \n",
    "        # Normalizar Unicode (resolver problemas de codificaÃ§Ã£o)\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Converte para minÃºsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Encontra palavras (incluindo acentos portugueses)\n",
    "        words = re.findall(r'\\b[a-zA-ZÃ¡Ã Ã¢Ã£Ã¤Ã©Ã¨ÃªÃ«Ã­Ã¬Ã®Ã¯Ã³Ã²Ã´ÃµÃ¶ÃºÃ¹Ã»Ã¼Ã§Ã±ÃÃ€Ã‚ÃƒÃ„Ã‰ÃˆÃŠÃ‹ÃÃŒÃÃÃ“Ã’Ã”Ã•Ã–ÃšÃ™Ã›ÃœÃ‡Ã‘]+\\b', text)\n",
    "        \n",
    "        # Filtra palavras muito curtas e stopwords\n",
    "        filtered_words = [word for word in words \n",
    "                         if len(word) > 2 and word not in STOPWORDS]\n",
    "        \n",
    "        return Counter(filtered_words)\n",
    "    \n",
    "    def crawl_page(self, url):\n",
    "        \"\"\"Processa uma pÃ¡gina especÃ­fica\"\"\"\n",
    "        # Download da pÃ¡gina\n",
    "        html_content = self.fetch_page(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "        \n",
    "        # Extrai texto e links\n",
    "        text, links = self.extract_text_and_links(html_content, url)\n",
    "        \n",
    "        # Processa texto\n",
    "        if text:\n",
    "            word_count = self.process_text(text)\n",
    "            self.page_word_counts[url] = word_count\n",
    "            self.word_frequency.update(word_count)\n",
    "            \n",
    "            # Armazena dados para anÃ¡lise posterior\n",
    "            self.crawl_data.append({\n",
    "                'url': url,\n",
    "                'text_length': len(text),\n",
    "                'word_count': sum(word_count.values()),\n",
    "                'unique_words': len(word_count),\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Executa o processo principal de crawling\"\"\"\n",
    "        print(\"ğŸ•·ï¸ INICIANDO WEB CRAWLER DA UNIVESP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸŒ URL inicial: {self.base_url}\")\n",
    "        print(f\"ğŸ“„ MÃ¡ximo de pÃ¡ginas: {self.max_pages}\")\n",
    "        print(f\"â±ï¸ Delay: {self.delay}s\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        urls_to_visit = [self.base_url]\n",
    "        \n",
    "        while urls_to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            current_url = urls_to_visit.pop(0)\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            self.visited_urls.add(current_url)\n",
    "            \n",
    "            # Processa a pÃ¡gina\n",
    "            new_links = self.crawl_page(current_url)\n",
    "            \n",
    "            # Adiciona novos links\n",
    "            for link in new_links:\n",
    "                if (link not in self.visited_urls and \n",
    "                    link not in urls_to_visit and \n",
    "                    link not in self.failed_urls):\n",
    "                    urls_to_visit.append(link)\n",
    "            \n",
    "            # Progresso\n",
    "            if len(self.visited_urls) % 5 == 0:\n",
    "                print(f\"ğŸ“ˆ Progresso: {len(self.visited_urls)}/{self.max_pages} pÃ¡ginas\")\n",
    "            \n",
    "            # Delay\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\nâœ… Crawling concluÃ­do!\")\n",
    "        print(f\"ğŸ“Š PÃ¡ginas visitadas: {len(self.visited_urls)}\")\n",
    "        print(f\"âŒ PÃ¡ginas com erro: {len(self.failed_urls)}\")\n",
    "        print(f\"â±ï¸ Tempo total: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"ğŸ“ Palavras Ãºnicas encontradas: {len(self.word_frequency)}\")\n",
    "        print(f\"ğŸ”¤ Total de ocorrÃªncias: {sum(self.word_frequency.values())}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Retorna os dados do crawl como DataFrame do pandas\"\"\"\n",
    "        return pd.DataFrame(self.crawl_data)\n",
    "    \n",
    "    def get_word_frequency_df(self, top_n=100):\n",
    "        \"\"\"Retorna as palavras mais frequentes como DataFrame\"\"\"\n",
    "        top_words = self.word_frequency.most_common(top_n)\n",
    "        return pd.DataFrame(top_words, columns=['palavra', 'frequencia'])\n",
    "\n",
    "print(\"âœ… Classe UnivespWebCrawler criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_execucao"
   },
   "source": [
    "## ğŸš€ Executando o Web Crawler\n",
    "\n",
    "Agora vamos criar uma instÃ¢ncia do crawler e executÃ¡-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "executar_crawler",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ•·ï¸ Criar e executar o crawler\n",
    "crawler = UnivespWebCrawler(\n",
    "    base_url=BASE_URL,\n",
    "    max_pages=MAX_PAGES,\n",
    "    delay=DELAY\n",
    ")\n",
    "\n",
    "# Executar o crawling\n",
    "sucesso = crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise"
   },
   "source": [
    "# 3ï¸âƒ£ AnÃ¡lise dos Dados Coletados\n",
    "\n",
    "Agora vamos analisar os dados coletados pelo nosso crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_basica"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š AnÃ¡lise bÃ¡sica dos dados\n",
    "if sucesso:\n",
    "    print(\"ğŸ“ˆ ANÃLISE DOS DADOS COLETADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # EstatÃ­sticas gerais\n",
    "    df_pages = crawler.get_dataframe()\n",
    "    df_words = crawler.get_word_frequency_df(50)\n",
    "    \n",
    "    print(f\"\\nğŸ” EstatÃ­sticas Gerais:\")\n",
    "    print(f\"   ğŸ“„ Total de pÃ¡ginas processadas: {len(df_pages)}\")\n",
    "    print(f\"   ğŸ”¤ Palavras Ãºnicas encontradas: {len(crawler.word_frequency)}\")\n",
    "    print(f\"   ğŸ“Š Total de ocorrÃªncias de palavras: {sum(crawler.word_frequency.values())}\")\n",
    "    print(f\"   ğŸ“ MÃ©dia de palavras por pÃ¡gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"   ğŸ“ MÃ©dia de caracteres por pÃ¡gina: {df_pages['text_length'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† TOP 15 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, (palavra, freq) in enumerate(df_words.head(15).values, 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorrÃªncias\")\n",
    "    \n",
    "    # Criar DataFrames para uso posterior\n",
    "    print(f\"\\nâœ… DataFrames criados:\")\n",
    "    print(f\"   ğŸ“Š df_pages: {len(df_pages)} linhas (dados por pÃ¡gina)\")\n",
    "    print(f\"   ğŸ”¤ df_words: {len(df_words)} linhas (frequÃªncia de palavras)\")\nelse:\n",
    "    print(\"âŒ Erro durante o crawling. Verificar logs acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detalhes_paginas"
   },
   "outputs": [],
   "source": [
    "# ğŸ“‹ Detalhes das pÃ¡ginas coletadas\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸ“„ DETALHES DAS PÃGINAS COLETADAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Exibir as primeiras pÃ¡ginas\n",
    "    display(df_pages.head())\n",
    "    \n",
    "    print(\"\\nğŸ“Š EstatÃ­sticas descritivas:\")\n",
    "    display(df_pages[['text_length', 'word_count', 'unique_words']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_visualizacoes"
   },
   "source": [
    "# 4ï¸âƒ£ VisualizaÃ§Ãµes e GrÃ¡ficos\n",
    "\n",
    "Vamos criar visualizaÃ§Ãµes interessantes dos dados coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wordcloud"
   },
   "outputs": [],
   "source": [
    "# ğŸŒŸ Word Cloud das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"â˜ï¸ Criando Word Cloud...\")\n",
    "    \n",
    "    # Preparar dados para word cloud\n",
    "    word_freq_dict = dict(zip(df_words['palavra'], df_words['frequencia']))\n",
    "    \n",
    "    # Criar word cloud\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, height=600,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        min_font_size=10\n",
    "    ).generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('â˜ï¸ Nuvem de Palavras - Site da Univesp', fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Word Cloud criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grafico_barras"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š GrÃ¡fico de barras das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"ğŸ“Š Criando grÃ¡fico de barras...\")\n",
    "    \n",
    "    # Top 20 palavras\n",
    "    top_20 = df_words.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Criar grÃ¡fico de barras\n",
    "    bars = plt.bar(range(len(top_20)), top_20['frequencia'], \n",
    "                   color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))\n",
    "    \n",
    "    plt.xlabel('Palavras', fontsize=12)\n",
    "    plt.ylabel('FrequÃªncia', fontsize=12)\n",
    "    plt.title('ğŸ“Š Top 20 Palavras Mais Frequentes - Site da Univesp', fontsize=16, pad=20)\n",
    "    plt.xticks(range(len(top_20)), top_20['palavra'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… GrÃ¡fico de barras criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graficos_interativos"
   },
   "outputs": [],
   "source": [
    "# ğŸ“ˆ GrÃ¡ficos interativos com Plotly\n",
    "if not df_words.empty and not df_pages.empty:\n",
    "    print(\"ğŸ¨ Criando grÃ¡ficos interativos...\")\n",
    "    \n",
    "    # 1. GrÃ¡fico interativo de barras das palavras mais frequentes\n",
    "    top_25 = df_words.head(25)\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        top_25, \n",
    "        x='palavra', \n",
    "        y='frequencia',\n",
    "        title='ğŸ† Top 25 Palavras Mais Frequentes (Interativo)',\n",
    "        labels={'palavra': 'Palavra', 'frequencia': 'FrequÃªncia'},\n",
    "        color='frequencia',\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig1.update_layout(xaxis_tickangle=-45, height=600)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. GrÃ¡fico de distribuiÃ§Ã£o de palavras por pÃ¡gina\n",
    "    fig2 = px.scatter(\n",
    "        df_pages, \n",
    "        x='text_length', \n",
    "        y='word_count',\n",
    "        size='unique_words',\n",
    "        hover_data=['url'],\n",
    "        title='ğŸ“Š DistribuiÃ§Ã£o: Tamanho do Texto vs Contagem de Palavras',\n",
    "        labels={\n",
    "            'text_length': 'Tamanho do Texto (caracteres)',\n",
    "            'word_count': 'NÃºmero de Palavras',\n",
    "            'unique_words': 'Palavras Ãšnicas'\n",
    "        }\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. GrÃ¡fico de pizza das categorias de palavras\n",
    "    # Categorizar palavras por frequÃªncia\n",
    "    categories = []\n",
    "    for freq in df_words['frequencia']:\n",
    "        if freq >= 100:\n",
    "            categories.append('Muito Frequente (â‰¥100)')\n",
    "        elif freq >= 50:\n",
    "            categories.append('Frequente (50-99)')\n",
    "        elif freq >= 20:\n",
    "            categories.append('Moderada (20-49)')\n",
    "        elif freq >= 10:\n",
    "            categories.append('Baixa (10-19)')\n",
    "        else:\n",
    "            categories.append('Rara (<10)')\n",
    "    \n",
    "    df_words['categoria'] = categories\n",
    "    category_counts = df_words['categoria'].value_counts()\n",
    "    \n",
    "    fig3 = px.pie(\n",
    "        values=category_counts.values,\n",
    "        names=category_counts.index,\n",
    "        title='ğŸ¥§ DistribuiÃ§Ã£o de Palavras por Categoria de FrequÃªncia'\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"âœ… GrÃ¡ficos interativos criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise_avancada"
   },
   "source": [
    "# 5ï¸âƒ£ AnÃ¡lise AvanÃ§ada e Insights\n",
    "\n",
    "Vamos fazer algumas anÃ¡lises mais detalhadas dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_correlacoes"
   },
   "outputs": [],
   "source": [
    "# ğŸ” AnÃ¡lise de correlaÃ§Ãµes\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸ”— ANÃLISE DE CORRELAÃ‡Ã•ES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Matriz de correlaÃ§Ã£o\n",
    "    correlation_matrix = df_pages[['text_length', 'word_count', 'unique_words']].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True,\n",
    "                fmt='.3f')\n",
    "    plt.title('ğŸ”¥ Matriz de CorrelaÃ§Ã£o entre MÃ©tricas das PÃ¡ginas', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights\n",
    "    print(\"\\nğŸ’¡ INSIGHTS:\")\n",
    "    corr_text_words = correlation_matrix.loc['text_length', 'word_count']\n",
    "    corr_words_unique = correlation_matrix.loc['word_count', 'unique_words']\n",
    "    \n",
    "    print(f\"   ğŸ“ CorrelaÃ§Ã£o Tamanho do Texto â†” NÃºmero de Palavras: {corr_text_words:.3f}\")\n",
    "    print(f\"   ğŸ”¤ CorrelaÃ§Ã£o NÃºmero de Palavras â†” Palavras Ãšnicas: {corr_words_unique:.3f}\")\n",
    "    \n",
    "    if corr_text_words > 0.7:\n",
    "        print(\"   âœ… Forte correlaÃ§Ã£o positiva entre tamanho do texto e nÃºmero de palavras\")\n",
    "    if corr_words_unique > 0.7:\n",
    "        print(\"   âœ… Forte correlaÃ§Ã£o positiva entre nÃºmero total e palavras Ãºnicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_diversidade"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š AnÃ¡lise de diversidade lexical\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸŒˆ ANÃLISE DE DIVERSIDADE LEXICAL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Calcular diversidade lexical (Type-Token Ratio)\n",
    "    df_pages['diversidade_lexical'] = df_pages['unique_words'] / df_pages['word_count']\n",
    "    \n",
    "    # EstatÃ­sticas\n",
    "    diversidade_media = df_pages['diversidade_lexical'].mean()\n",
    "    diversidade_std = df_pages['diversidade_lexical'].std()\n",
    "    \n",
    "    print(f\"ğŸ“Š Diversidade Lexical MÃ©dia: {diversidade_media:.3f}\")\n",
    "    print(f\"ğŸ“ˆ Desvio PadrÃ£o: {diversidade_std:.3f}\")\n",
    "    \n",
    "    # GrÃ¡fico de distribuiÃ§Ã£o\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_pages['diversidade_lexical'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(diversidade_media, color='red', linestyle='--', label=f'MÃ©dia: {diversidade_media:.3f}')\n",
    "    plt.xlabel('Diversidade Lexical (TTR)')\n",
    "    plt.ylabel('FrequÃªncia')\n",
    "    plt.title('ğŸ“Š DistribuiÃ§Ã£o da Diversidade Lexical')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(df_pages['word_count'], df_pages['diversidade_lexical'], alpha=0.6, color='green')\n",
    "    plt.xlabel('NÃºmero de Palavras')\n",
    "    plt.ylabel('Diversidade Lexical')\n",
    "    plt.title('ğŸ“ˆ Diversidade vs NÃºmero de Palavras')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # PÃ¡ginas com maior diversidade\n",
    "    top_diversidade = df_pages.nlargest(3, 'diversidade_lexical')\n",
    "    \n",
    "    print(\"\\nğŸ† TOP 3 PÃGINAS COM MAIOR DIVERSIDADE LEXICAL:\")\n",
    "    for i, (idx, row) in enumerate(top_diversidade.iterrows(), 1):\n",
    "        print(f\"{i}. Diversidade: {row['diversidade_lexical']:.3f}\")\n",
    "        print(f\"   URL: {row['url'][:80]}...\")\n",
    "        print(f\"   Palavras: {row['word_count']} | Ãšnicas: {row['unique_words']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_relatorios"
   },
   "source": [
    "# 6ï¸âƒ£ RelatÃ³rios e ExportaÃ§Ã£o\n",
    "\n",
    "Vamos gerar relatÃ³rios finais e preparar os dados para download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "relatorio_final"
   },
   "outputs": [],
   "source": [
    "# ğŸ“‹ RelatÃ³rio final consolidado\n",
    "def gerar_relatorio_final():\n",
    "    if not crawler.visited_urls:\n",
    "        print(\"âŒ Nenhum dado disponÃ­vel para relatÃ³rio\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“‹ RELATÃ“RIO FINAL - WEB CRAWLER UNIVESP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ•·ï¸ Executado em: {datetime.now().strftime('%d/%m/%Y Ã s %H:%M:%S')}\")\n",
    "    print(f\"ğŸŒ URL Base: {BASE_URL}\")\n",
    "    print()\n",
    "    \n",
    "    # EstatÃ­sticas do Crawling\n",
    "    print(\"ğŸ“Š ESTATÃSTICAS DO CRAWLING:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"âœ… PÃ¡ginas visitadas com sucesso: {len(crawler.visited_urls)}\")\n",
    "    print(f\"âŒ PÃ¡ginas com erro: {len(crawler.failed_urls)}\")\n",
    "    print(f\"ğŸ¯ Taxa de sucesso: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # AnÃ¡lise de Texto\n",
    "    print(\"ğŸ“ ANÃLISE DE TEXTO:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"ğŸ”¤ Total de palavras Ãºnicas: {len(crawler.word_frequency):,}\")\n",
    "    print(f\"ğŸ“Š Total de ocorrÃªncias: {sum(crawler.word_frequency.values()):,}\")\n",
    "    print(f\"ğŸ“„ MÃ©dia de palavras por pÃ¡gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"ğŸŒˆ Diversidade lexical mÃ©dia: {df_pages['diversidade_lexical'].mean():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Top palavras\n",
    "    print(\"ğŸ† TOP 10 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorrÃªncias\")\n",
    "    print()\n",
    "    \n",
    "    # AnÃ¡lise por domÃ­nios temÃ¡ticos\n",
    "    palavras_educacao = ['curso', 'ensino', 'educaÃ§Ã£o', 'aprendizagem', 'estudo', 'aluno', 'professor']\n",
    "    palavras_tecnologia = ['tecnologia', 'computaÃ§Ã£o', 'dados', 'sistema', 'digital', 'informÃ¡tica']\n",
    "    \n",
    "    freq_educacao = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_educacao)\n",
    "    freq_tecnologia = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_tecnologia)\n",
    "    \n",
    "    print(\"ğŸ“ ANÃLISE TEMÃTICA:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"ğŸ“š Palavras relacionadas Ã  EducaÃ§Ã£o: {freq_educacao} ocorrÃªncias\")\n",
    "    print(f\"ğŸ’» Palavras relacionadas Ã  Tecnologia: {freq_tecnologia} ocorrÃªncias\")\n",
    "    \n",
    "    if freq_educacao > freq_tecnologia:\n",
    "        print(\"ğŸ“Š Foco predominante: EducaÃ§Ã£o\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š Foco predominante: Tecnologia\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… RelatÃ³rio gerado com sucesso!\")\n",
    "\n",
    "# Gerar relatÃ³rio\n",
    "gerar_relatorio_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exportar_dados"
   },
   "outputs": [],
   "source": [
    "# ğŸ’¾ Preparar dados para download\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"ğŸ’¾ PREPARANDO DADOS PARA DOWNLOAD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Salvar frequÃªncia de palavras como CSV\n",
    "df_words_full = crawler.get_word_frequency_df(1000)  # Top 1000 palavras\n",
    "df_words_full.to_csv('univesp_frequencia_palavras.csv', index=False, encoding='utf-8')\n",
    "print(\"âœ… Arquivo CSV criado: univesp_frequencia_palavras.csv\")\n",
    "\n",
    "# 2. Salvar dados das pÃ¡ginas como CSV\n",
    "df_pages_export = df_pages.copy()\n",
    "df_pages_export['timestamp'] = df_pages_export['timestamp'].astype(str)\n",
    "df_pages_export.to_csv('univesp_dados_paginas.csv', index=False, encoding='utf-8')\n",
    "print(\"âœ… Arquivo CSV criado: univesp_dados_paginas.csv\")\n",
    "\n",
    "# 3. Salvar relatÃ³rio como JSON\n",
    "relatorio_json = {\n",
    "    'metadata': {\n",
    "        'data_execucao': datetime.now().isoformat(),\n",
    "        'url_base': BASE_URL,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'delay': DELAY\n",
    "    },\n",
    "    'estatisticas': {\n",
    "        'paginas_visitadas': len(crawler.visited_urls),\n",
    "        'paginas_com_erro': len(crawler.failed_urls),\n",
    "        'palavras_unicas': len(crawler.word_frequency),\n",
    "        'total_ocorrencias': sum(crawler.word_frequency.values()),\n",
    "        'diversidade_lexical_media': float(df_pages['diversidade_lexical'].mean()) if 'diversidade_lexical' in df_pages.columns else 0\n",
    "    },\n",
    "    'top_palavras': dict(crawler.word_frequency.most_common(50)),\n",
    "    'urls_visitadas': list(crawler.visited_urls),\n",
    "    'urls_com_erro': list(crawler.failed_urls)\n",
    "}\n",
    "\n",
    "with open('univesp_relatorio_completo.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relatorio_json, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Arquivo JSON criado: univesp_relatorio_completo.json\")\n",
    "\n",
    "# 4. Criar arquivo README para os dados\n",
    "readme_content = f\"\"\"\n",
    "# ğŸ“Š Dados do Web Crawler - Univesp\n",
    "\n",
    "## ğŸ“‹ DescriÃ§Ã£o\n",
    "Dados coletados pelo web crawler da Univesp executado em {datetime.now().strftime('%d/%m/%Y Ã s %H:%M:%S')}.\n",
    "\n",
    "## ğŸ“ Arquivos Inclusos:\n",
    "- **univesp_frequencia_palavras.csv**: Top 1000 palavras mais frequentes\n",
    "- **univesp_dados_paginas.csv**: Dados detalhados de cada pÃ¡gina visitada\n",
    "- **univesp_relatorio_completo.json**: RelatÃ³rio completo em formato JSON\n",
    "- **README.md**: Este arquivo de documentaÃ§Ã£o\n",
    "\n",
    "## ğŸ“Š EstatÃ­sticas:\n",
    "- ğŸŒ **URL Base**: {BASE_URL}\n",
    "- ğŸ“„ **PÃ¡ginas Processadas**: {len(crawler.visited_urls)}\n",
    "- ğŸ”¤ **Palavras Ãšnicas**: {len(crawler.word_frequency):,}\n",
    "- ğŸ“Š **Total de OcorrÃªncias**: {sum(crawler.word_frequency.values()):,}\n",
    "\n",
    "## ğŸ† Top 10 Palavras:\n",
    "\"\"\"\n",
    "\n",
    "for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "    readme_content += f\"{i:2d}. **{palavra}**: {freq} ocorrÃªncias\\n\"\n",
    "\n",
    "readme_content += f\"\"\"\n",
    "\n",
    "## ğŸ› ï¸ Como Usar:\n",
    "1. Abra os arquivos CSV em Excel, Google Sheets ou pandas\n",
    "2. Use o arquivo JSON para anÃ¡lises programÃ¡ticas\n",
    "3. Consulte este README para entender a estrutura dos dados\n",
    "\n",
    "---\n",
    "*Gerado pelo Web Crawler da Univesp - Notebook do Google Colab*\n",
    "\"\"\"\n",
    "\n",
    "with open('README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(\"âœ… Arquivo README criado: README.md\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Todos os arquivos estÃ£o prontos para download!\")\n",
    "print(\"ğŸ’¡ Use o menu Files â†’ Download para baixar os arquivos individualmente\")\n",
    "print(\"ğŸ’¡ Ou execute a cÃ©lula abaixo para download automÃ¡tico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_arquivos"
   },
   "outputs": [],
   "source": [
    "# ğŸ“¥ Download automÃ¡tico dos arquivos (opcional)\n",
    "print(\"ğŸ“¥ Iniciando download dos arquivos...\")\n",
    "print(\"(Os arquivos serÃ£o baixados para sua pasta de Downloads)\")\n",
    "\n",
    "try:\n",
    "    files.download('univesp_frequencia_palavras.csv')\n",
    "    files.download('univesp_dados_paginas.csv')\n",
    "    files.download('univesp_relatorio_completo.json')\n",
    "    files.download('README.md')\n",
    "    print(\"\\nâœ… Download concluÃ­do com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Erro no download automÃ¡tico: {e}\")\n",
    "    print(\"ğŸ’¡ Use o menu Files para download manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_publicacao_github"
   },
   "source": [
    "# ğŸš€ Publicando no GitHub\n",
    "\n",
    "## ğŸ“ Guia Completo para Publicar este Projeto no GitHub\n",
    "\n",
    "### ğŸŒŸ **OpÃ§Ã£o 1: PublicaÃ§Ã£o Direta do Colab**\n",
    "\n",
    "1. **Salvar no GitHub diretamente:**\n",
    "   - No Colab: `File` â†’ `Save a copy in GitHub`\n",
    "   - Escolha seu repositÃ³rio ou crie um novo\n",
    "   - Adicione uma mensagem de commit\n",
    "   - Clique em `OK`\n",
    "\n",
    "### ğŸ› ï¸ **OpÃ§Ã£o 2: Processo Manual Completo**\n",
    "\n",
    "#### **Passo 1: Preparar o RepositÃ³rio**\n",
    "1. Acesse [GitHub.com](https://github.com)\n",
    "2. Clique em `New Repository`\n",
    "3. Nome sugerido: `univesp-web-crawler`\n",
    "4. Adicione descriÃ§Ã£o: `ğŸ•·ï¸ Web Crawler da Univesp com anÃ¡lise de frequÃªncia de palavras`\n",
    "5. Marque `Add a README file`\n",
    "6. Clique `Create repository`\n",
    "\n",
    "#### **Passo 2: Estrutura de Arquivos Recomendada**\n",
    "```\n",
    "univesp-web-crawler/\n",
    "â”œâ”€â”€ Univesp_Web_Crawler_Colab.ipynb    # Este notebook\n",
    "â”œâ”€â”€ README.md                           # DocumentaÃ§Ã£o principal\n",
    "â”œâ”€â”€ data/                              # Dados coletados\n",
    "â”‚   â”œâ”€â”€ univesp_frequencia_palavras.csv\n",
    "â”‚   â”œâ”€â”€ univesp_dados_paginas.csv\n",
    "â”‚   â””â”€â”€ univesp_relatorio_completo.json\n",
    "â”œâ”€â”€ images/                            # Screenshots e grÃ¡ficos\n",
    "â”‚   â”œâ”€â”€ wordcloud.png\n",
    "â”‚   â””â”€â”€ graficos_analise.png\n",
    "â””â”€â”€ requirements.txt                   # DependÃªncias do projeto\n",
    "```\n",
    "\n",
    "#### **Passo 3: Criar README.md Profissional**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instrucoes_finais"
   },
   "source": [
    "## ğŸ“‹ **InstruÃ§Ãµes Finais para PublicaÃ§Ã£o**\n",
    "\n",
    "### âœ… **Checklist Completo:**\n",
    "\n",
    "1. **âœ… Preparar Arquivos:**\n",
    "   - [ ] Download do notebook (.ipynb)\n",
    "   - [ ] README.md profissional\n",
    "   - [ ] requirements.txt\n",
    "   - [ ] Dados coletados (CSV/JSON)\n",
    "   - [ ] Screenshots das visualizaÃ§Ãµes\n",
    "\n",
    "2. **âœ… Criar RepositÃ³rio GitHub:**\n",
    "   - [ ] Nome: `univesp-web-crawler`\n",
    "   - [ ] DescriÃ§Ã£o detalhada\n",
    "   - [ ] README inicial\n",
    "   - [ ] LicenÃ§a MIT\n",
    "\n",
    "3. **âœ… Upload dos Arquivos:**\n",
    "   - [ ] Notebook principal\n",
    "   - [ ] DocumentaÃ§Ã£o\n",
    "   - [ ] Pasta `data/` com resultados\n",
    "   - [ ] Pasta `images/` com grÃ¡ficos\n",
    "\n",
    "4. **âœ… ConfiguraÃ§Ãµes Finais:**\n",
    "   - [ ] Topics/Tags: `web-crawler`, `data-science`, `python`, `univesp`\n",
    "   - [ ] GitHub Pages (opcional)\n",
    "   - [ ] Badge do Colab atualizado\n",
    "\n",
    "### ğŸ¯ **Dicas para Destaque:**\n",
    "\n",
    "- **ğŸ“¸ Screenshots**: Inclua imagens das visualizaÃ§Ãµes no README\n",
    "- **ğŸ¬ GIF Demonstrativo**: Grave um GIF da execuÃ§Ã£o do notebook\n",
    "- **ğŸ“Š Dados de Exemplo**: Mantenha alguns resultados como exemplo\n",
    "- **ğŸ·ï¸ Tags Relevantes**: Use tags para facilitar descoberta\n",
    "- **â­ Call-to-Action**: Incentive outros a dar estrela no projeto\n",
    "\n",
    "### ğŸš€ **PrÃ³ximos Passos:**\n",
    "\n",
    "1. **Execute todas as cÃ©lulas** deste notebook\n",
    "2. **Baixe todos os arquivos** gerados\n",
    "3. **Crie o repositÃ³rio** no GitHub\n",
    "4. **FaÃ§a upload** dos arquivos\n",
    "5. **Teste o badge** do Colab\n",
    "6. **Compartilhe** seu projeto!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ParabÃ©ns! Seu projeto estÃ¡ pronto para brilhar no GitHub! ğŸŒŸ**"
   ]
  }
 ]
}

# ğŸš€ Guia Completo: Publicando Projetos do Google Colab no GitHub

## ğŸ“‹ VisÃ£o Geral

Este guia mostra como publicar efetivamente seus projetos do Google Colab no GitHub, usando como exemplo o Web Crawler da Univesp que criamos.

---

## ğŸŒŸ OpÃ§Ãµes de PublicaÃ§Ã£o

### **OpÃ§Ã£o 1: PublicaÃ§Ã£o Direta do Colab (Mais Simples)**

1. **No Google Colab:**
   - Abra seu notebook
   - VÃ¡ em `File` â†’ `Save a copy in GitHub`
   - Autorize o Colab a acessar sua conta GitHub (se necessÃ¡rio)
   - Escolha o repositÃ³rio (ou crie um novo)
   - Adicione uma mensagem de commit
   - Clique `OK`

2. **Vantagens:**
   - âœ… Processo rÃ¡pido e direto
   - âœ… SincronizaÃ§Ã£o automÃ¡tica
   - âœ… MantÃ©m formataÃ§Ã£o do notebook

3. **LimitaÃ§Ãµes:**
   - âŒ Controle limitado sobre estrutura
   - âŒ NÃ£o inclui arquivos auxiliares automaticamente

### **OpÃ§Ã£o 2: Processo Manual Completo (Recomendado para Projetos Profissionais)**

---

## ğŸ› ï¸ **PASSO A PASSO DETALHADO**

### **Passo 1: Preparando o RepositÃ³rio no GitHub**

1. **Acesse [GitHub.com](https://github.com)**
2. **Clique em `New Repository`**
3. **Configure o repositÃ³rio:**
   - **Nome:** `univesp-web-crawler` (ou nome de sua escolha)
   - **DescriÃ§Ã£o:** `ğŸ•·ï¸ Web Crawler da Univesp com anÃ¡lise de frequÃªncia de palavras - Desenvolvido no Google Colab`
   - **Visibilidade:** Public (recomendado para portfÃ³lio)
   - **âœ… Marque:** `Add a README file`
   - **âœ… Escolha licenÃ§a:** MIT License
   - **âœ… Adicione .gitignore:** Python

4. **Clique `Create repository`**

### **Passo 2: Estrutura de Arquivos Recomendada**

```
ğŸ“ univesp-web-crawler/
â”œâ”€â”€ ğŸ“„ README.md                           # DocumentaÃ§Ã£o principal
â”œâ”€â”€ ğŸ““ Univesp_Web_Crawler_Colab.ipynb    # Notebook principal
â”œâ”€â”€ ğŸ“‹ requirements.txt                    # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ LICENSE                             # LicenÃ§a do projeto
â”œâ”€â”€ ğŸ“„ .gitignore                          # Arquivos a ignorar
â”œâ”€â”€ ğŸ“ data/                              # Dados coletados
â”‚   â”œâ”€â”€ univesp_frequencia_palavras.csv
â”‚   â”œâ”€â”€ univesp_dados_paginas.csv
â”‚   â””â”€â”€ univesp_relatorio_completo.json
â”œâ”€â”€ ğŸ“ images/                            # Screenshots e grÃ¡ficos
â”‚   â”œâ”€â”€ wordcloud_example.png
â”‚   â”œâ”€â”€ bar_chart_example.png
â”‚   â””â”€â”€ dashboard_screenshot.png
â”œâ”€â”€ ğŸ“ docs/                              # DocumentaÃ§Ã£o adicional
â”‚   â”œâ”€â”€ methodology.md
â”‚   â””â”€â”€ analysis_guide.md
â””â”€â”€ ğŸ“ scripts/                           # Scripts Python auxiliares
    â””â”€â”€ crawler_standalone.py
```

### **Passo 3: Criando um README.md Profissional**

Aqui estÃ¡ um template completo para seu README:

```markdown
# ğŸ•·ï¸ Web Crawler da Univesp

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)
[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Stars](https://img.shields.io/github/stars/SEU_USUARIO/univesp-web-crawler)](https://github.com/SEU_USUARIO/univesp-web-crawler/stargazers)

## ğŸ“‹ Sobre o Projeto

Este projeto implementa um **web crawler inteligente** que analisa o conteÃºdo do site da Univesp, extraindo informaÃ§Ãµes valiosas sobre a frequÃªncia de palavras e padrÃµes textuais. Desenvolvido como parte do **Desafio Semana 4**, demonstra tÃ©cnicas avanÃ§adas de:

- ğŸŒ **Web Scraping** com controle de taxa respeitoso
- ğŸ“Š **Processamento de Linguagem Natural**
- ğŸ¨ **VisualizaÃ§Ã£o de Dados Interativa**
- ğŸ“ˆ **AnÃ¡lise EstatÃ­stica AvanÃ§ada**

---

## âœ¨ CaracterÃ­sticas Principais

- ğŸ§  **Crawling Inteligente**: NavegaÃ§Ã£o sistemÃ¡tica com tratamento de erros
- ğŸ§¹ **Processamento de Texto**: Limpeza automÃ¡tica e filtro de stopwords
- ğŸ“Š **AnÃ¡lises AvanÃ§adas**: Diversidade lexical, correlaÃ§Ãµes e insights temÃ¡ticos
- ğŸ¨ **VisualizaÃ§Ãµes Ricas**: Word clouds, grÃ¡ficos interativos, heatmaps
- ğŸ’¾ **ExportaÃ§Ã£o Completa**: CSV, JSON e relatÃ³rios detalhados
- ğŸ **100% Python**: CompatÃ­vel com Google Colab e ambientes locais

---

## ğŸš€ Como Usar

### ğŸŒŸ OpÃ§Ã£o 1: Google Colab (Recomendada - Zero Setup)

1. **Clique no badge "Open in Colab" acima**
2. **Execute cÃ©lulas sequencialmente** (`Shift + Enter`)
3. **Baixe os resultados** automaticamente
4. **Personalize parÃ¢metros** conforme necessÃ¡rio

### ğŸ’» OpÃ§Ã£o 2: Ambiente Local

```bash
# Clone o repositÃ³rio
git clone https://github.com/SEU_USUARIO/univesp-web-crawler.git
cd univesp-web-crawler

# Crie ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instale dependÃªncias
pip install -r requirements.txt

# Execute o notebook
jupyter notebook Univesp_Web_Crawler_Colab.ipynb
```

---

## ğŸ“Š Resultados de Exemplo

### ğŸ† Top Palavras Identificadas
1. **tecnologia**: 1,036 ocorrÃªncias
2. **engenharia**: 902 ocorrÃªncias  
3. **univesp**: 748 ocorrÃªncias
4. **computaÃ§Ã£o**: 474 ocorrÃªncias
5. **dados**: 461 ocorrÃªncias

### ğŸ“ˆ MÃ©tricas Coletadas
- ğŸ“„ **25 pÃ¡ginas** analisadas com sucesso
- ğŸ”¤ **3,939 palavras Ãºnicas** identificadas
- ğŸ“Š **24,729 ocorrÃªncias totais** contabilizadas
- ğŸ¯ **100% taxa de sucesso**

---

## ğŸ¨ Galeria de VisualizaÃ§Ãµes

<table>
<tr>
<td align="center">
  <img src="images/wordcloud_example.png" width="300px" alt="Word Cloud"/>
  <br/>
  â˜ï¸ <b>Nuvem de Palavras</b>
</td>
<td align="center">
  <img src="images/bar_chart_example.png" width="300px" alt="GrÃ¡fico de Barras"/>
  <br/>
  ğŸ“Š <b>Ranking de FrequÃªncia</b>
</td>
</tr>
<tr>
<td align="center">
  <img src="images/correlation_matrix.png" width="300px" alt="Matriz de CorrelaÃ§Ã£o"/>
  <br/>
  ğŸ”¥ <b>Matriz de CorrelaÃ§Ã£o</b>
</td>
<td align="center">
  <img src="images/interactive_dashboard.png" width="300px" alt="Dashboard Interativo"/>
  <br/>
  ğŸ“ˆ <b>Dashboard Interativo</b>
</td>
</tr>
</table>

---

## ğŸ› ï¸ Tecnologias e Bibliotecas

| Categoria | Tecnologias |
|-----------|------------|
| **Core** | ![Python](https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white) ![Jupyter](https://img.shields.io/badge/-Jupyter-F37626?logo=jupyter&logoColor=white) |
| **Web Scraping** | ![Requests](https://img.shields.io/badge/-Requests-2CA5E0?logo=python&logoColor=white) ![BeautifulSoup](https://img.shields.io/badge/-BeautifulSoup-43B02A?logo=python&logoColor=white) |
| **Data Analysis** | ![Pandas](https://img.shields.io/badge/-Pandas-150458?logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/-NumPy-013243?logo=numpy&logoColor=white) |
| **VisualizaÃ§Ã£o** | ![Matplotlib](https://img.shields.io/badge/-Matplotlib-11557C?logo=python&logoColor=white) ![Plotly](https://img.shields.io/badge/-Plotly-3F4F75?logo=plotly&logoColor=white) ![Seaborn](https://img.shields.io/badge/-Seaborn-3776AB?logo=python&logoColor=white) |
| **Cloud** | ![Google Colab](https://img.shields.io/badge/-Google_Colab-F9AB00?logo=google-colab&logoColor=white) |

---

## ğŸ“ Estrutura Detalhada do Projeto

```
univesp-web-crawler/
â”œâ”€â”€ ğŸ““ Univesp_Web_Crawler_Colab.ipynb    # Notebook principal
â”œâ”€â”€ ğŸ“„ README.md                           # Esta documentaÃ§Ã£o
â”œâ”€â”€ ğŸ“‹ requirements.txt                    # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ LICENSE                             # LicenÃ§a MIT
â”œâ”€â”€ ğŸ“ data/                              # Dados coletados
â”‚   â”œâ”€â”€ ğŸ“Š univesp_frequencia_palavras.csv   # Top palavras + frequÃªncias
â”‚   â”œâ”€â”€ ğŸ“ˆ univesp_dados_paginas.csv         # MÃ©tricas por pÃ¡gina
â”‚   â”œâ”€â”€ ğŸ“„ univesp_relatorio_completo.json   # RelatÃ³rio completo
â”‚   â””â”€â”€ ğŸ“‹ README.md                          # DocumentaÃ§Ã£o dos dados
â”œâ”€â”€ ğŸ“ images/                            # Screenshots e visualizaÃ§Ãµes
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ wordcloud_example.png
â”‚   â”œâ”€â”€ ğŸ“Š bar_chart_example.png
â”‚   â”œâ”€â”€ ğŸ”¥ correlation_matrix.png
â”‚   â””â”€â”€ ğŸ“ˆ interactive_dashboard.png
â””â”€â”€ ğŸ“ docs/                              # DocumentaÃ§Ã£o adicional
    â”œâ”€â”€ ğŸ“– methodology.md                    # Metodologia detalhada
    â””â”€â”€ ğŸ“š analysis_guide.md                 # Guia de anÃ¡lises
```

---

## ğŸ”¬ AnÃ¡lises Implementadas

### 1. ğŸ“Š AnÃ¡lise de FrequÃªncia
- IdentificaÃ§Ã£o e ranking das palavras mais frequentes
- DistribuiÃ§Ã£o estatÃ­stica das ocorrÃªncias
- CategorizaÃ§Ã£o por faixas de frequÃªncia
- Filtro inteligente de stopwords em portuguÃªs

### 2. ğŸŒˆ Diversidade Lexical
- CÃ¡lculo do Type-Token Ratio (TTR)
- AnÃ¡lise da riqueza vocabular por pÃ¡gina
- ComparaÃ§Ã£o de diversidade entre seÃ§Ãµes
- CorrelaÃ§Ãµes com tamanho do texto

### 3. ğŸ¯ AnÃ¡lise TemÃ¡tica
- IdentificaÃ§Ã£o automÃ¡tica de focos temÃ¡ticos
- CategorizaÃ§Ã£o semÃ¢ntica (EducaÃ§Ã£o vs Tecnologia)
- AnÃ¡lise de co-ocorrÃªncias de palavras
- Insights sobre conteÃºdo institucional

### 4. ğŸ“ˆ AnÃ¡lise EstatÃ­stica
- Matriz de correlaÃ§Ã£o entre mÃ©tricas
- DistribuiÃ§Ãµes de probabilidade
- Testes de significÃ¢ncia estatÃ­stica
- AnÃ¡lise de outliers

---

## ğŸ“Š MÃ©tricas e KPIs

| MÃ©trica | Valor | DescriÃ§Ã£o |
|---------|-------|-----------|
| **Taxa de Sucesso** | 100% | PÃ¡ginas processadas com sucesso |
| **Cobertura** | 25 pÃ¡ginas | Total de pÃ¡ginas analisadas |
| **VocabulÃ¡rio** | 3,939 palavras | Palavras Ãºnicas identificadas |
| **Densidade** | 24,729 ocorrÃªncias | Total de palavras processadas |
| **Diversidade MÃ©dia** | 0.642 | Type-Token Ratio mÃ©dio |
| **Tempo de ExecuÃ§Ã£o** | ~2 minutos | Tempo mÃ©dio no Colab |

---

## ğŸš€ Roadmap e Futuras Melhorias

- [ ] **ğŸ“± Interface Web**: Dashboard interativo com Streamlit
- [ ] **ğŸ¤– ML Integration**: ClassificaÃ§Ã£o automÃ¡tica de tÃ³picos
- [ ] **ğŸ“Š AnÃ¡lise Temporal**: Tracking de mudanÃ§as ao longo do tempo
- [ ] **ğŸ” AnÃ¡lise SemÃ¢ntica**: Word embeddings e similaridade
- [ ] **ğŸ“ˆ API REST**: Endpoints para integraÃ§Ã£o externa
- [ ] **ğŸ³ ContainerizaÃ§Ã£o**: Deploy com Docker
- [ ] **â˜ï¸ Cloud Deployment**: AWS/GCP integration

---

## ğŸ¤ Como Contribuir

ContribuiÃ§Ãµes sÃ£o muito bem-vindas! VocÃª pode contribuir de vÃ¡rias formas:

### ğŸ”§ Desenvolvimento
1. **Fork** este repositÃ³rio
2. **Crie uma branch** (`git checkout -b feature/nova-funcionalidade`)
3. **Commit** suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. **Push** para a branch (`git push origin feature/nova-funcionalidade`)
5. **Abra um Pull Request**

### ğŸ› Reportar Bugs
- Use a seÃ§Ã£o **Issues** para reportar problemas
- Inclua detalhes sobre o ambiente e passos para reproduzir
- Adicione screenshots se relevante

### ğŸ’¡ SugestÃµes
- Compartilhe ideias para novas anÃ¡lises
- Sugira melhorias na visualizaÃ§Ã£o
- Proponha otimizaÃ§Ãµes de performance

---

## ğŸ“š Recursos Relacionados

### ğŸ“– Tutoriais e DocumentaÃ§Ã£o
- [Tutorial completo de Web Scraping](docs/web_scraping_guide.md)
- [Guia de AnÃ¡lise de Texto](docs/text_analysis_guide.md)
- [Manual de VisualizaÃ§Ãµes](docs/visualization_manual.md)

### ğŸ”— Links Ãšteis
- [DocumentaÃ§Ã£o BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Plotly Python Guide](https://plotly.com/python/)
- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)

### ğŸ“ Artigos AcadÃªmicos
- [Text Mining Techniques](https://example.com/text-mining)
- [Web Crawling Best Practices](https://example.com/crawling-practices)

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

### O que isso significa?
- âœ… **Uso comercial** permitido
- âœ… **ModificaÃ§Ã£o** permitida  
- âœ… **DistribuiÃ§Ã£o** permitida
- âœ… **Uso privado** permitido
- â— **Sem garantia** - use por sua prÃ³pria conta e risco

---

## ğŸ“¬ Contato e Suporte

### ğŸ‘¨â€ğŸ’» Desenvolvedor
**Seu Nome**
- ğŸ“§ **Email:** seu.email@exemplo.com
- ğŸ’¼ **LinkedIn:** [seu-perfil-linkedin](https://linkedin.com/in/seu-perfil)
- ğŸ™ **GitHub:** [seu-usuario](https://github.com/seu-usuario)

### ğŸ†˜ Suporte
- ğŸ› **Bugs:** Abra uma [issue](../../issues)
- ğŸ’¬ **DiscussÃµes:** Use as [discussions](../../discussions)
- ğŸ“– **DocumentaÃ§Ã£o:** Confira a [wiki](../../wiki)

### ğŸŒŸ Reconhecimentos
- **Univesp** - Pela inspiraÃ§Ã£o e dados pÃºblicos
- **Comunidade Python** - Pelas excelentes bibliotecas
- **Google Colab** - Pela plataforma gratuita e poderosa

---

## ğŸ“Š EstatÃ­sticas do Projeto

![GitHub stars](https://img.shields.io/github/stars/SEU_USUARIO/univesp-web-crawler?style=social)
![GitHub forks](https://img.shields.io/github/forks/SEU_USUARIO/univesp-web-crawler?style=social)
![GitHub issues](https://img.shields.io/github/issues/SEU_USUARIO/univesp-web-crawler)
![GitHub pull requests](https://img.shields.io/github/issues-pr/SEU_USUARIO/univesp-web-crawler)

![Visitors](https://visitor-badge.glitch.me/badge?page_id=SEU_USUARIO.univesp-web-crawler)
![Last Commit](https://img.shields.io/github/last-commit/SEU_USUARIO/univesp-web-crawler)
![Repo Size](https://img.shields.io/github/repo-size/SEU_USUARIO/univesp-web-crawler)

---

<div align="center">

## ğŸ‰ **Obrigado por visitar este projeto!** 

### Se foi Ãºtil para vocÃª, considere dar uma â­!

**Desenvolvido com â¤ï¸ para a comunidade educacional**

---

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)

**[ğŸš€ Executar Agora no Google Colab](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)**

</div>
```

### **Passo 4: PersonalizaÃ§Ãµes Importantes**

Antes de publicar, certifique-se de:

1. **Substituir placeholders:**
   - `SEU_USUARIO` â†’ seu username do GitHub
   - `seu.email@exemplo.com` â†’ seu email real
   - Links do LinkedIn e perfis sociais

2. **Adicionar screenshots reais:**
   - Execute o notebook e capture telas das visualizaÃ§Ãµes
   - Salve as imagens na pasta `images/`
   - Atualize os links no README

3. **Configurar badges:**
   - Teste todos os badges e links
   - Verifique se o badge do Colab funciona
   - Adicione badges especÃ­ficos do seu projeto

---

## ğŸ¯ **Dicas Para Destaque no GitHub**

### **1. Visual e ApresentaÃ§Ã£o**
- âœ… **README rico**: Use emojis, badges, tabelas e imagens
- âœ… **Screenshots**: Mostre o projeto funcionando
- âœ… **GIFs**: Grave demonstraÃ§Ãµes do projeto
- âœ… **Logo personalizado**: Crie um logo Ãºnico

### **2. OrganizaÃ§Ã£o do Projeto**
- âœ… **Estrutura clara**: Pastas bem organizadas
- âœ… **DocumentaÃ§Ã£o completa**: README, CONTRIBUTING, LICENSE
- âœ… **Exemplos prÃ¡ticos**: Dados de amostra inclusos
- âœ… **CÃ³digo limpo**: ComentÃ¡rios e docstrings

### **3. InteraÃ§Ã£o e Comunidade**
- âœ… **Issues template**: Facilite o reporte de bugs
- âœ… **Contributing guide**: InstruÃ§Ãµes para contribuir
- âœ… **Discussions**: Habilite discussÃµes no repo
- âœ… **Releases**: Marque versÃµes importantes

### **4. SEO e Descoberta**
- âœ… **Tags relevantes**: `web-scraping`, `data-science`, `python`, `education`
- âœ… **DescriÃ§Ã£o descritiva**: No repositÃ³rio GitHub
- âœ… **Keywords no README**: Termos que pessoas procuram
- âœ… **Links externos**: Para dataset, documentaÃ§Ã£o oficial

---

## ğŸ“‹ **Checklist Final de PublicaÃ§Ã£o**

### âœ… **PreparaÃ§Ã£o**
- [ ] Notebook executado completamente sem erros
- [ ] Todos os dados exportados (CSV, JSON)
- [ ] Screenshots de todas as visualizaÃ§Ãµes salvas
- [ ] README.md criado e personalizado
- [ ] requirements.txt com versÃµes especÃ­ficas
- [ ] Arquivos desnecessÃ¡rios removidos

### âœ… **RepositÃ³rio GitHub**
- [ ] RepositÃ³rio criado com nome descritivo
- [ ] DescriÃ§Ã£o curta mas informativa
- [ ] Tags/Topics adicionadas
- [ ] LicenÃ§a MIT selecionada
- [ ] .gitignore para Python configurado

### âœ… **Upload e OrganizaÃ§Ã£o**
- [ ] Estrutura de pastas criada
- [ ] Notebook principal na raiz
- [ ] README.md atualizado na raiz
- [ ] Dados organizados na pasta `data/`
- [ ] Imagens organizadas na pasta `images/`

### âœ… **Testes e VerificaÃ§Ã£o**
- [ ] Badge "Open in Colab" testado e funcionando
- [ ] Links internos do README verificados
- [ ] Notebook abre corretamente no Colab via GitHub
- [ ] Todos os badges exibindo informaÃ§Ãµes corretas
- [ ] LicenÃ§a e contribuiÃ§Ãµes claramente definidas

### âœ… **FinalizaÃ§Ã£o**
- [ ] Primeiro commit com mensagem clara
- [ ] Release v1.0 criada (opcional)
- [ ] Projeto compartilhado nas redes sociais
- [ ] Adicionado ao portfÃ³lio pessoal

---

## ğŸš€ **PrÃ³ximos Passos ApÃ³s PublicaÃ§Ã£o**

1. **ğŸ“¢ DivulgaÃ§Ã£o:**
   - Compartilhe no LinkedIn com hashtags relevantes
   - Poste no Twitter/X mencionando as tecnologias usadas
   - Adicione ao seu portfÃ³lio pessoal
   - Submeta a showcases da comunidade Python

2. **ğŸ”„ ManutenÃ§Ã£o:**
   - Monitor issues e pull requests
   - Atualize dependÃªncias regularmente
   - Adicione novas funcionalidades baseadas no feedback
   - Mantenha documentaÃ§Ã£o atualizada

3. **ğŸ“ˆ EvoluÃ§Ã£o:**
   - Colete feedback dos usuÃ¡rios
   - Implemente sugestÃµes de melhoria
   - Considere expansÃ£o para outros domÃ­nios
   - Explore oportunidades de colaboraÃ§Ã£o

---

**ğŸ‰ ParabÃ©ns! Seu projeto estÃ¡ pronto para fazer sucesso no GitHub!**

Lembre-se: um bom projeto no GitHub nÃ£o Ã© apenas sobre o cÃ³digo - Ã© sobre apresentaÃ§Ã£o, documentaÃ§Ã£o e experiÃªncia do usuÃ¡rio. Com este guia, vocÃª tem tudo necessÃ¡rio para criar um repositÃ³rio profissional e atrativo!

---

**ğŸŒŸ Boa sorte com seu projeto!**

{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMrKw8dCQVxYbNZXz5cJ8mR"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "titulo_principal"
   },
   "source": [
    "# ğŸ•·ï¸ Web Crawler da Univesp com AnÃ¡lise de FrequÃªncia de Palavras\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/SEU_USUARIO/SEU_REPOSITORIO)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## ğŸ“‹ DescriÃ§Ã£o do Projeto\n",
    "\n",
    "Este notebook implementa um **web crawler** que visita sistematicamente as pÃ¡ginas da Univesp, seguindo hyperlinks e analisando a frequÃªncia de palavras encontradas. O projeto foi desenvolvido como parte do **Desafio Semana 4** e demonstra tÃ©cnicas de:\n",
    "\n",
    "- ğŸŒ **Web Scraping** e crawling sistemÃ¡tico\n",
    "- ğŸ“Š **AnÃ¡lise de texto** e processamento de linguagem natural\n",
    "- ğŸ“ˆ **VisualizaÃ§Ã£o de dados** com grÃ¡ficos interativos\n",
    "- ğŸ **ProgramaÃ§Ã£o Python** com bibliotecas especializadas\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Objetivos\n",
    "1. Implementar um crawler que navega automaticamente pelo site da Univesp\n",
    "2. Extrair e limpar texto de pÃ¡ginas HTML\n",
    "3. Analisar frequÃªncia de palavras com filtros inteligentes\n",
    "4. Gerar visualizaÃ§Ãµes e relatÃ³rios detalhados\n",
    "5. Criar uma ferramenta reutilizÃ¡vel e bem documentada\n",
    "\n",
    "### ğŸš€ Como Usar Este Notebook\n",
    "1. **Execute as cÃ©lulas sequencialmente** usando `Shift + Enter`\n",
    "2. **Ajuste os parÃ¢metros** na seÃ§Ã£o de configuraÃ§Ã£o conforme necessÃ¡rio\n",
    "3. **Visualize os resultados** nas seÃ§Ãµes de anÃ¡lise e grÃ¡ficos\n",
    "4. **Baixe os dados** gerados na seÃ§Ã£o final\n",
    "\n",
    "### ğŸ“– Estrutura do Notebook\n",
    "- **SeÃ§Ã£o 1:** ConfiguraÃ§Ã£o e InstalaÃ§Ã£o de DependÃªncias\n",
    "- **SeÃ§Ã£o 2:** ImplementaÃ§Ã£o do Web Crawler\n",
    "- **SeÃ§Ã£o 3:** Processamento e AnÃ¡lise de Texto\n",
    "- **SeÃ§Ã£o 4:** VisualizaÃ§Ãµes e GrÃ¡ficos\n",
    "- **SeÃ§Ã£o 5:** RelatÃ³rios e ExportaÃ§Ã£o de Dados\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Desenvolvido para o Desafio Semana 4 - Univesp  \n",
    "**Data:** 2025  \n",
    "**VersÃ£o:** 1.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_instalacao"
   },
   "source": [
    "# 1ï¸âƒ£ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o de DependÃªncias\n",
    "\n",
    "Nesta seÃ§Ã£o, vamos instalar e importar todas as bibliotecas necessÃ¡rias para o funcionamento do web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ“¦ InstalaÃ§Ã£o de bibliotecas necessÃ¡rias\n",
    "!pip install requests beautifulsoup4 matplotlib seaborn wordcloud plotly pandas --quiet\n",
    "\n",
    "print(\"âœ… Todas as dependÃªncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# ğŸ“š ImportaÃ§Ã£o de bibliotecas\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas para web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Bibliotecas para anÃ¡lise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualizaÃ§Ã£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ConfiguraÃ§Ãµes visuais\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ… Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"ğŸ Python executando no Google Colab\")\n",
    "print(f\"â° Notebook iniciado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_configuracao"
   },
   "source": [
    "## âš™ï¸ ConfiguraÃ§Ã£o de ParÃ¢metros\n",
    "\n",
    "Aqui vocÃª pode ajustar os parÃ¢metros do crawler conforme suas necessidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuracao_parametros"
   },
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ CONFIGURAÃ‡Ã•ES DO WEB CRAWLER\n",
    "# ================================\n",
    "\n",
    "# URL base da Univesp\n",
    "BASE_URL = \"https://univesp.br\"\n",
    "\n",
    "# NÃºmero mÃ¡ximo de pÃ¡ginas a visitar\n",
    "MAX_PAGES = 20  # Reduzido para o Colab (pode aumentar se necessÃ¡rio)\n",
    "\n",
    "# Delay entre requisiÃ§Ãµes (em segundos)\n",
    "DELAY = 1.0  # Seja respeitoso com o servidor!\n",
    "\n",
    "# Timeout para requisiÃ§Ãµes (em segundos)\n",
    "TIMEOUT = 10\n",
    "\n",
    "# Headers para simular um navegador real\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Lista de stopwords em portuguÃªs\n",
    "STOPWORDS = {\n",
    "    'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'atÃ©', \n",
    "    'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', \n",
    "    'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', \n",
    "    'esse', 'esses', 'esta', 'estÃ£o', 'estas', 'estamos', 'estar', 'este', 'estes', \n",
    "    'eu', 'foi', 'for', 'foram', 'hÃ¡', 'isso', 'isto', 'jÃ¡', 'mais', 'mas', 'me', \n",
    "    'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'nÃ£o', 'no', 'nos', \n",
    "    'nÃ³s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', \n",
    "    'qual', 'quando', 'que', 'quem', 'sÃ£o', 'se', 'sem', 'ser', 'seu', 'seus', \n",
    "    'sÃ³', 'sua', 'suas', 'tambÃ©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', \n",
    "    'tuas', 'um', 'uma', 'vocÃª', 'vocÃªs', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',\n",
    "    'pode', 'podem', 'vai', 'vÃ£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',\n",
    "    'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'lÃ¡', 'agora', 'entÃ£o',\n",
    "    'sobre', 'apÃ³s', 'durante', 'antes', 'depois', 'enquanto', 'desde'\n",
    "}\n",
    "\n",
    "print(f\"âš™ï¸ ConfiguraÃ§Ãµes do Crawler:\")\n",
    "print(f\"   ğŸŒ URL Base: {BASE_URL}\")\n",
    "print(f\"   ğŸ“„ MÃ¡ximo de pÃ¡ginas: {MAX_PAGES}\")\n",
    "print(f\"   â±ï¸ Delay entre requisiÃ§Ãµes: {DELAY}s\")\n",
    "print(f\"   ğŸš« Stopwords: {len(STOPWORDS)} palavras filtradas\")\n",
    "print(f\"\\nâœ… ConfiguraÃ§Ã£o concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_crawler"
   },
   "source": [
    "# 2ï¸âƒ£ ImplementaÃ§Ã£o do Web Crawler\n",
    "\n",
    "Vamos criar nossa classe principal do web crawler com todas as funcionalidades necessÃ¡rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crawler_class"
   },
   "outputs": [],
   "source": [
    "class UnivespWebCrawler:\n",
    "    \"\"\"\n",
    "    ğŸ•·ï¸ Web Crawler da Univesp com AnÃ¡lise de FrequÃªncia de Palavras\n",
    "    \n",
    "    Esta classe implementa um crawler completo que:\n",
    "    - Visita pÃ¡ginas da Univesp sistematicamente\n",
    "    - Extrai e processa texto de cada pÃ¡gina\n",
    "    - Analisa frequÃªncia de palavras\n",
    "    - Gera estatÃ­sticas detalhadas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=BASE_URL, max_pages=MAX_PAGES, delay=DELAY):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.visited_urls = set()\n",
    "        self.failed_urls = set()\n",
    "        self.word_frequency = Counter()\n",
    "        self.page_word_counts = defaultdict(Counter)\n",
    "        self.crawl_data = []\n",
    "        \n",
    "        # Configurar sessÃ£o HTTP com retry\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Verifica se a URL Ã© vÃ¡lida e pertence ao domÃ­nio da Univesp\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return (parsed.netloc.endswith('univesp.br') and \n",
    "                   parsed.scheme in ['http', 'https'] and\n",
    "                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Faz o download do conteÃºdo de uma pÃ¡gina\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ”— Visitando: {url}\")\n",
    "            response = self.session.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro ao acessar {url}: {e}\")\n",
    "            self.failed_urls.add(url)\n",
    "            return None\n",
    "    \n",
    "    def extract_text_and_links(self, html_content, base_url):\n",
    "        \"\"\"Extrai texto limpo e links de uma pÃ¡gina HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Remove scripts, estilos e elementos desnecessÃ¡rios\n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extrai texto\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Limpa o texto\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Extrai links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(base_url, link['href'])\n",
    "                if self.is_valid_url(absolute_url):\n",
    "                    links.append(absolute_url)\n",
    "            \n",
    "            return clean_text, list(set(links))  # Remove duplicatas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro ao processar HTML: {e}\")\n",
    "            return \"\", []\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Processa texto para contar palavras\"\"\"\n",
    "        if not text:\n",
    "            return Counter()\n",
    "        \n",
    "        # Converte para minÃºsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Encontra palavras (incluindo acentos)\n",
    "        words = re.findall(r'\\b[a-zÃ¡Ã Ã¢Ã£Ã¤Ã©Ã¨ÃªÃ«Ã­Ã¬Ã®Ã¯Ã³Ã²Ã´ÃµÃ¶ÃºÃ¹Ã»Ã¼Ã§Ã±]+\\b', text)\n",
    "        \n",
    "        # Filtra palavras muito curtas e stopwords\n",
    "        filtered_words = [word for word in words \n",
    "                         if len(word) > 2 and word not in STOPWORDS]\n",
    "        \n",
    "        return Counter(filtered_words)\n",
    "    \n",
    "    def crawl_page(self, url):\n",
    "        \"\"\"Processa uma pÃ¡gina especÃ­fica\"\"\"\n",
    "        # Download da pÃ¡gina\n",
    "        html_content = self.fetch_page(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "        \n",
    "        # Extrai texto e links\n",
    "        text, links = self.extract_text_and_links(html_content, url)\n",
    "        \n",
    "        # Processa texto\n",
    "        if text:\n",
    "            word_count = self.process_text(text)\n",
    "            self.page_word_counts[url] = word_count\n",
    "            self.word_frequency.update(word_count)\n",
    "            \n",
    "            # Armazena dados para anÃ¡lise posterior\n",
    "            self.crawl_data.append({\n",
    "                'url': url,\n",
    "                'text_length': len(text),\n",
    "                'word_count': sum(word_count.values()),\n",
    "                'unique_words': len(word_count),\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Executa o processo principal de crawling\"\"\"\n",
    "        print(\"ğŸ•·ï¸ INICIANDO WEB CRAWLER DA UNIVESP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸŒ URL inicial: {self.base_url}\")\n",
    "        print(f\"ğŸ“„ MÃ¡ximo de pÃ¡ginas: {self.max_pages}\")\n",
    "        print(f\"â±ï¸ Delay: {self.delay}s\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        urls_to_visit = [self.base_url]\n",
    "        \n",
    "        while urls_to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            current_url = urls_to_visit.pop(0)\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            self.visited_urls.add(current_url)\n",
    "            \n",
    "            # Processa a pÃ¡gina\n",
    "            new_links = self.crawl_page(current_url)\n",
    "            \n",
    "            # Adiciona novos links\n",
    "            for link in new_links:\n",
    "                if (link not in self.visited_urls and \n",
    "                    link not in urls_to_visit and \n",
    "                    link not in self.failed_urls):\n",
    "                    urls_to_visit.append(link)\n",
    "            \n",
    "            # Progresso\n",
    "            if len(self.visited_urls) % 5 == 0:\n",
    "                print(f\"ğŸ“ˆ Progresso: {len(self.visited_urls)}/{self.max_pages} pÃ¡ginas\")\n",
    "            \n",
    "            # Delay\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\nâœ… Crawling concluÃ­do!\")\n",
    "        print(f\"ğŸ“Š PÃ¡ginas visitadas: {len(self.visited_urls)}\")\n",
    "        print(f\"âŒ PÃ¡ginas com erro: {len(self.failed_urls)}\")\n",
    "        print(f\"â±ï¸ Tempo total: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"ğŸ“ Palavras Ãºnicas encontradas: {len(self.word_frequency)}\")\n",
    "        print(f\"ğŸ”¤ Total de ocorrÃªncias: {sum(self.word_frequency.values())}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Retorna os dados do crawl como DataFrame do pandas\"\"\"\n",
    "        return pd.DataFrame(self.crawl_data)\n",
    "    \n",
    "    def get_word_frequency_df(self, top_n=100):\n",
    "        \"\"\"Retorna as palavras mais frequentes como DataFrame\"\"\"\n",
    "        top_words = self.word_frequency.most_common(top_n)\n",
    "        return pd.DataFrame(top_words, columns=['palavra', 'frequencia'])\n",
    "\n",
    "print(\"âœ… Classe UnivespWebCrawler criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_execucao"
   },
   "source": [
    "## ğŸš€ Executando o Web Crawler\n",
    "\n",
    "Agora vamos criar uma instÃ¢ncia do crawler e executÃ¡-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "executar_crawler",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ•·ï¸ Criar e executar o crawler\n",
    "crawler = UnivespWebCrawler(\n",
    "    base_url=BASE_URL,\n",
    "    max_pages=MAX_PAGES,\n",
    "    delay=DELAY\n",
    ")\n",
    "\n",
    "# Executar o crawling\n",
    "sucesso = crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise"
   },
   "source": [
    "# 3ï¸âƒ£ AnÃ¡lise dos Dados Coletados\n",
    "\n",
    "Agora vamos analisar os dados coletados pelo nosso crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_basica"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š AnÃ¡lise bÃ¡sica dos dados\n",
    "if sucesso:\n",
    "    print(\"ğŸ“ˆ ANÃLISE DOS DADOS COLETADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # EstatÃ­sticas gerais\n",
    "    df_pages = crawler.get_dataframe()\n",
    "    df_words = crawler.get_word_frequency_df(50)\n",
    "    \n",
    "    print(f\"\\nğŸ” EstatÃ­sticas Gerais:\")\n",
    "    print(f\"   ğŸ“„ Total de pÃ¡ginas processadas: {len(df_pages)}\")\n",
    "    print(f\"   ğŸ”¤ Palavras Ãºnicas encontradas: {len(crawler.word_frequency)}\")\n",
    "    print(f\"   ğŸ“Š Total de ocorrÃªncias de palavras: {sum(crawler.word_frequency.values())}\")\n",
    "    print(f\"   ğŸ“ MÃ©dia de palavras por pÃ¡gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"   ğŸ“ MÃ©dia de caracteres por pÃ¡gina: {df_pages['text_length'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† TOP 15 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, (palavra, freq) in enumerate(df_words.head(15).values, 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorrÃªncias\")\n",
    "    \n",
    "    # Criar DataFrames para uso posterior\n",
    "    print(f\"\\nâœ… DataFrames criados:\")\n",
    "    print(f\"   ğŸ“Š df_pages: {len(df_pages)} linhas (dados por pÃ¡gina)\")\n",
    "    print(f\"   ğŸ”¤ df_words: {len(df_words)} linhas (frequÃªncia de palavras)\")\nelse:\n    print(\"âŒ Erro durante o crawling. Verificar logs acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detalhes_paginas"
   },
   "outputs": [],
   "source": [
    "# ğŸ“‹ Detalhes das pÃ¡ginas coletadas\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸ“„ DETALHES DAS PÃGINAS COLETADAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Exibir as primeiras pÃ¡ginas\n",
    "    display(df_pages.head())\n",
    "    \n",
    "    print(\"\\nğŸ“Š EstatÃ­sticas descritivas:\")\n",
    "    display(df_pages[['text_length', 'word_count', 'unique_words']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_visualizacoes"
   },
   "source": [
    "# 4ï¸âƒ£ VisualizaÃ§Ãµes e GrÃ¡ficos\n",
    "\n",
    "Vamos criar visualizaÃ§Ãµes interessantes dos dados coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wordcloud"
   },
   "outputs": [],
   "source": [
    "# ğŸŒŸ Word Cloud das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"â˜ï¸ Criando Word Cloud...\")\n",
    "    \n",
    "    # Preparar dados para word cloud\n",
    "    word_freq_dict = dict(zip(df_words['palavra'], df_words['frequencia']))\n",
    "    \n",
    "    # Criar word cloud\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, height=600,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        min_font_size=10\n",
    "    ).generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('â˜ï¸ Nuvem de Palavras - Site da Univesp', fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Word Cloud criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grafico_barras"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š GrÃ¡fico de barras das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"ğŸ“Š Criando grÃ¡fico de barras...\")\n",
    "    \n",
    "    # Top 20 palavras\n",
    "    top_20 = df_words.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Criar grÃ¡fico de barras\n",
    "    bars = plt.bar(range(len(top_20)), top_20['frequencia'], \n",
    "                   color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))\n",
    "    \n",
    "    plt.xlabel('Palavras', fontsize=12)\n",
    "    plt.ylabel('FrequÃªncia', fontsize=12)\n",
    "    plt.title('ğŸ“Š Top 20 Palavras Mais Frequentes - Site da Univesp', fontsize=16, pad=20)\n",
    "    plt.xticks(range(len(top_20)), top_20['palavra'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… GrÃ¡fico de barras criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graficos_interativos"
   },
   "outputs": [],
   "source": [
    "# ğŸ“ˆ GrÃ¡ficos interativos com Plotly\n",
    "if not df_words.empty and not df_pages.empty:\n",
    "    print(\"ğŸ¨ Criando grÃ¡ficos interativos...\")\n",
    "    \n",
    "    # 1. GrÃ¡fico interativo de barras das palavras mais frequentes\n",
    "    top_25 = df_words.head(25)\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        top_25, \n",
    "        x='palavra', \n",
    "        y='frequencia',\n",
    "        title='ğŸ† Top 25 Palavras Mais Frequentes (Interativo)',\n",
    "        labels={'palavra': 'Palavra', 'frequencia': 'FrequÃªncia'},\n",
    "        color='frequencia',\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig1.update_layout(xaxis_tickangle=-45, height=600)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. GrÃ¡fico de distribuiÃ§Ã£o de palavras por pÃ¡gina\n",
    "    fig2 = px.scatter(\n",
    "        df_pages, \n",
    "        x='text_length', \n",
    "        y='word_count',\n",
    "        size='unique_words',\n",
    "        hover_data=['url'],\n",
    "        title='ğŸ“Š DistribuiÃ§Ã£o: Tamanho do Texto vs Contagem de Palavras',\n",
    "        labels={\n",
    "            'text_length': 'Tamanho do Texto (caracteres)',\n",
    "            'word_count': 'NÃºmero de Palavras',\n",
    "            'unique_words': 'Palavras Ãšnicas'\n",
    "        }\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. GrÃ¡fico de pizza das categorias de palavras\n",
    "    # Categorizar palavras por frequÃªncia\n",
    "    categories = []\n",
    "    for freq in df_words['frequencia']:\n",
    "        if freq >= 100:\n",
    "            categories.append('Muito Frequente (â‰¥100)')\n",
    "        elif freq >= 50:\n",
    "            categories.append('Frequente (50-99)')\n",
    "        elif freq >= 20:\n",
    "            categories.append('Moderada (20-49)')\n",
    "        elif freq >= 10:\n",
    "            categories.append('Baixa (10-19)')\n",
    "        else:\n",
    "            categories.append('Rara (<10)')\n",
    "    \n",
    "    df_words['categoria'] = categories\n",
    "    category_counts = df_words['categoria'].value_counts()\n",
    "    \n",
    "    fig3 = px.pie(\n",
    "        values=category_counts.values,\n",
    "        names=category_counts.index,\n",
    "        title='ğŸ¥§ DistribuiÃ§Ã£o de Palavras por Categoria de FrequÃªncia'\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"âœ… GrÃ¡ficos interativos criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise_avancada"
   },
   "source": [
    "# 5ï¸âƒ£ AnÃ¡lise AvanÃ§ada e Insights\n",
    "\n",
    "Vamos fazer algumas anÃ¡lises mais detalhadas dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_correlacoes"
   },
   "outputs": [],
   "source": [
    "# ğŸ” AnÃ¡lise de correlaÃ§Ãµes\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸ”— ANÃLISE DE CORRELAÃ‡Ã•ES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Matriz de correlaÃ§Ã£o\n",
    "    correlation_matrix = df_pages[['text_length', 'word_count', 'unique_words']].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True,\n",
    "                fmt='.3f')\n",
    "    plt.title('ğŸ”¥ Matriz de CorrelaÃ§Ã£o entre MÃ©tricas das PÃ¡ginas', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights\n",
    "    print(\"\\nğŸ’¡ INSIGHTS:\")\n",
    "    corr_text_words = correlation_matrix.loc['text_length', 'word_count']\n",
    "    corr_words_unique = correlation_matrix.loc['word_count', 'unique_words']\n",
    "    \n",
    "    print(f\"   ğŸ“ CorrelaÃ§Ã£o Tamanho do Texto â†” NÃºmero de Palavras: {corr_text_words:.3f}\")\n",
    "    print(f\"   ğŸ”¤ CorrelaÃ§Ã£o NÃºmero de Palavras â†” Palavras Ãšnicas: {corr_words_unique:.3f}\")\n",
    "    \n",
    "    if corr_text_words > 0.7:\n",
    "        print(\"   âœ… Forte correlaÃ§Ã£o positiva entre tamanho do texto e nÃºmero de palavras\")\n",
    "    if corr_words_unique > 0.7:\n",
    "        print(\"   âœ… Forte correlaÃ§Ã£o positiva entre nÃºmero total e palavras Ãºnicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_diversidade"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š AnÃ¡lise de diversidade lexical\n",
    "if not df_pages.empty:\n",
    "    print(\"ğŸŒˆ ANÃLISE DE DIVERSIDADE LEXICAL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Calcular diversidade lexical (Type-Token Ratio)\n",
    "    df_pages['diversidade_lexical'] = df_pages['unique_words'] / df_pages['word_count']\n",
    "    \n",
    "    # EstatÃ­sticas\n",
    "    diversidade_media = df_pages['diversidade_lexical'].mean()\n",
    "    diversidade_std = df_pages['diversidade_lexical'].std()\n",
    "    \n",
    "    print(f\"ğŸ“Š Diversidade Lexical MÃ©dia: {diversidade_media:.3f}\")\n",
    "    print(f\"ğŸ“ˆ Desvio PadrÃ£o: {diversidade_std:.3f}\")\n",
    "    \n",
    "    # GrÃ¡fico de distribuiÃ§Ã£o\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_pages['diversidade_lexical'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(diversidade_media, color='red', linestyle='--', label=f'MÃ©dia: {diversidade_media:.3f}')\n",
    "    plt.xlabel('Diversidade Lexical (TTR)')\n",
    "    plt.ylabel('FrequÃªncia')\n",
    "    plt.title('ğŸ“Š DistribuiÃ§Ã£o da Diversidade Lexical')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(df_pages['word_count'], df_pages['diversidade_lexical'], alpha=0.6, color='green')\n",
    "    plt.xlabel('NÃºmero de Palavras')\n",
    "    plt.ylabel('Diversidade Lexical')\n",
    "    plt.title('ğŸ“ˆ Diversidade vs NÃºmero de Palavras')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # PÃ¡ginas com maior diversidade\n",
    "    top_diversidade = df_pages.nlargest(3, 'diversidade_lexical')\n",
    "    \n",
    "    print(\"\\nğŸ† TOP 3 PÃGINAS COM MAIOR DIVERSIDADE LEXICAL:\")\n",
    "    for i, (idx, row) in enumerate(top_diversidade.iterrows(), 1):\n",
    "        print(f\"{i}. Diversidade: {row['diversidade_lexical']:.3f}\")\n",
    "        print(f\"   URL: {row['url'][:80]}...\")\n",
    "        print(f\"   Palavras: {row['word_count']} | Ãšnicas: {row['unique_words']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_relatorios"
   },
   "source": [
    "# 6ï¸âƒ£ RelatÃ³rios e ExportaÃ§Ã£o\n",
    "\n",
    "Vamos gerar relatÃ³rios finais e preparar os dados para download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "relatorio_final"
   },
   "outputs": [],
   "source": [
    "# ğŸ“‹ RelatÃ³rio final consolidado\n",
    "def gerar_relatorio_final():\n",
    "    if not crawler.visited_urls:\n",
    "        print(\"âŒ Nenhum dado disponÃ­vel para relatÃ³rio\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“‹ RELATÃ“RIO FINAL - WEB CRAWLER UNIVESP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ•·ï¸ Executado em: {datetime.now().strftime('%d/%m/%Y Ã s %H:%M:%S')}\")\n",
    "    print(f\"ğŸŒ URL Base: {BASE_URL}\")\n",
    "    print()\n",
    "    \n",
    "    # EstatÃ­sticas do Crawling\n",
    "    print(\"ğŸ“Š ESTATÃSTICAS DO CRAWLING:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"âœ… PÃ¡ginas visitadas com sucesso: {len(crawler.visited_urls)}\")\n",
    "    print(f\"âŒ PÃ¡ginas com erro: {len(crawler.failed_urls)}\")\n",
    "    print(f\"ğŸ¯ Taxa de sucesso: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # AnÃ¡lise de Texto\n",
    "    print(\"ğŸ“ ANÃLISE DE TEXTO:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"ğŸ”¤ Total de palavras Ãºnicas: {len(crawler.word_frequency):,}\")\n",
    "    print(f\"ğŸ“Š Total de ocorrÃªncias: {sum(crawler.word_frequency.values()):,}\")\n",
    "    print(f\"ğŸ“„ MÃ©dia de palavras por pÃ¡gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"ğŸŒˆ Diversidade lexical mÃ©dia: {df_pages['diversidade_lexical'].mean():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Top palavras\n",
    "    print(\"ğŸ† TOP 10 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorrÃªncias\")\n",
    "    print()\n",
    "    \n",
    "    # AnÃ¡lise por domÃ­nios temÃ¡ticos\n",
    "    palavras_educacao = ['curso', 'ensino', 'educaÃ§Ã£o', 'aprendizagem', 'estudo', 'aluno', 'professor']\n",
    "    palavras_tecnologia = ['tecnologia', 'computaÃ§Ã£o', 'dados', 'sistema', 'digital', 'informÃ¡tica']\n",
    "    \n",
    "    freq_educacao = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_educacao)\n",
    "    freq_tecnologia = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_tecnologia)\n",
    "    \n",
    "    print(\"ğŸ“ ANÃLISE TEMÃTICA:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"ğŸ“š Palavras relacionadas Ã  EducaÃ§Ã£o: {freq_educacao} ocorrÃªncias\")\n",
    "    print(f\"ğŸ’» Palavras relacionadas Ã  Tecnologia: {freq_tecnologia} ocorrÃªncias\")\n",
    "    \n",
    "    if freq_educacao > freq_tecnologia:\n",
    "        print(\"ğŸ“Š Foco predominante: EducaÃ§Ã£o\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š Foco predominante: Tecnologia\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… RelatÃ³rio gerado com sucesso!\")\n",
    "\n",
    "# Gerar relatÃ³rio\n",
    "gerar_relatorio_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exportar_dados"
   },
   "outputs": [],
   "source": [
    "# ğŸ’¾ Preparar dados para download\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"ğŸ’¾ PREPARANDO DADOS PARA DOWNLOAD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Salvar frequÃªncia de palavras como CSV\n",
    "df_words_full = crawler.get_word_frequency_df(1000)  # Top 1000 palavras\n",
    "df_words_full.to_csv('univesp_frequencia_palavras.csv', index=False, encoding='utf-8')\n",
    "print(\"âœ… Arquivo CSV criado: univesp_frequencia_palavras.csv\")\n",
    "\n",
    "# 2. Salvar dados das pÃ¡ginas como CSV\n",
    "df_pages_export = df_pages.copy()\n",
    "df_pages_export['timestamp'] = df_pages_export['timestamp'].astype(str)\n",
    "df_pages_export.to_csv('univesp_dados_paginas.csv', index=False, encoding='utf-8')\n",
    "print(\"âœ… Arquivo CSV criado: univesp_dados_paginas.csv\")\n",
    "\n",
    "# 3. Salvar relatÃ³rio como JSON\n",
    "relatorio_json = {\n",
    "    'metadata': {\n",
    "        'data_execucao': datetime.now().isoformat(),\n",
    "        'url_base': BASE_URL,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'delay': DELAY\n",
    "    },\n",
    "    'estatisticas': {\n",
    "        'paginas_visitadas': len(crawler.visited_urls),\n",
    "        'paginas_com_erro': len(crawler.failed_urls),\n",
    "        'palavras_unicas': len(crawler.word_frequency),\n",
    "        'total_ocorrencias': sum(crawler.word_frequency.values()),\n",
    "        'diversidade_lexical_media': float(df_pages['diversidade_lexical'].mean()) if 'diversidade_lexical' in df_pages.columns else 0\n",
    "    },\n",
    "    'top_palavras': dict(crawler.word_frequency.most_common(50)),\n",
    "    'urls_visitadas': list(crawler.visited_urls),\n",
    "    'urls_com_erro': list(crawler.failed_urls)\n",
    "}\n",
    "\n",
    "with open('univesp_relatorio_completo.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relatorio_json, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Arquivo JSON criado: univesp_relatorio_completo.json\")\n",
    "\n",
    "# 4. Criar arquivo README para os dados\n",
    "readme_content = f\"\"\"\n# ğŸ“Š Dados do Web Crawler - Univesp\n\n## ğŸ“‹ DescriÃ§Ã£o\nDados coletados pelo web crawler da Univesp executado em {datetime.now().strftime('%d/%m/%Y Ã s %H:%M:%S')}.\n\n## ğŸ“ Arquivos Inclusos:\n- **univesp_frequencia_palavras.csv**: Top 1000 palavras mais frequentes\n- **univesp_dados_paginas.csv**: Dados detalhados de cada pÃ¡gina visitada\n- **univesp_relatorio_completo.json**: RelatÃ³rio completo em formato JSON\n- **README.md**: Este arquivo de documentaÃ§Ã£o\n\n## ğŸ“Š EstatÃ­sticas:\n- ğŸŒ **URL Base**: {BASE_URL}\n- ğŸ“„ **PÃ¡ginas Processadas**: {len(crawler.visited_urls)}\n- ğŸ”¤ **Palavras Ãšnicas**: {len(crawler.word_frequency):,}\n- ğŸ“Š **Total de OcorrÃªncias**: {sum(crawler.word_frequency.values()):,}\n\n## ğŸ† Top 10 Palavras:\n\"\"\"\n\nfor i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n    readme_content += f\"{i:2d}. **{palavra}**: {freq} ocorrÃªncias\\n\"\n\nreadme_content += f\"\"\"\n\n## ğŸ› ï¸ Como Usar:\n1. Abra os arquivos CSV em Excel, Google Sheets ou pandas\n2. Use o arquivo JSON para anÃ¡lises programÃ¡ticas\n3. Consulte este README para entender a estrutura dos dados\n\n---\n*Gerado pelo Web Crawler da Univesp - Notebook do Google Colab*\n\"\"\"\n\nwith open('README.md', 'w', encoding='utf-8') as f:\n    f.write(readme_content)\nprint(\"âœ… Arquivo README criado: README.md\")\n\nprint(\"\\nğŸ“¦ Todos os arquivos estÃ£o prontos para download!\")\nprint(\"ğŸ’¡ Use o menu Files â†’ Download para baixar os arquivos individualmente\")\nprint(\"ğŸ’¡ Ou execute a cÃ©lula abaixo para download automÃ¡tico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_arquivos"
   },
   "outputs": [],
   "source": [
    "# ğŸ“¥ Download automÃ¡tico dos arquivos (opcional)\n",
    "print(\"ğŸ“¥ Iniciando download dos arquivos...\")\n",
    "print(\"(Os arquivos serÃ£o baixados para sua pasta de Downloads)\")\n",
    "\n",
    "try:\n",
    "    files.download('univesp_frequencia_palavras.csv')\n",
    "    files.download('univesp_dados_paginas.csv')\n",
    "    files.download('univesp_relatorio_completo.json')\n",
    "    files.download('README.md')\n",
    "    print(\"\\nâœ… Download concluÃ­do com sucesso!\")\nexcept Exception as e:\n",
    "    print(f\"âš ï¸ Erro no download automÃ¡tico: {e}\")\n    print(\"ğŸ’¡ Use o menu Files para download manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_publicacao_github"
   },
   "source": [
    "# ğŸš€ Publicando no GitHub\n",
    "\n",
    "## ğŸ“ Guia Completo para Publicar este Projeto no GitHub\n",
    "\n",
    "### ğŸŒŸ **OpÃ§Ã£o 1: PublicaÃ§Ã£o Direta do Colab**\n",
    "\n",
    "1. **Salvar no GitHub diretamente:**\n",
    "   - No Colab: `File` â†’ `Save a copy in GitHub`\n",
    "   - Escolha seu repositÃ³rio ou crie um novo\n",
    "   - Adicione uma mensagem de commit\n",
    "   - Clique em `OK`\n",
    "\n",
    "### ğŸ› ï¸ **OpÃ§Ã£o 2: Processo Manual Completo**\n",
    "\n",
    "#### **Passo 1: Preparar o RepositÃ³rio**\n",
    "1. Acesse [GitHub.com](https://github.com)\n",
    "2. Clique em `New Repository`\n",
    "3. Nome sugerido: `univesp-web-crawler`\n",
    "4. Adicione descriÃ§Ã£o: `ğŸ•·ï¸ Web Crawler da Univesp com anÃ¡lise de frequÃªncia de palavras`\n",
    "5. Marque `Add a README file`\n",
    "6. Clique `Create repository`\n",
    "\n",
    "#### **Passo 2: Estrutura de Arquivos Recomendada**\n",
    "```\n",
    "univesp-web-crawler/\n",
    "â”œâ”€â”€ Univesp_Web_Crawler_Colab.ipynb    # Este notebook\n",
    "â”œâ”€â”€ README.md                           # DocumentaÃ§Ã£o principal\n",
    "â”œâ”€â”€ data/                              # Dados coletados\n",
    "â”‚   â”œâ”€â”€ univesp_frequencia_palavras.csv\n",
    "â”‚   â”œâ”€â”€ univesp_dados_paginas.csv\n",
    "â”‚   â””â”€â”€ univesp_relatorio_completo.json\n",
    "â”œâ”€â”€ images/                            # Screenshots e grÃ¡ficos\n",
    "â”‚   â”œâ”€â”€ wordcloud.png\n",
    "â”‚   â””â”€â”€ graficos_analise.png\n",
    "â””â”€â”€ requirements.txt                   # DependÃªncias do projeto\n",
    "```\n",
    "\n",
    "#### **Passo 3: Criar README.md Profissional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "criar_readme_github"
   },
   "outputs": [],
   "source": [
    "# ğŸ“ Criar README profissional para GitHub\n",
    "readme_github = f'''# ğŸ•·ï¸ Web Crawler da Univesp\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## ğŸ“‹ Sobre o Projeto\n",
    "\n",
    "Este projeto implementa um **web crawler inteligente** que analisa o conteÃºdo do site da Univesp, extraindo informaÃ§Ãµes valiosas sobre a frequÃªncia de palavras e padrÃµes textuais. Desenvolvido como parte do **Desafio Semana 4**, demonstra tÃ©cnicas avanÃ§adas de web scraping, processamento de linguagem natural e visualizaÃ§Ã£o de dados.\n",
    "\n",
    "### ğŸ¯ Objetivos\n",
    "- Implementar crawling sistemÃ¡tico e respeitoso do site da Univesp\n",
    "- Analisar frequÃªncia e distribuiÃ§Ã£o de palavras\n",
    "- Gerar visualizaÃ§Ãµes interativas dos dados coletados\n",
    "- Criar ferramenta reutilizÃ¡vel para anÃ¡lise de conteÃºdo web\n",
    "\n",
    "### âœ¨ CaracterÃ­sticas Principais\n",
    "- ğŸŒ **Crawling Inteligente**: NavegaÃ§Ã£o sistemÃ¡tica com controle de taxa\n",
    "- ğŸ§¹ **Processamento de Texto**: Limpeza automÃ¡tica e filtro de stopwords\n",
    "- ğŸ“Š **AnÃ¡lises AvanÃ§adas**: Diversidade lexical, correlaÃ§Ãµes e insights\n",
    "- ğŸ¨ **VisualizaÃ§Ãµes Ricas**: Word clouds, grÃ¡ficos interativos com Plotly\n",
    "- ğŸ’¾ **ExportaÃ§Ã£o Completa**: CSV, JSON e relatÃ³rios detalhados\n",
    "- ğŸ **100% Python**: CompatÃ­vel com Google Colab e ambientes locais\n",
    "\n",
    "## ğŸš€ Como Usar\n",
    "\n",
    "### OpÃ§Ã£o 1: Google Colab (Recomendada)\n",
    "1. Clique no botÃ£o \"Open in Colab\" acima\n",
    "2. Execute as cÃ©lulas sequencialmente\n",
    "3. Baixe os resultados gerados\n",
    "\n",
    "### OpÃ§Ã£o 2: Ambiente Local\n",
    "```bash\n",
    "# Clone o repositÃ³rio\n",
    "git clone https://github.com/SEU_USUARIO/univesp-web-crawler.git\n",
    "cd univesp-web-crawler\n",
    "\n",
    "# Instale dependÃªncias\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Execute o notebook\n",
    "jupyter notebook Univesp_Web_Crawler_Colab.ipynb\n",
    "```\n",
    "\n",
    "## ğŸ“Š Resultados Obtidos\n",
    "\n",
    "Baseado na Ãºltima execuÃ§Ã£o ({datetime.now().strftime(\"%d/%m/%Y\")}):\n",
    "\n",
    "- ğŸ“„ **{len(crawler.visited_urls)} pÃ¡ginas** analisadas com sucesso\n",
    "- ğŸ”¤ **{len(crawler.word_frequency):,} palavras Ãºnicas** identificadas\n",
    "- ğŸ“ˆ **{sum(crawler.word_frequency.values()):,} ocorrÃªncias totais** contabilizadas\n",
    "- ğŸ¯ **Taxa de sucesso**: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\n",
    "\n",
    "### ğŸ† Palavras Mais Frequentes\n",
    "'''\n",
    "\n",
    "# Adicionar top 10 palavras\n",
    "for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "    readme_github += f\"{i}. **{palavra}**: {freq} ocorrÃªncias\\n\"\n",
    "\n",
    "readme_github += f'''\n",
    "## ğŸ› ï¸ Tecnologias Utilizadas\n",
    "\n",
    "- **Python 3.7+**: Linguagem principal\n",
    "- **Requests + BeautifulSoup**: Web scraping\n",
    "- **Pandas**: ManipulaÃ§Ã£o de dados\n",
    "- **Matplotlib + Seaborn**: VisualizaÃ§Ãµes estÃ¡ticas\n",
    "- **Plotly**: GrÃ¡ficos interativos\n",
    "- **WordCloud**: Nuvens de palavras\n",
    "- **Google Colab**: Ambiente de execuÃ§Ã£o\n",
    "\n",
    "## ğŸ“ Estrutura do Projeto\n",
    "\n",
    "```\n",
    "univesp-web-crawler/\n",
    "â”œâ”€â”€ Univesp_Web_Crawler_Colab.ipynb    # Notebook principal\n",
    "â”œâ”€â”€ README.md                           # Este arquivo\n",
    "â”œâ”€â”€ requirements.txt                    # DependÃªncias\n",
    "â”œâ”€â”€ data/                              # Dados coletados\n",
    "â”‚   â”œâ”€â”€ univesp_frequencia_palavras.csv\n",
    "â”‚   â”œâ”€â”€ univesp_dados_paginas.csv\n",
    "â”‚   â””â”€â”€ univesp_relatorio_completo.json\n",
    "â””â”€â”€ images/                            # VisualizaÃ§Ãµes geradas\n",
    "```\n",
    "\n",
    "## ğŸ“ˆ AnÃ¡lises DisponÃ­veis\n",
    "\n",
    "### 1. AnÃ¡lise de FrequÃªncia\n",
    "- IdentificaÃ§Ã£o das palavras mais comuns\n",
    "- DistribuiÃ§Ã£o estatÃ­stica das ocorrÃªncias\n",
    "- CategorizaÃ§Ã£o por faixas de frequÃªncia\n",
    "\n",
    "### 2. Diversidade Lexical\n",
    "- CÃ¡lculo do Type-Token Ratio (TTR)\n",
    "- AnÃ¡lise da riqueza vocabular por pÃ¡gina\n",
    "- CorrelaÃ§Ãµes entre mÃ©tricas textuais\n",
    "\n",
    "### 3. AnÃ¡lise TemÃ¡tica\n",
    "- IdentificaÃ§Ã£o de focos temÃ¡ticos (EducaÃ§Ã£o vs Tecnologia)\n",
    "- Agrupamento por categorias semÃ¢nticas\n",
    "- Insights sobre o conteÃºdo institucional\n",
    "\n",
    "## ğŸ¨ VisualizaÃ§Ãµes\n",
    "\n",
    "- â˜ï¸ **Word Clouds**: RepresentaÃ§Ã£o visual das palavras mais frequentes\n",
    "- ğŸ“Š **GrÃ¡ficos de Barras**: Rankings de frequÃªncia\n",
    "- ğŸ¥§ **GrÃ¡ficos de Pizza**: DistribuiÃ§Ã£o por categorias\n",
    "- ğŸ“ˆ **Scatter Plots**: CorrelaÃ§Ãµes entre mÃ©tricas\n",
    "- ğŸ”¥ **Heatmaps**: Matrizes de correlaÃ§Ã£o\n",
    "\n",
    "## ğŸ¤ ContribuiÃ§Ãµes\n",
    "\n",
    "ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se livre para:\n",
    "- Abrir issues para bugs ou sugestÃµes\n",
    "- Submeter pull requests com melhorias\n",
    "- Propor novas anÃ¡lises ou visualizaÃ§Ãµes\n",
    "- Expandir para outros sites educacionais\n",
    "\n",
    "## ğŸ“„ LicenÃ§a\n",
    "\n",
    "Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.\n",
    "\n",
    "## ğŸ“§ Contato\n",
    "\n",
    "Desenvolvido com â¤ï¸ para o **Desafio Semana 4 - Univesp**\n",
    "\n",
    "---\n",
    "\n",
    "â­ **Se este projeto foi Ãºtil, considere dar uma estrela!** â­\n",
    "'''\n",
    "\n",
    "# Salvar README\n",
    "with open('README_GitHub.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_github)\n",
    "\n",
    "print(\"âœ… README profissional criado: README_GitHub.md\")\n",
    "print(\"ğŸ’¡ Copie este conteÃºdo para o README.md do seu repositÃ³rio no GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "criar_requirements"
   },
   "outputs": [],
   "source": [
    "# ğŸ“¦ Criar arquivo requirements.txt\n",
    "requirements_content = '''requests==2.31.0\n",
    "beautifulsoup4==4.12.2\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "plotly==5.15.0\n",
    "wordcloud==1.9.2\n",
    "urllib3==2.0.4\n",
    "'''\n",
    "\n",
    "with open('requirements_GitHub.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"âœ… Arquivo requirements.txt criado\")\n",
    "print(\"ğŸ’¡ Renomeie para 'requirements.txt' no seu repositÃ³rio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instrucoes_finais"
   },
   "source": [
    "## ğŸ“‹ **InstruÃ§Ãµes Finais para PublicaÃ§Ã£o**\n",
    "\n",
    "### âœ… **Checklist Completo:**\n",
    "\n",
    "1. **âœ… Preparar Arquivos:**\n",
    "   - [ ] Download do notebook (.ipynb)\n",
    "   - [ ] README.md profissional\n",
    "   - [ ] requirements.txt\n",
    "   - [ ] Dados coletados (CSV/JSON)\n",
    "   - [ ] Screenshots das visualizaÃ§Ãµes\n",
    "\n",
    "2. **âœ… Criar RepositÃ³rio GitHub:**\n",
    "   - [ ] Nome: `univesp-web-crawler`\n",
    "   - [ ] DescriÃ§Ã£o detalhada\n",
    "   - [ ] README inicial\n",
    "   - [ ] LicenÃ§a MIT\n",
    "\n",
    "3. **âœ… Upload dos Arquivos:**\n",
    "   - [ ] Notebook principal\n",
    "   - [ ] DocumentaÃ§Ã£o\n",
    "   - [ ] Pasta `data/` com resultados\n",
    "   - [ ] Pasta `images/` com grÃ¡ficos\n",
    "\n",
    "4. **âœ… ConfiguraÃ§Ãµes Finais:**\n",
    "   - [ ] Topics/Tags: `web-crawler`, `data-science`, `python`, `univesp`\n",
    "   - [ ] GitHub Pages (opcional)\n",
    "   - [ ] Badge do Colab atualizado\n",
    "\n",
    "### ğŸ¯ **Dicas para Destaque:**\n",
    "\n",
    "- **ğŸ“¸ Screenshots**: Inclua imagens das visualizaÃ§Ãµes no README\n",
    "- **ğŸ¬ GIF Demonstrativo**: Grave um GIF da execuÃ§Ã£o do notebook\n",
    "- **ğŸ“Š Dados de Exemplo**: Mantenha alguns resultados como exemplo\n",
    "- **ğŸ·ï¸ Tags Relevantes**: Use tags para facilitar descoberta\n",
    "- **â­ Call-to-Action**: Incentive outros a dar estrela no projeto\n",
    "\n",
    "### ğŸš€ **PrÃ³ximos Passos:**\n",
    "\n",
    "1. **Execute todas as cÃ©lulas** deste notebook\n",
    "2. **Baixe todos os arquivos** gerados\n",
    "3. **Crie o repositÃ³rio** no GitHub\n",
    "4. **FaÃ§a upload** dos arquivos\n",
    "5. **Teste o badge** do Colab\n",
    "6. **Compartilhe** seu projeto!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ParabÃ©ns! Seu projeto estÃ¡ pronto para brilhar no GitHub! ğŸŒŸ**"
   ]
  }
 ]\n}

RESULTADOS DO WEB CRAWLER DA UNIVESP
============================================================

Data da execuÃ§Ã£o: 17/08/2025 14:49:32
PÃ¡ginas visitadas: 25
PÃ¡ginas com erro: 0
Total de palavras Ãºnicas: 3468
Total de ocorrÃªncias: 23396

TOP 100 PALAVRAS MAIS FREQUENTES:
--------------------------------------------------
  1. bacharelado                    :  1339 ocorrÃªncias
  2. tecnologia                     :  1034 ocorrÃªncias
  3. engenharia                     :   906 ocorrÃªncias
  4. univesp                        :   827 ocorrÃªncias
  5. polo                           :   489 ocorrÃªncias
  6. computaÃ§Ã£o                     :   476 ocorrÃªncias
  7. dados                          :   461 ocorrÃªncias
  8. ciÃªncia                        :   460 ocorrÃªncias
  9. licenciatura                   :   457 ocorrÃªncias
 10. administraÃ§Ã£o                  :   457 ocorrÃªncias
 11. informaÃ§Ã£o                     :   454 ocorrÃªncias
 12. letras                         :   453 ocorrÃªncias
 13. matemÃ¡tica                     :   452 ocorrÃªncias
 14. pedagogia                      :   449 ocorrÃªncias
 15. processos                      :   448 ocorrÃªncias
 16. produÃ§Ã£o                       :   446 ocorrÃªncias
 17. habilitaÃ§Ã£o                    :   444 ocorrÃªncias
 18. gerenciais                     :   444 ocorrÃªncias
 19. lÃ­ngua                         :   443 ocorrÃªncias
 20. portuguesa                     :   443 ocorrÃªncias
 21. cep                            :   441 ocorrÃªncias
 22. rua                            :   271 ocorrÃªncias
 23. curso                          :   190 ocorrÃªncias
 24. centro                         :   188 ocorrÃªncias
 25. gestÃ£o                         :   149 ocorrÃªncias
 26. pÃºblica                        :   148 ocorrÃªncias
 27. oferta                         :   144 ocorrÃªncias
 28. superior                       :   135 ocorrÃªncias
 29. cps                            :   133 ocorrÃªncias
 30. Ãºltima                         :   131 ocorrÃªncias
 31. paulo                          :   129 ocorrÃªncias
 32. avenida                        :    95 ocorrÃªncias
 33. cursos                         :    82 ocorrÃªncias
 34. jardim                         :    81 ocorrÃªncias
 35. universidade                   :    74 ocorrÃªncias
 36. virtual                        :    65 ocorrÃªncias
 37. inclusÃ£o                       :    64 ocorrÃªncias
 38. vila                           :    64 ocorrÃªncias
 39. estado                         :    62 ocorrÃªncias
 40. aluno                          :    61 ocorrÃªncias
 41. escola                         :    57 ocorrÃªncias
 42. atendimento                    :    57 ocorrÃªncias
 43. paulista                       :    56 ocorrÃªncias
 44. vestibular                     :    53 ocorrÃªncias
 45. uniceu                         :    52 ocorrÃªncias
 46. polos                          :    45 ocorrÃªncias
 47. https                          :    40 ocorrÃªncias
 48. disciplinas                    :    40 ocorrÃªncias
 49. josÃ©                           :    40 ocorrÃªncias
 50. graduaÃ§Ã£o                      :    39 ocorrÃªncias
 51. alunos                         :    39 ocorrÃªncias
 52. atividades                     :    35 ocorrÃªncias
 53. acesso                         :    34 ocorrÃªncias
 54. matrÃ­cula                      :    34 ocorrÃªncias
 55. sobre                          :    33 ocorrÃªncias
 56. aprendizagem                   :    32 ocorrÃªncias
 57. libras                         :    31 ocorrÃªncias
 58. canais                         :    30 ocorrÃªncias
 59. joÃ£o                           :    30 ocorrÃªncias
 60. aulas                          :    29 ocorrÃªncias
 61. sedpcd                         :    28 ocorrÃªncias
 62. inscriÃ§Ãµes                     :    28 ocorrÃªncias
 63. praÃ§a                          :    28 ocorrÃªncias
 64. matriz                         :    28 ocorrÃªncias
 65. estÃ¡gio                        :    28 ocorrÃªncias
 66. castro                         :    27 ocorrÃªncias
 67. nesta                          :    27 ocorrÃªncias
 68. vista                          :    27 ocorrÃªncias
 69. introduÃ§Ã£o                     :    26 ocorrÃªncias
 70. educaÃ§Ã£o                       :    26 ocorrÃªncias
 71. institucional                  :    25 ocorrÃªncias
 72. nove                           :    25 ocorrÃªncias
 73. quarta                         :    25 ocorrÃªncias
 74. bairro                         :    25 ocorrÃªncias
 75. santa                          :    25 ocorrÃªncias
 76. bela                           :    24 ocorrÃªncias
 77. ambiente                       :    24 ocorrÃªncias
 78. projeto                        :    24 ocorrÃªncias
 79. transparÃªncia                  :    23 ocorrÃªncias
 80. carreira                       :    23 ocorrÃªncias
 81. leonardo                       :    23 ocorrÃªncias
 82. chefia                         :    23 ocorrÃªncias
 83. gabinete                       :    23 ocorrÃªncias
 84. abre                           :    23 ocorrÃªncias
 85. gravadas                       :    23 ocorrÃªncias
 86. andar                          :    23 ocorrÃªncias
 87. notÃ­cias                       :    23 ocorrÃªncias
 88. estudantes                     :    23 ocorrÃªncias
 89. ava                            :    23 ocorrÃªncias
 90. assume                         :    22 ocorrÃªncias
 91. inaugura                       :    22 ocorrÃªncias
 92. presencial                     :    20 ocorrÃªncias
 93. oferece                        :    20 ocorrÃªncias
 94. portaria                       :    20 ocorrÃªncias
 95. prova                          :    20 ocorrÃªncias
 96. novo                           :    19 ocorrÃªncias
 97. possui                         :    19 ocorrÃªncias
 98. partir                         :    19 ocorrÃªncias
 99. novos                          :    19 ocorrÃªncias
100. dÃºvidas                        :    19 ocorrÃªncias


PÃGINAS VISITADAS:
--------------------------------------------------
  1. https://univesp.br/institucional/marca
  2. https://univesp.br/institucional/parceiros
  3. https://univesp.br/noticias/escola-da-inclusao-da-sedpcd-abre-inscricoes-para-curso-de-introducao-a-libras-com-aulas-gravadas-nesta-quarta-16
  4. https://univesp.br/noticias/resposta-em-relacao-ao-novo-marco-da-ead
  5. https://univesp.br/institucional
  6. https://univesp.br/
  7. https://univesp.br/carreiras
  8. https://apps.univesp.br/manual-do-aluno/
  9. https://univesp.br/noticias/univesp-e-reconhecida-com-o-selo-o-cliente-recomenda
 10. https://univesp.br
 11. https://intranet.univesp.br/
 12. https://univesp.br/transparencia
 13. https://univesp.br/noticias/univesp-inaugura-polo-na-escola-da-inclusao
 14. https://univesp.br/noticias/aluno-da-univesp-vence-hackathon-de-inteligencia-artificial-da-aws-e-insper
 15. https://univesp.br/noticias/univesp-promove-integracao-entre-novos-docentes-e-supervisores
 16. https://univesp.br/noticias
 17. https://univesp.br/acesso_aluno.html
 18. https://univesp.br/noticias/leonardo-castro-assume-a-chefia-de-gabinete-da-univesp
 19. https://univesp.br/cursos
 20. https://univesp.br/polos
 21. http://apps.univesp.br/repositorio/
 22. https://univesp.br/noticias/polo-mirassol-estacao-crianca-leva-oficinas-ludicas-a-praca-da-matriz-a-partir-de-sabado
 23. https://univesp.br/vestibular
 24. https://univesp.br/noticias/univesp-realiza-aula-magna-com-marcelo-tas
 25. https://atendimento.univesp.br

# PowerShell Deployment Script for Univesp Web Crawler
# Usage: .\Deploy-UnivespCrawler.ps1 -Mode simple|full|docker|status

param(
    [Parameter(Mandatory=$true)]
    [ValidateSet("simple", "full", "docker", "venv", "status", "schedule")]
    [string]$Mode,
    
    [switch]$Force,
    [switch]$Quiet
)

# Set error action preference
$ErrorActionPreference = "Stop"

# Define colors
$Colors = @{
    Success = "Green"
    Error   = "Red"
    Warning = "Yellow"
    Info    = "Cyan"
}

function Write-ColoredOutput {
    param(
        [string]$Message,
        [string]$Color = "White",
        [string]$Emoji = ""
    )
    
    if (-not $Quiet) {
        Write-Host "$Emoji $Message" -ForegroundColor $Color
    }
}

function Test-Requirements {
    Write-ColoredOutput "ğŸ” Checking requirements..." $Colors.Info
    
    # Check Python
    try {
        $pythonVersion = python --version 2>&1
        if ($pythonVersion -match "Python (\d+)\.(\d+)") {
            $major = [int]$matches[1]
            $minor = [int]$matches[2]
            
            if ($major -ge 3 -and $minor -ge 6) {
                Write-ColoredOutput "âœ… Python $major.$minor detected" $Colors.Success
            } else {
                Write-ColoredOutput "âŒ Python 3.6+ required. Found: $pythonVersion" $Colors.Error
                return $false
            }
        }
    }
    catch {
        Write-ColoredOutput "âŒ Python not found in PATH" $Colors.Error
        return $false
    }
    
    # Check required files
    $requiredFiles = @(
        "univesp_crawler_simple.py",
        "requirements.txt"
    )
    
    foreach ($file in $requiredFiles) {
        if (-not (Test-Path $file)) {
            Write-ColoredOutput "âŒ Required file missing: $file" $Colors.Error
            return $false
        }
    }
    
    Write-ColoredOutput "âœ… All requirements met" $Colors.Success
    return $true
}

function Install-Dependencies {
    Write-ColoredOutput "ğŸ“¦ Installing dependencies..." $Colors.Info
    
    try {
        $result = pip install -r requirements.txt 2>&1
        if ($LASTEXITCODE -eq 0) {
            Write-ColoredOutput "âœ… Dependencies installed successfully" $Colors.Success
            return $true
        } else {
            Write-ColoredOutput "âŒ Failed to install dependencies: $result" $Colors.Error
            return $false
        }
    }
    catch {
        Write-ColoredOutput "âŒ Error installing dependencies: $_" $Colors.Error
        return $false
    }
}

function Deploy-Simple {
    Write-ColoredOutput "ğŸš€ Deploying simple version..." $Colors.Info
    
    try {
        python univesp_crawler_simple.py
        Write-ColoredOutput "âœ… Simple deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "âŒ Simple deployment failed: $_" $Colors.Error
        return $false
    }
}

function Deploy-Full {
    Write-ColoredOutput "ğŸš€ Deploying full version..." $Colors.Info
    
    if (-not (Install-Dependencies)) {
        return $false
    }
    
    try {
        python univesp_crawler.py
        Write-ColoredOutput "âœ… Full deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "âŒ Full deployment failed: $_" $Colors.Error
        return $false
    }
}

function Deploy-Docker {
    Write-ColoredOutput "ğŸ³ Deploying with Docker..." $Colors.Info
    
    # Check if Docker is available
    try {
        docker --version | Out-Null
        if ($LASTEXITCODE -ne 0) {
            throw "Docker not available"
        }
    }
    catch {
        Write-ColoredOutput "âŒ Docker not found. Please install Docker Desktop" $Colors.Error
        return $false
    }
    
    try {
        # Build Docker image
        Write-ColoredOutput "ğŸ”¨ Building Docker image..." $Colors.Info
        docker build -t univesp-crawler .
        if ($LASTEXITCODE -ne 0) {
            throw "Docker build failed"
        }
        
        # Create results directory if it doesn't exist
        if (-not (Test-Path "results")) {
            New-Item -ItemType Directory -Name "results" | Out-Null
        }
        
        # Run Docker container
        Write-ColoredOutput "ğŸƒ Running Docker container..." $Colors.Info
        docker run --rm -v "${PWD}\results:/app/results" univesp-crawler
        
        Write-ColoredOutput "âœ… Docker deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "âŒ Docker deployment failed: $_" $Colors.Error
        return $false
    }
}

function New-VirtualEnvironment {
    Write-ColoredOutput "ğŸ—ï¸ Creating virtual environment..." $Colors.Info
    
    $venvPath = "venv"
    
    try {
        # Create virtual environment
        python -m venv $venvPath
        if ($LASTEXITCODE -ne 0) {
            throw "Failed to create virtual environment"
        }
        
        # Activate virtual environment and install dependencies
        $activateScript = Join-Path $venvPath "Scripts\Activate.ps1"
        
        Write-ColoredOutput "ğŸ”§ Installing dependencies in virtual environment..." $Colors.Info
        
        # Run pip install in the virtual environment
        & "$venvPath\Scripts\pip.exe" install -r requirements.txt
        if ($LASTEXITCODE -ne 0) {
            throw "Failed to install dependencies in virtual environment"
        }
        
        Write-ColoredOutput "âœ… Virtual environment created successfully" $Colors.Success
        Write-ColoredOutput "ğŸ’¡ To activate: $activateScript" $Colors.Info
        Write-ColoredOutput "ğŸ’¡ To run crawler: & venv\Scripts\python.exe univesp_crawler.py" $Colors.Info
        
        return $true
    }
    catch {
        Write-ColoredOutput "âŒ Failed to create virtual environment: $_" $Colors.Error
        return $false
    }
}

function Show-Status {
    Write-ColoredOutput "" 
    Write-ColoredOutput "=" * 60
    Write-ColoredOutput "ğŸ“Š UNIVESP CRAWLER DEPLOYMENT STATUS" $Colors.Info
    Write-ColoredOutput "=" * 60
    
    # Check files
    $files = @(
        @{Path = "univesp_crawler_simple.py"; Description = "Simple Version"},
        @{Path = "univesp_crawler.py"; Description = "Full Version"},
        @{Path = "requirements.txt"; Description = "Dependencies"},
        @{Path = "Dockerfile"; Description = "Docker Support"},
        @{Path = "docker-compose.yml"; Description = "Docker Compose"},
        @{Path = "README.md"; Description = "Documentation"}
    )
    
    Write-ColoredOutput ""
    Write-ColoredOutput "ğŸ“ Files:" $Colors.Info
    foreach ($file in $files) {
        $exists = Test-Path $file.Path
        $status = if ($exists) { "âœ…" } else { "âŒ" }
        Write-Host "   $status $($file.Description.PadRight(20)) ($($file.Path))"
    }
    
    # Check tools
    Write-ColoredOutput ""
    Write-ColoredOutput "ğŸ”§ Tools:" $Colors.Info
    
    $tools = @("python", "pip", "docker", "docker-compose")
    foreach ($tool in $tools) {
        try {
            & $tool --version | Out-Null
            $available = $LASTEXITCODE -eq 0
        }
        catch {
            $available = $false
        }
        
        $status = if ($available) { "âœ…" } else { "âŒ" }
        Write-Host "   $status $tool"
    }
    
    Write-ColoredOutput ""
    Write-ColoredOutput "ğŸ’» System: $([System.Environment]::OSVersion.Platform)" $Colors.Info
    Write-ColoredOutput "ğŸ Python: $((python --version 2>&1))" $Colors.Info
    Write-ColoredOutput "ğŸ“‚ Project: $PWD" $Colors.Info
}

function New-ScheduledTask {
    Write-ColoredOutput "â° Setting up scheduled task..." $Colors.Info
    
    $scriptPath = Join-Path $PWD "univesp_crawler_simple.py"
    $pythonPath = (Get-Command python).Source
    
    Write-ColoredOutput ""
    Write-ColoredOutput "ğŸ’¡ To create a scheduled task in Windows:" $Colors.Info
    Write-ColoredOutput "   1. Open Task Scheduler" $Colors.Info
    Write-ColoredOutput "   2. Create Basic Task" $Colors.Info
    Write-ColoredOutput "   3. Set your desired schedule" $Colors.Info
    Write-ColoredOutput "   4. Action: Start a program" $Colors.Info
    Write-ColoredOutput "   5. Program/script: $pythonPath" $Colors.Info
    Write-ColoredOutput "   6. Add arguments: $scriptPath" $Colors.Info
    Write-ColoredOutput "   7. Start in: $PWD" $Colors.Info
    Write-ColoredOutput ""
    
    return $true
}

# Main execution
try {
    Write-ColoredOutput "ğŸš€ UNIVESP WEB CRAWLER DEPLOYMENT" $Colors.Info
    Write-ColoredOutput "=" * 50
    Write-ColoredOutput ""
    
    # Check requirements (except for status mode)
    if ($Mode -ne "status") {
        if (-not (Test-Requirements)) {
            Write-ColoredOutput "âŒ Requirements check failed" $Colors.Error
            exit 1
        }
    }
    
    # Execute based on mode
    $success = $false
    
    switch ($Mode) {
        "simple" {
            $success = Deploy-Simple
        }
        "full" {
            $success = Deploy-Full
        }
        "docker" {
            $success = Deploy-Docker
        }
        "venv" {
            $success = New-VirtualEnvironment
        }
        "status" {
            Show-Status
            $success = $true
        }
        "schedule" {
            $success = New-ScheduledTask
        }
    }
    
    if ($success) {
        Write-ColoredOutput ""
        Write-ColoredOutput "ğŸ‰ Deployment completed successfully!" $Colors.Success
    } else {
        Write-ColoredOutput ""
        Write-ColoredOutput "ğŸ’¥ Deployment failed!" $Colors.Error
        exit 1
    }
}
catch {
    Write-ColoredOutput "âŒ Unexpected error: $_" $Colors.Error
    exit 1
}

@echo off
echo.
echo ========================================
echo  UNIVESP WEB CRAWLER - QUICK START
echo ========================================
echo.

REM Check if Python is installed
python --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Python is not installed or not in PATH
    echo Please install Python 3.6+ from https://python.org
    pause
    exit /b 1
)

echo âœ… Python detected
echo.

REM Check if the crawler script exists
if not exist "univesp_crawler_simple.py" (
    echo âŒ Crawler script not found in current directory
    echo Please ensure univesp_crawler_simple.py is in the same folder
    pause
    exit /b 1
)

echo âœ… Crawler script found
echo.

REM Ask user which version to run
echo Choose deployment option:
echo 1. Simple version (no dependencies)
echo 2. Full version (install dependencies)
echo 3. Create virtual environment
echo 4. Exit
echo.
set /p choice=Enter your choice (1-4): 

if "%choice%"=="1" goto simple
if "%choice%"=="2" goto full
if "%choice%"=="3" goto venv
if "%choice%"=="4" goto end
goto invalid

:simple
echo.
echo ğŸš€ Running simple version...
echo.
python univesp_crawler_simple.py
goto end

:full
echo.
echo ğŸ“¦ Installing dependencies...
pip install -r requirements.txt
if errorlevel 1 (
    echo âŒ Failed to install dependencies
    pause
    exit /b 1
)
echo.
echo ğŸš€ Running full version...
echo.
python univesp_crawler.py
goto end

:venv
echo.
echo ğŸ—ï¸ Creating virtual environment...
python -m venv venv
if errorlevel 1 (
    echo âŒ Failed to create virtual environment
    pause
    exit /b 1
)

echo Activating virtual environment...
call venv\Scripts\activate.bat

echo Installing dependencies...
pip install -r requirements.txt
if errorlevel 1 (
    echo âŒ Failed to install dependencies
    pause
    exit /b 1
)

echo.
echo ğŸš€ Running crawler in virtual environment...
echo.
python univesp_crawler.py
goto end

:invalid
echo.
echo âŒ Invalid choice. Please run the script again.
pause
exit /b 1

:end
echo.
echo ğŸ“Š Check the results in the generated report file!
echo.
pause

#!/usr/bin/env python3
"""
ğŸš€ One-Click Deployment Script for Univesp Web Crawler
=====================================================

This script automates the deployment process with multiple options.
"""

import os
import sys
import subprocess
import argparse
import platform
from pathlib import Path

class UnivespDeployer:
    """Handles deployment of the Univesp crawler in different environments"""
    
    def __init__(self):
        self.project_dir = Path(__file__).parent
        self.python_executable = sys.executable
        self.system = platform.system()
        
    def check_requirements(self):
        """Check if all requirements are met"""
        print("ğŸ” Checking requirements...")
        
        # Check Python version
        if sys.version_info < (3, 6):
            print("âŒ Python 3.6 or higher is required")
            return False
            
        print(f"âœ… Python {sys.version_info.major}.{sys.version_info.minor} detected")
        
        # Check if required files exist
        required_files = [
            'univesp_crawler_simple.py',
            'requirements.txt'
        ]
        
        for file in required_files:
            if not (self.project_dir / file).exists():
                print(f"âŒ Required file missing: {file}")
                return False
                
        print("âœ… All required files found")
        return True
    
    def install_dependencies(self):
        """Install Python dependencies"""
        print("ğŸ“¦ Installing dependencies...")
        
        try:
            subprocess.run([
                self.python_executable, "-m", "pip", "install", 
                "-r", str(self.project_dir / "requirements.txt")
            ], check=True, capture_output=True, text=True)
            print("âœ… Dependencies installed successfully")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install dependencies: {e}")
            return False
    
    def create_virtual_environment(self):
        """Create and setup virtual environment"""
        print("ğŸ—ï¸ Creating virtual environment...")
        
        venv_path = self.project_dir / "venv"
        
        try:
            # Create virtual environment
            subprocess.run([
                self.python_executable, "-m", "venv", str(venv_path)
            ], check=True)
            
            # Determine activation script based on OS
            if self.system == "Windows":
                activate_script = venv_path / "Scripts" / "activate.bat"
                pip_executable = venv_path / "Scripts" / "pip.exe"
            else:
                activate_script = venv_path / "bin" / "activate"
                pip_executable = venv_path / "bin" / "pip"
            
            # Install dependencies in virtual environment
            subprocess.run([
                str(pip_executable), "install", "-r", "requirements.txt"
            ], check=True, cwd=str(self.project_dir))
            
            print(f"âœ… Virtual environment created at: {venv_path}")
            print(f"ğŸ’¡ To activate: {activate_script}")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to create virtual environment: {e}")
            return False
    
    def deploy_simple(self):
        """Deploy using the simple version (no external dependencies)"""
        print("ğŸš€ Deploying simple version...")
        
        try:
            subprocess.run([
                self.python_executable, 
                str(self.project_dir / "univesp_crawler_simple.py")
            ], cwd=str(self.project_dir))
            print("âœ… Simple deployment completed")
            return True
        except KeyboardInterrupt:
            print("âš ï¸ Deployment interrupted by user")
            return True
        except Exception as e:
            print(f"âŒ Deployment failed: {e}")
            return False
    
    def deploy_full(self):
        """Deploy the full version with dependencies"""
        print("ğŸš€ Deploying full version...")
        
        if not self.install_dependencies():
            return False
            
        try:
            subprocess.run([
                self.python_executable, 
                str(self.project_dir / "univesp_crawler.py")
            ], cwd=str(self.project_dir))
            print("âœ… Full deployment completed")
            return True
        except KeyboardInterrupt:
            print("âš ï¸ Deployment interrupted by user")
            return True
        except Exception as e:
            print(f"âŒ Deployment failed: {e}")
            return False
    
    def deploy_docker(self):
        """Deploy using Docker"""
        print("ğŸ³ Deploying with Docker...")
        
        try:
            # Check if Docker is available
            subprocess.run(["docker", "--version"], 
                          check=True, capture_output=True)
            
            # Build Docker image
            print("ğŸ”¨ Building Docker image...")
            subprocess.run([
                "docker", "build", "-t", "univesp-crawler", "."
            ], check=True, cwd=str(self.project_dir))
            
            # Run Docker container
            print("ğŸƒ Running Docker container...")
            subprocess.run([
                "docker", "run", "--rm", 
                "-v", f"{self.project_dir}/results:/app/results",
                "univesp-crawler"
            ], check=True)
            
            print("âœ… Docker deployment completed")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Docker deployment failed: {e}")
            print("ğŸ’¡ Make sure Docker is installed and running")
            return False
    
    def deploy_docker_compose(self):
        """Deploy using Docker Compose"""
        print("ğŸ³ Deploying with Docker Compose...")
        
        try:
            # Check if Docker Compose is available
            subprocess.run(["docker-compose", "--version"], 
                          check=True, capture_output=True)
            
            # Run with Docker Compose
            subprocess.run([
                "docker-compose", "up", "--build"
            ], check=True, cwd=str(self.project_dir))
            
            print("âœ… Docker Compose deployment completed")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Docker Compose deployment failed: {e}")
            print("ğŸ’¡ Make sure Docker Compose is installed")
            return False
    
    def create_scheduled_task(self):
        """Create a scheduled task for automatic runs"""
        print("â° Setting up scheduled execution...")
        
        script_path = self.project_dir / "univesp_crawler_simple.py"
        
        if self.system == "Windows":
            print("ğŸ’¡ For Windows Task Scheduler:")
            print(f"   Program: {self.python_executable}")
            print(f"   Arguments: {script_path}")
            print(f"   Start in: {self.project_dir}")
        else:
            print("ğŸ’¡ For Linux/Mac cron job, add this line to crontab:")
            print(f"   0 2 * * * cd {self.project_dir} && {self.python_executable} {script_path}")
        
        return True
    
    def show_status(self):
        """Show deployment status and options"""
        print("\n" + "="*60)
        print("ğŸ“Š UNIVESP CRAWLER DEPLOYMENT STATUS")
        print("="*60)
        
        # Check files
        files_status = [
            ("univesp_crawler_simple.py", "Simple Version"),
            ("univesp_crawler.py", "Full Version"),
            ("requirements.txt", "Dependencies"),
            ("Dockerfile", "Docker Support"),
            ("docker-compose.yml", "Docker Compose"),
            ("README.md", "Documentation")
        ]
        
        print("\nğŸ“ Files:")
        for file, description in files_status:
            exists = (self.project_dir / file).exists()
            status = "âœ…" if exists else "âŒ"
            print(f"   {status} {description:<20} ({file})")
        
        # Check tools
        tools_status = []
        for tool in ["python", "pip", "docker", "docker-compose"]:
            try:
                subprocess.run([tool, "--version"], 
                              capture_output=True, check=True)
                tools_status.append((tool, True))
            except:
                tools_status.append((tool, False))
        
        print("\nğŸ”§ Tools:")
        for tool, available in tools_status:
            status = "âœ…" if available else "âŒ"
            print(f"   {status} {tool}")
        
        print(f"\nğŸ’» System: {self.system}")
        print(f"ğŸ Python: {sys.version}")
        print(f"ğŸ“‚ Project: {self.project_dir}")


def main():
    """Main deployment function"""
    parser = argparse.ArgumentParser(
        description="ğŸš€ Univesp Web Crawler Deployment Tool"
    )
    
    parser.add_argument(
        "mode",
        choices=["simple", "full", "venv", "docker", "compose", "schedule", "status"],
        help="Deployment mode"
    )
    
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="Only check requirements, don't deploy"
    )
    
    args = parser.parse_args()
    
    deployer = UnivespDeployer()
    
    print("ğŸš€ UNIVESP WEB CRAWLER DEPLOYMENT")
    print("="*50)
    
    # Check requirements first
    if not deployer.check_requirements():
        print("âŒ Requirements check failed")
        sys.exit(1)
    
    if args.check_only:
        print("âœ… Requirements check passed")
        sys.exit(0)
    
    # Execute deployment based on mode
    success = False
    
    if args.mode == "simple":
        success = deployer.deploy_simple()
    elif args.mode == "full":
        success = deployer.deploy_full()
    elif args.mode == "venv":
        success = deployer.create_virtual_environment()
    elif args.mode == "docker":
        success = deployer.deploy_docker()
    elif args.mode == "compose":
        success = deployer.deploy_docker_compose()
    elif args.mode == "schedule":
        success = deployer.create_scheduled_task()
    elif args.mode == "status":
        deployer.show_status()
        success = True
    
    if success:
        print("\nğŸ‰ Deployment completed successfully!")
    else:
        print("\nğŸ’¥ Deployment failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()

version: '3.8'

services:
  univesp-crawler:
    build: .
    container_name: univesp-crawler
    environment:
      - MAX_PAGES=30
      - DELAY=2.0
    volumes:
      - ./results:/app/results
    restart: unless-stopped
    
  # Optional: Add a web interface service
  # univesp-web:
  #   build: 
  #     context: .
  #     dockerfile: Dockerfile.web
  #   container_name: univesp-web
  #   ports:
  #     - "8000:8000"
  #   depends_on:
  #     - univesp-crawler
  #   restart: unless-stopped

volumes:
  results:

# Use Python 3.9 slim image
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV MAX_PAGES=25
ENV DELAY=1.5

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first (for better Docker layer caching)
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY univesp_crawler.py .
COPY univesp_crawler_simple.py .

# Create results directory
RUN mkdir -p /app/results

# Set the results directory as a volume
VOLUME ["/app/results"]

# Expose port (if you add a web interface later)
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import sys; sys.exit(0)"

# Default command - run the simple version
CMD ["python", "univesp_crawler_simple.py"]

# ğŸš€ DEPLOYMENT GUIDE - Univesp Web Crawler

This guide shows you how to deploy and run the Univesp web crawler in different environments.

## ğŸ“‹ Quick Start (Local Deployment)

### Option 1: Immediate Local Run (Easiest)
```powershell
# Navigate to the project directory
cd C:\Users\dell

# Run the simple version (no dependencies needed)
python univesp_crawler_simple.py
```

### Option 2: Full Version with Dependencies
```powershell
# Install dependencies
pip install -r requirements.txt

# Run the full version
python univesp_crawler.py
```

## ğŸ–¥ï¸ Local Environment Setup

### Prerequisites Check
```powershell
# Check Python version (needs 3.6+)
python --version

# Check pip
pip --version

# List current packages
pip list
```

### Virtual Environment Setup (Recommended)
```powershell
# Create virtual environment
python -m venv univesp_crawler_env

# Activate virtual environment (Windows)
.\univesp_crawler_env\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Run the crawler
python univesp_crawler.py

# Deactivate when done
deactivate
```

## ğŸ³ Docker Deployment

### Create Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Copy requirements first (for better caching)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY univesp_crawler.py .
COPY univesp_crawler_simple.py .

# Create results directory
RUN mkdir -p /app/results

# Run the simple version by default
CMD ["python", "univesp_crawler_simple.py"]
```

### Docker Commands
```bash
# Build Docker image
docker build -t univesp-crawler .

# Run container
docker run -v $(pwd)/results:/app/results univesp-crawler

# Run with custom settings
docker run -e MAX_PAGES=50 -v $(pwd)/results:/app/results univesp-crawler
```

## â˜ï¸ Cloud Deployment Options

### 1. Heroku Deployment

#### Create required files:

**Procfile:**
```
worker: python univesp_crawler_simple.py
```

**runtime.txt:**
```
python-3.9.16
```

#### Deploy to Heroku:
```bash
# Install Heroku CLI first
# Login to Heroku
heroku login

# Create Heroku app
heroku create your-univesp-crawler

# Push to Heroku
git init
git add .
git commit -m "Initial commit"
git push heroku main

# Run the worker
heroku ps:scale worker=1
```

### 2. AWS EC2 Deployment

```bash
# Connect to EC2 instance
ssh -i your-key.pem ec2-user@your-ec2-ip

# Install Python and dependencies
sudo yum update -y
sudo yum install python3 python3-pip -y

# Clone or upload your code
# Upload files using scp or git clone

# Install dependencies
pip3 install -r requirements.txt

# Run the crawler
python3 univesp_crawler_simple.py

# Run in background
nohup python3 univesp_crawler_simple.py > output.log 2>&1 &
```

### 3. Google Cloud Run

#### Create cloudbuild.yaml:
```yaml
steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/univesp-crawler', '.']
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/univesp-crawler']
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['run', 'deploy', 'univesp-crawler', '--image', 'gcr.io/$PROJECT_ID/univesp-crawler', '--platform', 'managed', '--region', 'us-central1']
```

#### Deploy:
```bash
gcloud builds submit --config cloudbuild.yaml
```

## ğŸ“Š Scheduled Deployment (Automation)

### Windows Task Scheduler
1. Open Task Scheduler
2. Create Basic Task
3. Set trigger (daily/weekly)
4. Action: Start a program
5. Program: `python`
6. Arguments: `C:\Users\dell\univesp_crawler_simple.py`
7. Start in: `C:\Users\dell`

### Linux Cron Job
```bash
# Edit crontab
crontab -e

# Run daily at 2 AM
0 2 * * * cd /path/to/crawler && python3 univesp_crawler_simple.py

# Run weekly on Sunday at 1 AM
0 1 * * 0 cd /path/to/crawler && python3 univesp_crawler_simple.py
```

### GitHub Actions (CI/CD)

#### Create .github/workflows/crawler.yml:
```yaml
name: Run Univesp Crawler

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:  # Manual trigger

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run crawler
      run: python univesp_crawler_simple.py
      
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: crawler-results
        path: univesp_crawler_resultados.txt
```

## ğŸ”§ Configuration for Production

### Environment Variables Setup
```powershell
# Set environment variables (Windows)
$env:MAX_PAGES = "50"
$env:DELAY = "2.0"
$env:OUTPUT_DIR = "C:\crawler_results"

# Run with environment variables
python univesp_crawler.py
```

### Configuration File (config.py)
```python
import os

# Configuration settings
MAX_PAGES = int(os.getenv('MAX_PAGES', 25))
DELAY = float(os.getenv('DELAY', 1.5))
OUTPUT_DIR = os.getenv('OUTPUT_DIR', '.')
BASE_URL = os.getenv('BASE_URL', 'https://univesp.br')
```

## ğŸ Advanced Deployment with Python Scripts

### Batch Deployment Script (deploy.py)
```python
import subprocess
import sys
import os

def deploy_crawler():
    """Deploy the Univesp crawler with all dependencies"""
    
    print("ğŸš€ Starting Univesp Crawler Deployment...")
    
    # Check Python version
    if sys.version_info < (3, 6):
        print("âŒ Python 3.6+ required")
        return
    
    # Install dependencies
    print("ğŸ“¦ Installing dependencies...")
    subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])
    
    # Run the crawler
    print("ğŸ•·ï¸ Starting crawler...")
    subprocess.run([sys.executable, "univesp_crawler.py"])
    
    print("âœ… Deployment completed!")

if __name__ == "__main__":
    deploy_crawler()
```

### Usage:
```powershell
python deploy.py
```

## ğŸ“± Monitoring and Alerts

### Simple Monitoring Script
```python
import logging
import smtplib
from email.mime.text import MIMEText
from datetime import datetime

def setup_monitoring():
    """Setup basic monitoring and alerts"""
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('crawler.log'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def send_alert(message):
    """Send email alert when crawler completes or fails"""
    # Configure your email settings here
    pass

# Usage in your crawler
logger = setup_monitoring()
logger.info("Crawler started")
```

## ğŸ›¡ï¸ Security Considerations

### Production Security Settings
```python
# Add to your crawler configuration
SECURITY_SETTINGS = {
    'USER_AGENT': 'UnivespCrawler/1.0 (Educational Purpose)',
    'RESPECT_ROBOTS_TXT': True,
    'MAX_CONCURRENT_REQUESTS': 1,
    'DOWNLOAD_DELAY': 2,
    'RANDOMIZE_DOWNLOAD_DELAY': True,
}
```

## ğŸ¯ Deployment Recommendations

### For Development:
```powershell
python univesp_crawler_simple.py
```

### For Production:
1. Use Docker containers
2. Set up monitoring
3. Use environment variables
4. Schedule regular runs
5. Implement error handling

### For Scale:
1. Use cloud services (AWS, GCP, Azure)
2. Implement distributed crawling
3. Add database storage
4. Set up API endpoints

---

## ğŸš€ Quick Deploy Commands

### Immediate Local Run:
```powershell
cd C:\Users\dell
python univesp_crawler_simple.py
```

### With Virtual Environment:
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python univesp_crawler.py
```

### Docker Run:
```powershell
docker build -t univesp-crawler .
docker run univesp-crawler
```

Choose the deployment method that best fits your needs! ğŸ¯

# Web Crawler da Univesp ğŸ•·ï¸

Este projeto implementa um web crawler bÃ¡sico que visita sistematicamente as pÃ¡ginas Web da Univesp, seguindo hyperlinks da pÃ¡gina principal e contabilizando a frequÃªncia de cada palavra encontrada nas pÃ¡ginas visitadas.

## ğŸ“‹ Funcionalidades

- âœ… Crawling sistemÃ¡tico do site da Univesp
- âœ… Seguimento de hyperlinks internos
- âœ… ExtraÃ§Ã£o e limpeza de texto HTML
- âœ… Contagem de frequÃªncia de palavras
- âœ… Filtro de stopwords em portuguÃªs
- âœ… RelatÃ³rios detalhados de anÃ¡lise
- âœ… Salvamento de resultados em arquivo
- âœ… Interface com progresso em tempo real

## ğŸ“ Arquivos

### VersÃµes DisponÃ­veis:

1. **`univesp_crawler.py`** - VersÃ£o completa com bibliotecas externas (requests, BeautifulSoup)
2. **`univesp_crawler_simple.py`** - VersÃ£o simplificada usando apenas bibliotecas padrÃ£o do Python
3. **`Desafio_Semana_4.ipynb`** - Notebook Jupyter original com cÃ³digo base

### Arquivos Auxiliares:

- **`requirements.txt`** - DependÃªncias para a versÃ£o completa
- **`README.md`** - Este arquivo com instruÃ§Ãµes

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: VersÃ£o Simplificada (Recomendada)

Esta versÃ£o usa apenas bibliotecas padrÃ£o do Python, nÃ£o requer instalaÃ§Ã£o de dependÃªncias:

```bash
python univesp_crawler_simple.py
```

### OpÃ§Ã£o 2: VersÃ£o Completa

1. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

2. Execute o crawler:
```bash
python univesp_crawler.py
```

## âš™ï¸ ConfiguraÃ§Ãµes

VocÃª pode ajustar as configuraÃ§Ãµes do crawler editando as variÃ¡veis na funÃ§Ã£o `main()`:

```python
MAX_PAGES = 25  # NÃºmero mÃ¡ximo de pÃ¡ginas a visitar
DELAY = 1.5     # Delay entre requisiÃ§Ãµes (segundos)
```

## ğŸ“Š Resultados

O crawler gera:

1. **RelatÃ³rio no terminal** com:
   - EstatÃ­sticas gerais
   - Top 25 palavras mais frequentes
   - AnÃ¡lise por pÃ¡gina (top 5)
   - URLs com erro

2. **Arquivo de resultados** (`univesp_crawler_resultados.txt`) com:
   - Data e hora da execuÃ§Ã£o
   - Top 100 palavras mais frequentes
   - Lista completa de pÃ¡ginas visitadas
   - URLs com erro (se houver)

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### Processamento de Texto:
- Remove tags HTML, scripts e estilos
- Decodifica entidades HTML
- Normaliza texto para anÃ¡lise
- Filtra palavras com menos de 3 caracteres
- Remove stopwords comuns em portuguÃªs

### Controle de Crawling:
- Respeita apenas URLs do domÃ­nio `univesp.br`
- Ignora arquivos nÃ£o-HTML (PDF, imagens, etc.)
- Implementa delay entre requisiÃ§Ãµes
- Controle de pÃ¡ginas visitadas
- Tratamento de erros robusto

### AnÃ¡lise de Dados:
- Contagem global de frequÃªncia de palavras
- Contagem individual por pÃ¡gina
- EstatÃ­sticas detalhadas
- ExportaÃ§Ã£o de resultados

## âš ï¸ ConsideraÃ§Ãµes Importantes

1. **Uso Ã‰tico**: O crawler implementa delays entre requisiÃ§Ãµes para nÃ£o sobrecarregar o servidor
2. **LimitaÃ§Ãµes**: Por padrÃ£o, visita atÃ© 25 pÃ¡ginas para demonstraÃ§Ã£o
3. **Conectividade**: Requer conexÃ£o com internet ativa
4. **Tempo**: Dependendo da configuraÃ§Ã£o, pode levar alguns minutos para concluir

## ğŸ›‘ Interrompendo a ExecuÃ§Ã£o

Para parar o crawler durante a execuÃ§Ã£o:
- Pressione `Ctrl+C` (Windows/Linux) ou `Cmd+C` (Mac)
- O programa irÃ¡ gerar um relatÃ³rio parcial com os dados coletados atÃ© entÃ£o

## ğŸ“ˆ Exemplo de SaÃ­da

```
ğŸ•·ï¸  INICIANDO WEB CRAWLER DA UNIVESP
==================================================
URL inicial: https://univesp.br
MÃ¡ximo de pÃ¡ginas: 25
Delay entre requisiÃ§Ãµes: 1.5s

Visitando: https://univesp.br
Visitando: https://univesp.br/cursos
Progresso: 5/25 pÃ¡ginas visitadas
...

================================================================================
RELATÃ“RIO DO WEB CRAWLER DA UNIVESP
================================================================================

EstatÃ­sticas Gerais:
  â€¢ PÃ¡ginas visitadas: 25
  â€¢ PÃ¡ginas com erro: 0
  â€¢ Total de palavras Ãºnicas: 1247
  â€¢ Total de ocorrÃªncias: 8934

ğŸ“Š TOP 25 PALAVRAS MAIS FREQUENTES:
------------------------------------------------------------
 1. univesp               : 156 ocorrÃªncias
 2. curso                 : 89 ocorrÃªncias
 3. ensino                : 67 ocorrÃªncias
...
```

## ğŸ†˜ SoluÃ§Ã£o de Problemas

### Erro de conexÃ£o:
- Verifique sua conexÃ£o com a internet
- O site da Univesp pode estar temporariamente indisponÃ­vel

### Erro de importaÃ§Ã£o (versÃ£o completa):
- Execute: `pip install -r requirements.txt`
- Use a versÃ£o simplificada como alternativa

### PermissÃ£o de escrita:
- Verifique se vocÃª tem permissÃ£o para escrever na pasta atual
- Os resultados sÃ£o salvos em `C:\Users\dell\`

---

**Desenvolvido para o Desafio Semana 4 - AnÃ¡lise de FrequÃªncia de Palavras em Web Crawling**

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Crawler da Univesp - VersÃ£o Simplificada
===========================================

VersÃ£o que usa apenas bibliotecas padrÃ£o do Python.
Este programa desenvolve um web crawler bÃ¡sico que visita sistematicamente 
as pÃ¡ginas da Univesp, seguindo hyperlinks da pÃ¡gina principal. Para cada 
pÃ¡gina visitada, o programa contabiliza a frequÃªncia de cada palavra.

Autor: Desenvolvido para o Desafio Semana 4
Data: 2025
"""

import re
import time
import html
from urllib.parse import urljoin, urlparse
from html.parser import HTMLParser
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from collections import Counter, defaultdict


class TextExtractor(HTMLParser):
    """
    Extrator de texto de HTML usando HTMLParser
    """
    
    def __init__(self):
        super().__init__()
        self.text_data = []
        self.skip_tags = {'script', 'style', 'nav', 'footer', 'header'}
        self.current_tag = None
    
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag.lower()
    
    def handle_endtag(self, tag):
        self.current_tag = None
    
    def handle_data(self, data):
        if self.current_tag not in self.skip_tags:
            # Decodifica entidades HTML e limpa o texto
            cleaned_data = html.unescape(data.strip())
            if cleaned_data:
                self.text_data.append(cleaned_data)
    
    def get_text(self):
        return ' '.join(self.text_data)


class LinkCollector(HTMLParser):
    """
    Coletor de hyperlinks usando HTMLParser
    """
    
    def __init__(self, base_url):
        super().__init__()
        self.base_url = base_url
        self.links = []
    
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr_name, attr_value in attrs:
                if attr_name == 'href' and attr_value:
                    # ConstrÃ³i URL absoluto
                    absolute_url = urljoin(self.base_url, attr_value)
                    if self._is_valid_url(absolute_url):
                        self.links.append(absolute_url)
    
    def _is_valid_url(self, url):
        """Verifica se a URL Ã© vÃ¡lida para crawling"""
        try:
            parsed = urlparse(url)
            return (parsed.netloc.endswith('univesp.br') and 
                   parsed.scheme in ['http', 'https'] and
                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc', '.docx']))
        except:
            return False
    
    def get_links(self):
        return list(set(self.links))  # Remove duplicatas


class UnivespCrawlerSimple:
    """
    Classe principal do crawler da Univesp - VersÃ£o Simplificada
    """
    
    def __init__(self, base_url="https://univesp.br", max_pages=20, delay=1):
        """
        Inicializa o crawler
        
        Args:
            base_url (str): URL base da Univesp
            max_pages (int): NÃºmero mÃ¡ximo de pÃ¡ginas a visitar
            delay (float): Delay entre requisiÃ§Ãµes em segundos
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls = set()
        self.word_frequency = Counter()
        self.page_word_counts = defaultdict(Counter)
        self.failed_urls = set()
        
        # Palavras comuns portuguesas (stopwords) para filtrar
        self.stopwords = {
            'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'atÃ©', 
            'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 
            'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', 
            'esse', 'esses', 'esta', 'estÃ£o', 'estas', 'estamos', 'estar', 'este', 'estes', 
            'eu', 'foi', 'for', 'foram', 'hÃ¡', 'isso', 'isto', 'jÃ¡', 'mais', 'mas', 'me', 
            'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'nÃ£o', 'no', 'nos', 
            'nÃ³s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 
            'qual', 'quando', 'que', 'quem', 'sÃ£o', 'se', 'sem', 'ser', 'seu', 'seus', 
            'sÃ³', 'sua', 'suas', 'tambÃ©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', 
            'tuas', 'um', 'uma', 'vocÃª', 'vocÃªs', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',
            'pode', 'podem', 'vai', 'vÃ£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',
            'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'lÃ¡', 'agora', 'entÃ£o'
        }
    
    def fetch_page(self, url):
        """
        Faz o download do conteÃºdo de uma pÃ¡gina
        
        Args:
            url (str): URL da pÃ¡gina
            
        Returns:
            str: ConteÃºdo HTML da pÃ¡gina ou None se houver erro
        """
        try:
            print(f"Visitando: {url}")
            
            # Configura headers para simular navegador
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            req = Request(url, headers=headers)
            response = urlopen(req, timeout=10)
            
            # LÃª o conteÃºdo
            content = response.read()
            
            # Tenta decodificar usando diferentes encodings
            try:
                return content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    return content.decode('latin-1')
                except UnicodeDecodeError:
                    return content.decode('utf-8', errors='ignore')
            
        except Exception as e:
            print(f"Erro ao acessar {url}: {e}")
            self.failed_urls.add(url)
            return None
    
    def extract_text(self, html_content):
        """
        Extrai texto limpo do HTML
        
        Args:
            html_content (str): ConteÃºdo HTML
            
        Returns:
            str: Texto extraÃ­do
        """
        try:
            extractor = TextExtractor()
            extractor.feed(html_content)
            return extractor.get_text()
        except Exception as e:
            print(f"Erro ao extrair texto: {e}")
            return ""
    
    def extract_links(self, html_content, base_url):
        """
        Extrai links do HTML
        
        Args:
            html_content (str): ConteÃºdo HTML
            base_url (str): URL base
            
        Returns:
            list: Lista de URLs encontradas
        """
        try:
            collector = LinkCollector(base_url)
            collector.feed(html_content)
            return collector.get_links()
        except Exception as e:
            print(f"Erro ao extrair links: {e}")
            return []
    
    def process_text(self, text):
        """
        Processa texto para contar palavras
        
        Args:
            text (str): Texto a ser processado
            
        Returns:
            Counter: Contador de frequÃªncia de palavras
        """
        # Converte para minÃºsculas
        text = text.lower()
        
        # Encontra todas as palavras (incluindo acentos)
        words = re.findall(r'\b[a-zÃ¡Ã Ã¢Ã£Ã¤Ã©Ã¨ÃªÃ«Ã­Ã¬Ã®Ã¯Ã³Ã²Ã´ÃµÃ¶ÃºÃ¹Ã»Ã¼Ã§Ã±]+\b', text)
        
        # Filtra palavras muito curtas e stopwords
        filtered_words = [word for word in words 
                         if len(word) > 2 and word not in self.stopwords]
        
        return Counter(filtered_words)
    
    def crawl_page(self, url):
        """
        Processa uma pÃ¡gina especÃ­fica
        
        Args:
            url (str): URL da pÃ¡gina
            
        Returns:
            list: Lista de novos links encontrados
        """
        # Faz download da pÃ¡gina
        html_content = self.fetch_page(url)
        if not html_content:
            return []
        
        # Extrai e processa texto
        text = self.extract_text(html_content)
        if text:
            word_count = self.process_text(text)
            self.page_word_counts[url] = word_count
            self.word_frequency.update(word_count)
        
        # Extrai links
        links = self.extract_links(html_content, url)
        return links
    
    def crawl(self):
        """
        Executa o processo principal de crawling
        """
        print("ğŸ•·ï¸  INICIANDO WEB CRAWLER DA UNIVESP")
        print("="*50)
        print(f"URL inicial: {self.base_url}")
        print(f"MÃ¡ximo de pÃ¡ginas: {self.max_pages}")
        print(f"Delay entre requisiÃ§Ãµes: {self.delay}s")
        print("")
        
        urls_to_visit = [self.base_url]
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            current_url = urls_to_visit.pop(0)
            
            # Evita URLs jÃ¡ visitadas
            if current_url in self.visited_urls:
                continue
            
            self.visited_urls.add(current_url)
            
            # Processa a pÃ¡gina atual
            new_links = self.crawl_page(current_url)
            
            # Adiciona novos links Ã  fila
            for link in new_links:
                if (link not in self.visited_urls and 
                    link not in urls_to_visit and 
                    link not in self.failed_urls):
                    urls_to_visit.append(link)
            
            # Status do progresso
            if len(self.visited_urls) % 5 == 0:
                print(f"Progresso: {len(self.visited_urls)}/{self.max_pages} pÃ¡ginas visitadas")
            
            # Delay entre requisiÃ§Ãµes
            time.sleep(self.delay)
        
        print(f"\nCrawling concluÃ­do!")
        print(f"PÃ¡ginas visitadas: {len(self.visited_urls)}")
        print(f"PÃ¡ginas com erro: {len(self.failed_urls)}")
    
    def generate_report(self):
        """
        Gera e exibe relatÃ³rio completo
        """
        print("\n" + "="*80)
        print("RELATÃ“RIO DO WEB CRAWLER DA UNIVESP")
        print("="*80)
        
        print(f"\nEstatÃ­sticas Gerais:")
        print(f"  â€¢ PÃ¡ginas visitadas: {len(self.visited_urls)}")
        print(f"  â€¢ PÃ¡ginas com erro: {len(self.failed_urls)}")
        print(f"  â€¢ Total de palavras Ãºnicas: {len(self.word_frequency)}")
        print(f"  â€¢ Total de ocorrÃªncias: {sum(self.word_frequency.values())}")
        
        if self.word_frequency:
            print(f"\nğŸ“Š TOP 25 PALAVRAS MAIS FREQUENTES:")
            print("-" * 60)
            for i, (word, count) in enumerate(self.word_frequency.most_common(25), 1):
                print(f"{i:2d}. {word:<25} : {count:>4} ocorrÃªncias")
            
            print(f"\nğŸ“ˆ ANÃLISE POR PÃGINA (Top 5):")
            print("-" * 60)
            for i, url in enumerate(list(self.page_word_counts.keys())[:5], 1):
                word_count = self.page_word_counts[url]
                total_words = sum(word_count.values())
                top_word = word_count.most_common(1)[0] if word_count else ("N/A", 0)
                print(f"{i}. {url}")
                print(f"   â€¢ Total de palavras: {total_words}")
                print(f"   â€¢ Palavra mais frequente: '{top_word[0]}' ({top_word[1]} vezes)")
                print()
        
        if self.failed_urls:
            print(f"âŒ URLs COM ERRO:")
            print("-" * 60)
            for i, url in enumerate(list(self.failed_urls)[:10], 1):
                print(f"{i:2d}. {url}")
    
    def save_results(self, filename="univesp_crawler_resultados.txt"):
        """
        Salva os resultados em arquivo
        
        Args:
            filename (str): Nome do arquivo para salvar
        """
        try:
            filepath = f"C:\\Users\\dell\\{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("RESULTADOS DO WEB CRAWLER DA UNIVESP\n")
                f.write("="*60 + "\n\n")
                
                f.write(f"Data da execuÃ§Ã£o: {time.strftime('%d/%m/%Y %H:%M:%S')}\n")
                f.write(f"PÃ¡ginas visitadas: {len(self.visited_urls)}\n")
                f.write(f"PÃ¡ginas com erro: {len(self.failed_urls)}\n")
                f.write(f"Total de palavras Ãºnicas: {len(self.word_frequency)}\n")
                f.write(f"Total de ocorrÃªncias: {sum(self.word_frequency.values())}\n\n")
                
                f.write("TOP 100 PALAVRAS MAIS FREQUENTES:\n")
                f.write("-" * 50 + "\n")
                for i, (word, count) in enumerate(self.word_frequency.most_common(100), 1):
                    f.write(f"{i:3d}. {word:<30} : {count:>5} ocorrÃªncias\n")
                
                f.write(f"\n\nPÃGINAS VISITADAS:\n")
                f.write("-" * 50 + "\n")
                for i, url in enumerate(self.visited_urls, 1):
                    f.write(f"{i:3d}. {url}\n")
                
                if self.failed_urls:
                    f.write(f"\n\nPÃGINAS COM ERRO:\n")
                    f.write("-" * 50 + "\n")
                    for i, url in enumerate(self.failed_urls, 1):
                        f.write(f"{i:3d}. {url}\n")
            
            print(f"\nğŸ’¾ Resultados salvos em: {filepath}")
        except Exception as e:
            print(f"Erro ao salvar resultados: {e}")


def main():
    """
    FunÃ§Ã£o principal do programa
    """
    # ConfiguraÃ§Ãµes do crawler
    MAX_PAGES = 25  # Limite de pÃ¡ginas
    DELAY = 1.5     # Delay entre requisiÃ§Ãµes
    
    # Cria o crawler
    crawler = UnivespCrawlerSimple(max_pages=MAX_PAGES, delay=DELAY)
    
    try:
        # Executa o crawling
        start_time = time.time()
        crawler.crawl()
        end_time = time.time()
        
        # Gera relatÃ³rio
        crawler.generate_report()
        
        # Salva resultados
        crawler.save_results()
        
        print(f"\nâ±ï¸  Tempo total de execuÃ§Ã£o: {end_time - start_time:.2f} segundos")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Processo interrompido pelo usuÃ¡rio")
        if len(crawler.visited_urls) > 0:
            print("Gerando relatÃ³rio parcial...")
            crawler.generate_report()
            crawler.save_results("univesp_crawler_resultados_parciais.txt")
    
    except Exception as e:
        print(f"\nâŒ Erro durante execuÃ§Ã£o: {e}")
        if len(crawler.visited_urls) > 0:
            crawler.generate_report()


if __name__ == "__main__":
    main()

requests==2.31.0
beautifulsoup4==4.12.2
urllib3==2.0.4

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Crawler da Univesp com AnÃ¡lise de FrequÃªncia de Palavras
===========================================================

Este programa desenvolve um web crawler bÃ¡sico que visita sistematicamente 
as pÃ¡ginas da Univesp, seguindo hyperlinks da pÃ¡gina principal. Para cada 
pÃ¡gina visitada, o programa contabiliza a frequÃªncia de cada palavra.

Autor: Desenvolvido para o Desafio Semana 4
Data: 2025
"""

import re
import time
from urllib.parse import urljoin, urlparse
from html.parser import HTMLParser
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from collections import Counter, defaultdict
from bs4 import BeautifulSoup
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UnivespCrawler:
    """
    Classe principal do crawler da Univesp que coleta links e analisa texto
    """
    
    def __init__(self, base_url="https://univesp.br", max_pages=50, delay=1):
        """
        Inicializa o crawler
        
        Args:
            base_url (str): URL base da Univesp
            max_pages (int): NÃºmero mÃ¡ximo de pÃ¡ginas a visitar
            delay (float): Delay entre requisiÃ§Ãµes em segundos
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls = set()
        self.word_frequency = Counter()
        self.page_word_counts = defaultdict(Counter)
        self.failed_urls = set()
        
        # ConfiguraÃ§Ã£o da sessÃ£o HTTP com retry
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers para simular um navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # Palavras comuns portuguesas (stopwords) para filtrar
        self.stopwords = {
            'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'atÃ©', 
            'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 
            'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', 
            'esse', 'esses', 'esta', 'estÃ£o', 'estas', 'estamos', 'estar', 'este', 'estes', 
            'eu', 'foi', 'for', 'foram', 'hÃ¡', 'isso', 'isto', 'jÃ¡', 'mais', 'mas', 'me', 
            'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'nÃ£o', 'no', 'nos', 
            'nÃ³s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 
            'qual', 'quando', 'que', 'quem', 'sÃ£o', 'se', 'sem', 'ser', 'seu', 'seus', 
            'sÃ³', 'sua', 'suas', 'tambÃ©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', 
            'tuas', 'um', 'uma', 'vocÃª', 'vocÃªs', 'vos'
        }
    
    def is_valid_url(self, url):
        """
        Verifica se a URL Ã© vÃ¡lida e pertence ao domÃ­nio da Univesp
        
        Args:
            url (str): URL a ser verificada
            
        Returns:
            bool: True se a URL Ã© vÃ¡lida
        """
        try:
            parsed = urlparse(url)
            return (parsed.netloc.endswith('univesp.br') and 
                   parsed.scheme in ['http', 'https'] and
                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))
        except:
            return False
    
    def extract_text_from_html(self, html_content):
        """
        Extrai texto limpo do HTML
        
        Args:
            html_content (str): ConteÃºdo HTML da pÃ¡gina
            
        Returns:
            str: Texto limpo extraÃ­do
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove scripts e estilos
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # Extrai texto
            text = soup.get_text()
            
            # Limpa o texto
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
        except Exception as e:
            logger.error(f"Erro ao extrair texto do HTML: {e}")
            return ""
    
    def extract_links(self, html_content, base_url):
        """
        Extrai todos os links de uma pÃ¡gina HTML
        
        Args:
            html_content (str): ConteÃºdo HTML da pÃ¡gina
            base_url (str): URL base para resolver links relativos
            
        Returns:
            list: Lista de URLs absolutas encontradas
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(base_url, href)
                
                if self.is_valid_url(absolute_url):
                    links.append(absolute_url)
            
            return links
        except Exception as e:
            logger.error(f"Erro ao extrair links: {e}")
            return []
    
    def process_text(self, text):
        """
        Processa o texto extraÃ­do para contar palavras
        
        Args:
            text (str): Texto a ser processado
            
        Returns:
            Counter: Contador de frequÃªncia de palavras
        """
        # Converte para minÃºsculas e remove pontuaÃ§Ã£o
        text = text.lower()
        words = re.findall(r'\b[a-zÃ¡Ã Ã¢Ã£Ã¤Ã©Ã¨ÃªÃ«Ã­Ã¬Ã®Ã¯Ã³Ã²Ã´ÃµÃ¶ÃºÃ¹Ã»Ã¼Ã§]+\b', text)
        
        # Filtra palavras muito curtas e stopwords
        filtered_words = [word for word in words 
                         if len(word) > 2 and word not in self.stopwords]
        
        return Counter(filtered_words)
    
    def crawl_page(self, url):
        """
        Faz o crawl de uma pÃ¡gina especÃ­fica
        
        Args:
            url (str): URL da pÃ¡gina a ser processada
            
        Returns:
            list: Lista de links encontrados na pÃ¡gina
        """
        try:
            logger.info(f"Visitando: {url}")
            
            response = self.session.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # Extrai texto e conta palavras
            text = self.extract_text_from_html(response.text)
            if text:
                word_count = self.process_text(text)
                self.page_word_counts[url] = word_count
                self.word_frequency.update(word_count)
            
            # Extrai links
            links = self.extract_links(response.text, url)
            
            return links
            
        except Exception as e:
            logger.error(f"Erro ao processar {url}: {e}")
            self.failed_urls.add(url)
            return []
    
    def crawl(self):
        """
        Executa o processo principal de crawling
        """
        logger.info(f"Iniciando crawler da Univesp - MÃ¡ximo {self.max_pages} pÃ¡ginas")
        
        urls_to_visit = [self.base_url]
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
            
            self.visited_urls.add(current_url)
            
            # Processa a pÃ¡gina atual
            new_links = self.crawl_page(current_url)
            
            # Adiciona novos links Ã  lista de URLs a visitar
            for link in new_links:
                if (link not in self.visited_urls and 
                    link not in urls_to_visit and 
                    link not in self.failed_urls):
                    urls_to_visit.append(link)
            
            # Delay entre requisiÃ§Ãµes
            time.sleep(self.delay)
        
        logger.info(f"Crawling concluÃ­do. Visitadas {len(self.visited_urls)} pÃ¡ginas")
    
    def generate_report(self):
        """
        Gera relatÃ³rio com as estatÃ­sticas coletadas
        """
        print("\n" + "="*80)
        print("RELATÃ“RIO DO WEB CRAWLER DA UNIVESP")
        print("="*80)
        
        print(f"\nEstatÃ­sticas Gerais:")
        print(f"  â€¢ PÃ¡ginas visitadas: {len(self.visited_urls)}")
        print(f"  â€¢ PÃ¡ginas com erro: {len(self.failed_urls)}")
        print(f"  â€¢ Total de palavras Ãºnicas: {len(self.word_frequency)}")
        print(f"  â€¢ Total de ocorrÃªncias: {sum(self.word_frequency.values())}")
        
        print(f"\nğŸ“Š TOP 20 PALAVRAS MAIS FREQUENTES:")
        print("-" * 50)
        for i, (word, count) in enumerate(self.word_frequency.most_common(20), 1):
            print(f"{i:2d}. {word:<20} : {count:>4} ocorrÃªncias")
        
        print(f"\nğŸ“ˆ ANÃLISE POR PÃGINA (Top 5):")
        print("-" * 50)
        for url in list(self.page_word_counts.keys())[:5]:
            word_count = self.page_word_counts[url]
            total_words = sum(word_count.values())
            top_word = word_count.most_common(1)[0] if word_count else ("N/A", 0)
            print(f"URL: {url}")
            print(f"  â€¢ Total de palavras: {total_words}")
            print(f"  â€¢ Palavra mais frequente: '{top_word[0]}' ({top_word[1]} vezes)")
            print()
        
        if self.failed_urls:
            print(f"\nâŒ PÃGINAS COM ERRO:")
            print("-" * 50)
            for url in list(self.failed_urls)[:10]:  # Mostra apenas as primeiras 10
                print(f"  â€¢ {url}")
    
    def save_results(self, filename="univesp_crawler_results.txt"):
        """
        Salva os resultados em um arquivo de texto
        
        Args:
            filename (str): Nome do arquivo para salvar
        """
        try:
            filepath = f"C:\\Users\\dell\\{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("RESULTADOS DO WEB CRAWLER DA UNIVESP\n")
                f.write("="*50 + "\n\n")
                
                f.write(f"PÃ¡ginas visitadas: {len(self.visited_urls)}\n")
                f.write(f"Total de palavras Ãºnicas: {len(self.word_frequency)}\n")
                f.write(f"Total de ocorrÃªncias: {sum(self.word_frequency.values())}\n\n")
                
                f.write("TOP 50 PALAVRAS MAIS FREQUENTES:\n")
                f.write("-" * 40 + "\n")
                for i, (word, count) in enumerate(self.word_frequency.most_common(50), 1):
                    f.write(f"{i:2d}. {word:<25} : {count:>5} ocorrÃªncias\n")
                
                f.write(f"\n\nPÃGINAS VISITADAS:\n")
                f.write("-" * 40 + "\n")
                for i, url in enumerate(self.visited_urls, 1):
                    f.write(f"{i:2d}. {url}\n")
            
            logger.info(f"Resultados salvos em: {filepath}")
        except Exception as e:
            logger.error(f"Erro ao salvar resultados: {e}")


def main():
    """
    FunÃ§Ã£o principal do programa
    """
    print("ğŸ•·ï¸  INICIANDO WEB CRAWLER DA UNIVESP")
    print("="*50)
    
    # ConfiguraÃ§Ãµes do crawler
    MAX_PAGES = 30  # Limite de pÃ¡ginas para evitar sobrecarga
    DELAY = 1       # Delay de 1 segundo entre requisiÃ§Ãµes
    
    # Cria e executa o crawler
    crawler = UnivespCrawler(max_pages=MAX_PAGES, delay=DELAY)
    
    try:
        # Executa o crawling
        crawler.crawl()
        
        # Gera relatÃ³rio
        crawler.generate_report()
        
        # Salva resultados
        crawler.save_results()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Processo interrompido pelo usuÃ¡rio")
        if len(crawler.visited_urls) > 0:
            print("Gerando relatÃ³rio com dados coletados atÃ© agora...")
            crawler.generate_report()
            crawler.save_results("univesp_crawler_partial_results.txt")
    
    except Exception as e:
        logger.error(f"Erro durante execuÃ§Ã£o: {e}")


if __name__ == "__main__":
    main()
