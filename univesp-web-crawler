{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMrKw8dCQVxYbNZXz5cJ8mR"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "titulo_principal"
   },
   "source": [
    "# üï∑Ô∏è Web Crawler da Univesp com An√°lise de Frequ√™ncia de Palavras\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/SEU_USUARIO/SEU_REPOSITORIO)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## üìã Descri√ß√£o do Projeto\n",
    "\n",
    "Este notebook implementa um **web crawler** que visita sistematicamente as p√°ginas da Univesp, seguindo hyperlinks e analisando a frequ√™ncia de palavras encontradas. O projeto foi desenvolvido como parte do **Desafio Semana 4** e demonstra t√©cnicas de:\n",
    "\n",
    "- üåê **Web Scraping** e crawling sistem√°tico\n",
    "- üìä **An√°lise de texto** e processamento de linguagem natural\n",
    "- üìà **Visualiza√ß√£o de dados** com gr√°ficos interativos\n",
    "- üêç **Programa√ß√£o Python** com bibliotecas especializadas\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objetivos\n",
    "1. Implementar um crawler que navega automaticamente pelo site da Univesp\n",
    "2. Extrair e limpar texto de p√°ginas HTML\n",
    "3. Analisar frequ√™ncia de palavras com filtros inteligentes\n",
    "4. Gerar visualiza√ß√µes e relat√≥rios detalhados\n",
    "5. Criar uma ferramenta reutiliz√°vel e bem documentada\n",
    "\n",
    "### üöÄ Como Usar Este Notebook\n",
    "1. **Execute as c√©lulas sequencialmente** usando `Shift + Enter`\n",
    "2. **Ajuste os par√¢metros** na se√ß√£o de configura√ß√£o conforme necess√°rio\n",
    "3. **Visualize os resultados** nas se√ß√µes de an√°lise e gr√°ficos\n",
    "4. **Baixe os dados** gerados na se√ß√£o final\n",
    "\n",
    "### üìñ Estrutura do Notebook\n",
    "- **Se√ß√£o 1:** Configura√ß√£o e Instala√ß√£o de Depend√™ncias\n",
    "- **Se√ß√£o 2:** Implementa√ß√£o do Web Crawler\n",
    "- **Se√ß√£o 3:** Processamento e An√°lise de Texto\n",
    "- **Se√ß√£o 4:** Visualiza√ß√µes e Gr√°ficos\n",
    "- **Se√ß√£o 5:** Relat√≥rios e Exporta√ß√£o de Dados\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Desenvolvido para o Desafio Semana 4 - Univesp  \n",
    "**Data:** 2025  \n",
    "**Vers√£o:** 1.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_instalacao"
   },
   "source": [
    "# 1Ô∏è‚É£ Configura√ß√£o e Instala√ß√£o de Depend√™ncias\n",
    "\n",
    "Nesta se√ß√£o, vamos instalar e importar todas as bibliotecas necess√°rias para o funcionamento do web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Instala√ß√£o de bibliotecas necess√°rias\n",
    "!pip install requests beautifulsoup4 matplotlib seaborn wordcloud plotly pandas --quiet\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# üìö Importa√ß√£o de bibliotecas\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "import unicodedata\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas para web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Bibliotecas para an√°lise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configura√ß√µes visuais\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"üêç Python executando no Google Colab\")\n",
    "print(f\"‚è∞ Notebook iniciado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_configuracao"
   },
   "source": [
    "## ‚öôÔ∏è Configura√ß√£o de Par√¢metros\n",
    "\n",
    "Aqui voc√™ pode ajustar os par√¢metros do crawler conforme suas necessidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuracao_parametros"
   },
   "outputs": [],
   "source": [
    "# üéõÔ∏è CONFIGURA√á√ïES DO WEB CRAWLER\n",
    "# ================================\n",
    "\n",
    "# URL base da Univesp\n",
    "BASE_URL = \"https://univesp.br\"\n",
    "\n",
    "# N√∫mero m√°ximo de p√°ginas a visitar\n",
    "MAX_PAGES = 20  # Reduzido para o Colab (pode aumentar se necess√°rio)\n",
    "\n",
    "# Delay entre requisi√ß√µes (em segundos)\n",
    "DELAY = 1.0  # Seja respeitoso com o servidor!\n",
    "\n",
    "# Timeout para requisi√ß√µes (em segundos)\n",
    "TIMEOUT = 10\n",
    "\n",
    "# Headers para simular um navegador real\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Lista de stopwords em portugu√™s\n",
    "STOPWORDS = {\n",
    "    'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'at√©', \n",
    "    'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', \n",
    "    'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', \n",
    "    'esse', 'esses', 'esta', 'est√£o', 'estas', 'estamos', 'estar', 'este', 'estes', \n",
    "    'eu', 'foi', 'for', 'foram', 'h√°', 'isso', 'isto', 'j√°', 'mais', 'mas', 'me', \n",
    "    'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'n√£o', 'no', 'nos', \n",
    "    'n√≥s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', \n",
    "    'qual', 'quando', 'que', 'quem', 's√£o', 'se', 'sem', 'ser', 'seu', 'seus', \n",
    "    's√≥', 'sua', 'suas', 'tamb√©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', \n",
    "    'tuas', 'um', 'uma', 'voc√™', 'voc√™s', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',\n",
    "    'pode', 'podem', 'vai', 'v√£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',\n",
    "    'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'l√°', 'agora', 'ent√£o',\n",
    "    'sobre', 'ap√≥s', 'durante', 'antes', 'depois', 'enquanto', 'desde', 'ser√°',\n",
    "    'ser√£o', 'est√°', 'est√£o', 'foi', 'foram', 'ter√°', 'ter√£o', 'at√©', 'atrav√©s',\n",
    "    'al√©m', 'tamb√©m', 'por√©m', 'contudo', 'todavia', 'entretanto', 'portanto',\n",
    "    'assim', 'ent√£o', 'logo', 'pois', 'porque', 'porqu√™', 'quando', 'quanto',\n",
    "    'qualquer', 'quaisquer', 'algum', 'alguns', 'alguma', 'algumas', 'nenhum',\n",
    "    'nenhuma', 'outro', 'outra', 'outros', 'outras', 'mesmo', 'mesma', 'mesmos',\n",
    "    'mesmas', 'tanto', 'tanta', 'tantos', 'tantas', 'quanto', 'quanta', 'quantos',\n",
    "    'quantas', 'tal', 'tais', 'cada', 'qualquer', 'seja', 'sejam', 'fosse',\n",
    "    'fossem', 'sendo', 'tendo', 'havendo', 'haver', 'ter', 'fazer', 'dizer',\n",
    "    'dar', 'ficar', 'ir', 'vir', 'sair', 'chegar', 'voltar', 'passar', 'levar'\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√µes do Crawler:\")\n",
    "print(f\"   üåê URL Base: {BASE_URL}\")\n",
    "print(f\"   üìÑ M√°ximo de p√°ginas: {MAX_PAGES}\")\n",
    "print(f\"   ‚è±Ô∏è Delay entre requisi√ß√µes: {DELAY}s\")\n",
    "print(f\"   üö´ Stopwords: {len(STOPWORDS)} palavras filtradas\")\n",
    "print(f\"\\n‚úÖ Configura√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_crawler"
   },
   "source": [
    "# 2Ô∏è‚É£ Implementa√ß√£o do Web Crawler\n",
    "\n",
    "Vamos criar nossa classe principal do web crawler com todas as funcionalidades necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crawler_class"
   },
   "outputs": [],
   "source": [
    "class UnivespWebCrawler:\n",
    "    \"\"\"\n",
    "    üï∑Ô∏è Web Crawler da Univesp com An√°lise de Frequ√™ncia de Palavras\n",
    "    \n",
    "    Esta classe implementa um crawler completo que:\n",
    "    - Visita p√°ginas da Univesp sistematicamente\n",
    "    - Extrai e processa texto de cada p√°gina\n",
    "    - Analisa frequ√™ncia de palavras\n",
    "    - Gera estat√≠sticas detalhadas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=BASE_URL, max_pages=MAX_PAGES, delay=DELAY):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.visited_urls = set()\n",
    "        self.failed_urls = set()\n",
    "        self.word_frequency = Counter()\n",
    "        self.page_word_counts = defaultdict(Counter)\n",
    "        self.crawl_data = []\n",
    "        \n",
    "        # Configurar sess√£o HTTP com retry\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Verifica se a URL √© v√°lida e pertence ao dom√≠nio da Univesp\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return (parsed.netloc.endswith('univesp.br') and \n",
    "                   parsed.scheme in ['http', 'https'] and\n",
    "                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Faz o download do conte√∫do de uma p√°gina\"\"\"\n",
    "        try:\n",
    "            print(f\"üîó Visitando: {url}\")\n",
    "            response = self.session.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Garantir codifica√ß√£o UTF-8\n",
    "            if response.encoding is None or response.encoding.lower() not in ['utf-8', 'utf8']:\n",
    "                response.encoding = 'utf-8'\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao acessar {url}: {e}\")\n",
    "            self.failed_urls.add(url)\n",
    "            return None\n",
    "    \n",
    "    def extract_text_and_links(self, html_content, base_url):\n",
    "        \"\"\"Extrai texto limpo e links de uma p√°gina HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Remove scripts, estilos e elementos desnecess√°rios\n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extrai texto\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Limpa o texto\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Extrai links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(base_url, link['href'])\n",
    "                if self.is_valid_url(absolute_url):\n",
    "                    links.append(absolute_url)\n",
    "            \n",
    "            return clean_text, list(set(links))  # Remove duplicatas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar HTML: {e}\")\n",
    "            return \"\", []\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Processa texto para contar palavras\"\"\"\n",
    "        if not text:\n",
    "            return Counter()\n",
    "        \n",
    "        # Normalizar Unicode (resolver problemas de codifica√ß√£o)\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Converte para min√∫sculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Encontra palavras (incluindo acentos portugueses)\n",
    "        words = re.findall(r'\\b[a-zA-Z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë]+\\b', text)\n",
    "        \n",
    "        # Filtra palavras muito curtas e stopwords\n",
    "        filtered_words = [word for word in words \n",
    "                         if len(word) > 2 and word not in STOPWORDS]\n",
    "        \n",
    "        return Counter(filtered_words)\n",
    "    \n",
    "    def crawl_page(self, url):\n",
    "        \"\"\"Processa uma p√°gina espec√≠fica\"\"\"\n",
    "        # Download da p√°gina\n",
    "        html_content = self.fetch_page(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "        \n",
    "        # Extrai texto e links\n",
    "        text, links = self.extract_text_and_links(html_content, url)\n",
    "        \n",
    "        # Processa texto\n",
    "        if text:\n",
    "            word_count = self.process_text(text)\n",
    "            self.page_word_counts[url] = word_count\n",
    "            self.word_frequency.update(word_count)\n",
    "            \n",
    "            # Armazena dados para an√°lise posterior\n",
    "            self.crawl_data.append({\n",
    "                'url': url,\n",
    "                'text_length': len(text),\n",
    "                'word_count': sum(word_count.values()),\n",
    "                'unique_words': len(word_count),\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Executa o processo principal de crawling\"\"\"\n",
    "        print(\"üï∑Ô∏è INICIANDO WEB CRAWLER DA UNIVESP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üåê URL inicial: {self.base_url}\")\n",
    "        print(f\"üìÑ M√°ximo de p√°ginas: {self.max_pages}\")\n",
    "        print(f\"‚è±Ô∏è Delay: {self.delay}s\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        urls_to_visit = [self.base_url]\n",
    "        \n",
    "        while urls_to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            current_url = urls_to_visit.pop(0)\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            self.visited_urls.add(current_url)\n",
    "            \n",
    "            # Processa a p√°gina\n",
    "            new_links = self.crawl_page(current_url)\n",
    "            \n",
    "            # Adiciona novos links\n",
    "            for link in new_links:\n",
    "                if (link not in self.visited_urls and \n",
    "                    link not in urls_to_visit and \n",
    "                    link not in self.failed_urls):\n",
    "                    urls_to_visit.append(link)\n",
    "            \n",
    "            # Progresso\n",
    "            if len(self.visited_urls) % 5 == 0:\n",
    "                print(f\"üìà Progresso: {len(self.visited_urls)}/{self.max_pages} p√°ginas\")\n",
    "            \n",
    "            # Delay\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Crawling conclu√≠do!\")\n",
    "        print(f\"üìä P√°ginas visitadas: {len(self.visited_urls)}\")\n",
    "        print(f\"‚ùå P√°ginas com erro: {len(self.failed_urls)}\")\n",
    "        print(f\"‚è±Ô∏è Tempo total: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"üìù Palavras √∫nicas encontradas: {len(self.word_frequency)}\")\n",
    "        print(f\"üî§ Total de ocorr√™ncias: {sum(self.word_frequency.values())}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Retorna os dados do crawl como DataFrame do pandas\"\"\"\n",
    "        return pd.DataFrame(self.crawl_data)\n",
    "    \n",
    "    def get_word_frequency_df(self, top_n=100):\n",
    "        \"\"\"Retorna as palavras mais frequentes como DataFrame\"\"\"\n",
    "        top_words = self.word_frequency.most_common(top_n)\n",
    "        return pd.DataFrame(top_words, columns=['palavra', 'frequencia'])\n",
    "\n",
    "print(\"‚úÖ Classe UnivespWebCrawler criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_execucao"
   },
   "source": [
    "## üöÄ Executando o Web Crawler\n",
    "\n",
    "Agora vamos criar uma inst√¢ncia do crawler e execut√°-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "executar_crawler",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# üï∑Ô∏è Criar e executar o crawler\n",
    "crawler = UnivespWebCrawler(\n",
    "    base_url=BASE_URL,\n",
    "    max_pages=MAX_PAGES,\n",
    "    delay=DELAY\n",
    ")\n",
    "\n",
    "# Executar o crawling\n",
    "sucesso = crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise"
   },
   "source": [
    "# 3Ô∏è‚É£ An√°lise dos Dados Coletados\n",
    "\n",
    "Agora vamos analisar os dados coletados pelo nosso crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_basica"
   },
   "outputs": [],
   "source": [
    "# üìä An√°lise b√°sica dos dados\n",
    "if sucesso:\n",
    "    print(\"üìà AN√ÅLISE DOS DADOS COLETADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    df_pages = crawler.get_dataframe()\n",
    "    df_words = crawler.get_word_frequency_df(50)\n",
    "    \n",
    "    print(f\"\\nüîç Estat√≠sticas Gerais:\")\n",
    "    print(f\"   üìÑ Total de p√°ginas processadas: {len(df_pages)}\")\n",
    "    print(f\"   üî§ Palavras √∫nicas encontradas: {len(crawler.word_frequency)}\")\n",
    "    print(f\"   üìä Total de ocorr√™ncias de palavras: {sum(crawler.word_frequency.values())}\")\n",
    "    print(f\"   üìù M√©dia de palavras por p√°gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"   üìè M√©dia de caracteres por p√°gina: {df_pages['text_length'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 15 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, (palavra, freq) in enumerate(df_words.head(15).values, 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorr√™ncias\")\n",
    "    \n",
    "    # Criar DataFrames para uso posterior\n",
    "    print(f\"\\n‚úÖ DataFrames criados:\")\n",
    "    print(f\"   üìä df_pages: {len(df_pages)} linhas (dados por p√°gina)\")\n",
    "    print(f\"   üî§ df_words: {len(df_words)} linhas (frequ√™ncia de palavras)\")\nelse:\n",
    "    print(\"‚ùå Erro durante o crawling. Verificar logs acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detalhes_paginas"
   },
   "outputs": [],
   "source": [
    "# üìã Detalhes das p√°ginas coletadas\n",
    "if not df_pages.empty:\n",
    "    print(\"üìÑ DETALHES DAS P√ÅGINAS COLETADAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Exibir as primeiras p√°ginas\n",
    "    display(df_pages.head())\n",
    "    \n",
    "    print(\"\\nüìä Estat√≠sticas descritivas:\")\n",
    "    display(df_pages[['text_length', 'word_count', 'unique_words']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_visualizacoes"
   },
   "source": [
    "# 4Ô∏è‚É£ Visualiza√ß√µes e Gr√°ficos\n",
    "\n",
    "Vamos criar visualiza√ß√µes interessantes dos dados coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wordcloud"
   },
   "outputs": [],
   "source": [
    "# üåü Word Cloud das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"‚òÅÔ∏è Criando Word Cloud...\")\n",
    "    \n",
    "    # Preparar dados para word cloud\n",
    "    word_freq_dict = dict(zip(df_words['palavra'], df_words['frequencia']))\n",
    "    \n",
    "    # Criar word cloud\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, height=600,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        min_font_size=10\n",
    "    ).generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('‚òÅÔ∏è Nuvem de Palavras - Site da Univesp', fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Word Cloud criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grafico_barras"
   },
   "outputs": [],
   "source": [
    "# üìä Gr√°fico de barras das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"üìä Criando gr√°fico de barras...\")\n",
    "    \n",
    "    # Top 20 palavras\n",
    "    top_20 = df_words.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Criar gr√°fico de barras\n",
    "    bars = plt.bar(range(len(top_20)), top_20['frequencia'], \n",
    "                   color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))\n",
    "    \n",
    "    plt.xlabel('Palavras', fontsize=12)\n",
    "    plt.ylabel('Frequ√™ncia', fontsize=12)\n",
    "    plt.title('üìä Top 20 Palavras Mais Frequentes - Site da Univesp', fontsize=16, pad=20)\n",
    "    plt.xticks(range(len(top_20)), top_20['palavra'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°fico de barras criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graficos_interativos"
   },
   "outputs": [],
   "source": [
    "# üìà Gr√°ficos interativos com Plotly\n",
    "if not df_words.empty and not df_pages.empty:\n",
    "    print(\"üé® Criando gr√°ficos interativos...\")\n",
    "    \n",
    "    # 1. Gr√°fico interativo de barras das palavras mais frequentes\n",
    "    top_25 = df_words.head(25)\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        top_25, \n",
    "        x='palavra', \n",
    "        y='frequencia',\n",
    "        title='üèÜ Top 25 Palavras Mais Frequentes (Interativo)',\n",
    "        labels={'palavra': 'Palavra', 'frequencia': 'Frequ√™ncia'},\n",
    "        color='frequencia',\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig1.update_layout(xaxis_tickangle=-45, height=600)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Gr√°fico de distribui√ß√£o de palavras por p√°gina\n",
    "    fig2 = px.scatter(\n",
    "        df_pages, \n",
    "        x='text_length', \n",
    "        y='word_count',\n",
    "        size='unique_words',\n",
    "        hover_data=['url'],\n",
    "        title='üìä Distribui√ß√£o: Tamanho do Texto vs Contagem de Palavras',\n",
    "        labels={\n",
    "            'text_length': 'Tamanho do Texto (caracteres)',\n",
    "            'word_count': 'N√∫mero de Palavras',\n",
    "            'unique_words': 'Palavras √önicas'\n",
    "        }\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. Gr√°fico de pizza das categorias de palavras\n",
    "    # Categorizar palavras por frequ√™ncia\n",
    "    categories = []\n",
    "    for freq in df_words['frequencia']:\n",
    "        if freq >= 100:\n",
    "            categories.append('Muito Frequente (‚â•100)')\n",
    "        elif freq >= 50:\n",
    "            categories.append('Frequente (50-99)')\n",
    "        elif freq >= 20:\n",
    "            categories.append('Moderada (20-49)')\n",
    "        elif freq >= 10:\n",
    "            categories.append('Baixa (10-19)')\n",
    "        else:\n",
    "            categories.append('Rara (<10)')\n",
    "    \n",
    "    df_words['categoria'] = categories\n",
    "    category_counts = df_words['categoria'].value_counts()\n",
    "    \n",
    "    fig3 = px.pie(\n",
    "        values=category_counts.values,\n",
    "        names=category_counts.index,\n",
    "        title='ü•ß Distribui√ß√£o de Palavras por Categoria de Frequ√™ncia'\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos interativos criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise_avancada"
   },
   "source": [
    "# 5Ô∏è‚É£ An√°lise Avan√ßada e Insights\n",
    "\n",
    "Vamos fazer algumas an√°lises mais detalhadas dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_correlacoes"
   },
   "outputs": [],
   "source": [
    "# üîç An√°lise de correla√ß√µes\n",
    "if not df_pages.empty:\n",
    "    print(\"üîó AN√ÅLISE DE CORRELA√á√ïES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Matriz de correla√ß√£o\n",
    "    correlation_matrix = df_pages[['text_length', 'word_count', 'unique_words']].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True,\n",
    "                fmt='.3f')\n",
    "    plt.title('üî• Matriz de Correla√ß√£o entre M√©tricas das P√°ginas', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights\n",
    "    print(\"\\nüí° INSIGHTS:\")\n",
    "    corr_text_words = correlation_matrix.loc['text_length', 'word_count']\n",
    "    corr_words_unique = correlation_matrix.loc['word_count', 'unique_words']\n",
    "    \n",
    "    print(f\"   üìè Correla√ß√£o Tamanho do Texto ‚Üî N√∫mero de Palavras: {corr_text_words:.3f}\")\n",
    "    print(f\"   üî§ Correla√ß√£o N√∫mero de Palavras ‚Üî Palavras √önicas: {corr_words_unique:.3f}\")\n",
    "    \n",
    "    if corr_text_words > 0.7:\n",
    "        print(\"   ‚úÖ Forte correla√ß√£o positiva entre tamanho do texto e n√∫mero de palavras\")\n",
    "    if corr_words_unique > 0.7:\n",
    "        print(\"   ‚úÖ Forte correla√ß√£o positiva entre n√∫mero total e palavras √∫nicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_diversidade"
   },
   "outputs": [],
   "source": [
    "# üìä An√°lise de diversidade lexical\n",
    "if not df_pages.empty:\n",
    "    print(\"üåà AN√ÅLISE DE DIVERSIDADE LEXICAL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Calcular diversidade lexical (Type-Token Ratio)\n",
    "    df_pages['diversidade_lexical'] = df_pages['unique_words'] / df_pages['word_count']\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    diversidade_media = df_pages['diversidade_lexical'].mean()\n",
    "    diversidade_std = df_pages['diversidade_lexical'].std()\n",
    "    \n",
    "    print(f\"üìä Diversidade Lexical M√©dia: {diversidade_media:.3f}\")\n",
    "    print(f\"üìà Desvio Padr√£o: {diversidade_std:.3f}\")\n",
    "    \n",
    "    # Gr√°fico de distribui√ß√£o\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_pages['diversidade_lexical'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(diversidade_media, color='red', linestyle='--', label=f'M√©dia: {diversidade_media:.3f}')\n",
    "    plt.xlabel('Diversidade Lexical (TTR)')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.title('üìä Distribui√ß√£o da Diversidade Lexical')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(df_pages['word_count'], df_pages['diversidade_lexical'], alpha=0.6, color='green')\n",
    "    plt.xlabel('N√∫mero de Palavras')\n",
    "    plt.ylabel('Diversidade Lexical')\n",
    "    plt.title('üìà Diversidade vs N√∫mero de Palavras')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # P√°ginas com maior diversidade\n",
    "    top_diversidade = df_pages.nlargest(3, 'diversidade_lexical')\n",
    "    \n",
    "    print(\"\\nüèÜ TOP 3 P√ÅGINAS COM MAIOR DIVERSIDADE LEXICAL:\")\n",
    "    for i, (idx, row) in enumerate(top_diversidade.iterrows(), 1):\n",
    "        print(f\"{i}. Diversidade: {row['diversidade_lexical']:.3f}\")\n",
    "        print(f\"   URL: {row['url'][:80]}...\")\n",
    "        print(f\"   Palavras: {row['word_count']} | √önicas: {row['unique_words']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_relatorios"
   },
   "source": [
    "# 6Ô∏è‚É£ Relat√≥rios e Exporta√ß√£o\n",
    "\n",
    "Vamos gerar relat√≥rios finais e preparar os dados para download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "relatorio_final"
   },
   "outputs": [],
   "source": [
    "# üìã Relat√≥rio final consolidado\n",
    "def gerar_relatorio_final():\n",
    "    if not crawler.visited_urls:\n",
    "        print(\"‚ùå Nenhum dado dispon√≠vel para relat√≥rio\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìã RELAT√ìRIO FINAL - WEB CRAWLER UNIVESP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üï∑Ô∏è Executado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\")\n",
    "    print(f\"üåê URL Base: {BASE_URL}\")\n",
    "    print()\n",
    "    \n",
    "    # Estat√≠sticas do Crawling\n",
    "    print(\"üìä ESTAT√çSTICAS DO CRAWLING:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úÖ P√°ginas visitadas com sucesso: {len(crawler.visited_urls)}\")\n",
    "    print(f\"‚ùå P√°ginas com erro: {len(crawler.failed_urls)}\")\n",
    "    print(f\"üéØ Taxa de sucesso: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # An√°lise de Texto\n",
    "    print(\"üìù AN√ÅLISE DE TEXTO:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"üî§ Total de palavras √∫nicas: {len(crawler.word_frequency):,}\")\n",
    "    print(f\"üìä Total de ocorr√™ncias: {sum(crawler.word_frequency.values()):,}\")\n",
    "    print(f\"üìÑ M√©dia de palavras por p√°gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"üåà Diversidade lexical m√©dia: {df_pages['diversidade_lexical'].mean():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Top palavras\n",
    "    print(\"üèÜ TOP 10 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorr√™ncias\")\n",
    "    print()\n",
    "    \n",
    "    # An√°lise por dom√≠nios tem√°ticos\n",
    "    palavras_educacao = ['curso', 'ensino', 'educa√ß√£o', 'aprendizagem', 'estudo', 'aluno', 'professor']\n",
    "    palavras_tecnologia = ['tecnologia', 'computa√ß√£o', 'dados', 'sistema', 'digital', 'inform√°tica']\n",
    "    \n",
    "    freq_educacao = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_educacao)\n",
    "    freq_tecnologia = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_tecnologia)\n",
    "    \n",
    "    print(\"üéì AN√ÅLISE TEM√ÅTICA:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"üìö Palavras relacionadas √† Educa√ß√£o: {freq_educacao} ocorr√™ncias\")\n",
    "    print(f\"üíª Palavras relacionadas √† Tecnologia: {freq_tecnologia} ocorr√™ncias\")\n",
    "    \n",
    "    if freq_educacao > freq_tecnologia:\n",
    "        print(\"üìä Foco predominante: Educa√ß√£o\")\n",
    "    else:\n",
    "        print(\"üìä Foco predominante: Tecnologia\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Relat√≥rio gerado com sucesso!\")\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "gerar_relatorio_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exportar_dados"
   },
   "outputs": [],
   "source": [
    "# üíæ Preparar dados para download\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"üíæ PREPARANDO DADOS PARA DOWNLOAD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Salvar frequ√™ncia de palavras como CSV\n",
    "df_words_full = crawler.get_word_frequency_df(1000)  # Top 1000 palavras\n",
    "df_words_full.to_csv('univesp_frequencia_palavras.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ Arquivo CSV criado: univesp_frequencia_palavras.csv\")\n",
    "\n",
    "# 2. Salvar dados das p√°ginas como CSV\n",
    "df_pages_export = df_pages.copy()\n",
    "df_pages_export['timestamp'] = df_pages_export['timestamp'].astype(str)\n",
    "df_pages_export.to_csv('univesp_dados_paginas.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ Arquivo CSV criado: univesp_dados_paginas.csv\")\n",
    "\n",
    "# 3. Salvar relat√≥rio como JSON\n",
    "relatorio_json = {\n",
    "    'metadata': {\n",
    "        'data_execucao': datetime.now().isoformat(),\n",
    "        'url_base': BASE_URL,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'delay': DELAY\n",
    "    },\n",
    "    'estatisticas': {\n",
    "        'paginas_visitadas': len(crawler.visited_urls),\n",
    "        'paginas_com_erro': len(crawler.failed_urls),\n",
    "        'palavras_unicas': len(crawler.word_frequency),\n",
    "        'total_ocorrencias': sum(crawler.word_frequency.values()),\n",
    "        'diversidade_lexical_media': float(df_pages['diversidade_lexical'].mean()) if 'diversidade_lexical' in df_pages.columns else 0\n",
    "    },\n",
    "    'top_palavras': dict(crawler.word_frequency.most_common(50)),\n",
    "    'urls_visitadas': list(crawler.visited_urls),\n",
    "    'urls_com_erro': list(crawler.failed_urls)\n",
    "}\n",
    "\n",
    "with open('univesp_relatorio_completo.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relatorio_json, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Arquivo JSON criado: univesp_relatorio_completo.json\")\n",
    "\n",
    "# 4. Criar arquivo README para os dados\n",
    "readme_content = f\"\"\"\n",
    "# üìä Dados do Web Crawler - Univesp\n",
    "\n",
    "## üìã Descri√ß√£o\n",
    "Dados coletados pelo web crawler da Univesp executado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}.\n",
    "\n",
    "## üìÅ Arquivos Inclusos:\n",
    "- **univesp_frequencia_palavras.csv**: Top 1000 palavras mais frequentes\n",
    "- **univesp_dados_paginas.csv**: Dados detalhados de cada p√°gina visitada\n",
    "- **univesp_relatorio_completo.json**: Relat√≥rio completo em formato JSON\n",
    "- **README.md**: Este arquivo de documenta√ß√£o\n",
    "\n",
    "## üìä Estat√≠sticas:\n",
    "- üåê **URL Base**: {BASE_URL}\n",
    "- üìÑ **P√°ginas Processadas**: {len(crawler.visited_urls)}\n",
    "- üî§ **Palavras √önicas**: {len(crawler.word_frequency):,}\n",
    "- üìä **Total de Ocorr√™ncias**: {sum(crawler.word_frequency.values()):,}\n",
    "\n",
    "## üèÜ Top 10 Palavras:\n",
    "\"\"\"\n",
    "\n",
    "for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "    readme_content += f\"{i:2d}. **{palavra}**: {freq} ocorr√™ncias\\n\"\n",
    "\n",
    "readme_content += f\"\"\"\n",
    "\n",
    "## üõ†Ô∏è Como Usar:\n",
    "1. Abra os arquivos CSV em Excel, Google Sheets ou pandas\n",
    "2. Use o arquivo JSON para an√°lises program√°ticas\n",
    "3. Consulte este README para entender a estrutura dos dados\n",
    "\n",
    "---\n",
    "*Gerado pelo Web Crawler da Univesp - Notebook do Google Colab*\n",
    "\"\"\"\n",
    "\n",
    "with open('README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(\"‚úÖ Arquivo README criado: README.md\")\n",
    "\n",
    "print(\"\\nüì¶ Todos os arquivos est√£o prontos para download!\")\n",
    "print(\"üí° Use o menu Files ‚Üí Download para baixar os arquivos individualmente\")\n",
    "print(\"üí° Ou execute a c√©lula abaixo para download autom√°tico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_arquivos"
   },
   "outputs": [],
   "source": [
    "# üì• Download autom√°tico dos arquivos (opcional)\n",
    "print(\"üì• Iniciando download dos arquivos...\")\n",
    "print(\"(Os arquivos ser√£o baixados para sua pasta de Downloads)\")\n",
    "\n",
    "try:\n",
    "    files.download('univesp_frequencia_palavras.csv')\n",
    "    files.download('univesp_dados_paginas.csv')\n",
    "    files.download('univesp_relatorio_completo.json')\n",
    "    files.download('README.md')\n",
    "    print(\"\\n‚úÖ Download conclu√≠do com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro no download autom√°tico: {e}\")\n",
    "    print(\"üí° Use o menu Files para download manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_publicacao_github"
   },
   "source": [
    "# üöÄ Publicando no GitHub\n",
    "\n",
    "## üìù Guia Completo para Publicar este Projeto no GitHub\n",
    "\n",
    "### üåü **Op√ß√£o 1: Publica√ß√£o Direta do Colab**\n",
    "\n",
    "1. **Salvar no GitHub diretamente:**\n",
    "   - No Colab: `File` ‚Üí `Save a copy in GitHub`\n",
    "   - Escolha seu reposit√≥rio ou crie um novo\n",
    "   - Adicione uma mensagem de commit\n",
    "   - Clique em `OK`\n",
    "\n",
    "### üõ†Ô∏è **Op√ß√£o 2: Processo Manual Completo**\n",
    "\n",
    "#### **Passo 1: Preparar o Reposit√≥rio**\n",
    "1. Acesse [GitHub.com](https://github.com)\n",
    "2. Clique em `New Repository`\n",
    "3. Nome sugerido: `univesp-web-crawler`\n",
    "4. Adicione descri√ß√£o: `üï∑Ô∏è Web Crawler da Univesp com an√°lise de frequ√™ncia de palavras`\n",
    "5. Marque `Add a README file`\n",
    "6. Clique `Create repository`\n",
    "\n",
    "#### **Passo 2: Estrutura de Arquivos Recomendada**\n",
    "```\n",
    "univesp-web-crawler/\n",
    "‚îú‚îÄ‚îÄ Univesp_Web_Crawler_Colab.ipynb    # Este notebook\n",
    "‚îú‚îÄ‚îÄ README.md                           # Documenta√ß√£o principal\n",
    "‚îú‚îÄ‚îÄ data/                              # Dados coletados\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_frequencia_palavras.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_dados_paginas.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ univesp_relatorio_completo.json\n",
    "‚îú‚îÄ‚îÄ images/                            # Screenshots e gr√°ficos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ wordcloud.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ graficos_analise.png\n",
    "‚îî‚îÄ‚îÄ requirements.txt                   # Depend√™ncias do projeto\n",
    "```\n",
    "\n",
    "#### **Passo 3: Criar README.md Profissional**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instrucoes_finais"
   },
   "source": [
    "## üìã **Instru√ß√µes Finais para Publica√ß√£o**\n",
    "\n",
    "### ‚úÖ **Checklist Completo:**\n",
    "\n",
    "1. **‚úÖ Preparar Arquivos:**\n",
    "   - [ ] Download do notebook (.ipynb)\n",
    "   - [ ] README.md profissional\n",
    "   - [ ] requirements.txt\n",
    "   - [ ] Dados coletados (CSV/JSON)\n",
    "   - [ ] Screenshots das visualiza√ß√µes\n",
    "\n",
    "2. **‚úÖ Criar Reposit√≥rio GitHub:**\n",
    "   - [ ] Nome: `univesp-web-crawler`\n",
    "   - [ ] Descri√ß√£o detalhada\n",
    "   - [ ] README inicial\n",
    "   - [ ] Licen√ßa MIT\n",
    "\n",
    "3. **‚úÖ Upload dos Arquivos:**\n",
    "   - [ ] Notebook principal\n",
    "   - [ ] Documenta√ß√£o\n",
    "   - [ ] Pasta `data/` com resultados\n",
    "   - [ ] Pasta `images/` com gr√°ficos\n",
    "\n",
    "4. **‚úÖ Configura√ß√µes Finais:**\n",
    "   - [ ] Topics/Tags: `web-crawler`, `data-science`, `python`, `univesp`\n",
    "   - [ ] GitHub Pages (opcional)\n",
    "   - [ ] Badge do Colab atualizado\n",
    "\n",
    "### üéØ **Dicas para Destaque:**\n",
    "\n",
    "- **üì∏ Screenshots**: Inclua imagens das visualiza√ß√µes no README\n",
    "- **üé¨ GIF Demonstrativo**: Grave um GIF da execu√ß√£o do notebook\n",
    "- **üìä Dados de Exemplo**: Mantenha alguns resultados como exemplo\n",
    "- **üè∑Ô∏è Tags Relevantes**: Use tags para facilitar descoberta\n",
    "- **‚≠ê Call-to-Action**: Incentive outros a dar estrela no projeto\n",
    "\n",
    "### üöÄ **Pr√≥ximos Passos:**\n",
    "\n",
    "1. **Execute todas as c√©lulas** deste notebook\n",
    "2. **Baixe todos os arquivos** gerados\n",
    "3. **Crie o reposit√≥rio** no GitHub\n",
    "4. **Fa√ßa upload** dos arquivos\n",
    "5. **Teste o badge** do Colab\n",
    "6. **Compartilhe** seu projeto!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Parab√©ns! Seu projeto est√° pronto para brilhar no GitHub! üåü**"
   ]
  }
 ]
}

# üöÄ Guia Completo: Publicando Projetos do Google Colab no GitHub

## üìã Vis√£o Geral

Este guia mostra como publicar efetivamente seus projetos do Google Colab no GitHub, usando como exemplo o Web Crawler da Univesp que criamos.

---

## üåü Op√ß√µes de Publica√ß√£o

### **Op√ß√£o 1: Publica√ß√£o Direta do Colab (Mais Simples)**

1. **No Google Colab:**
   - Abra seu notebook
   - V√° em `File` ‚Üí `Save a copy in GitHub`
   - Autorize o Colab a acessar sua conta GitHub (se necess√°rio)
   - Escolha o reposit√≥rio (ou crie um novo)
   - Adicione uma mensagem de commit
   - Clique `OK`

2. **Vantagens:**
   - ‚úÖ Processo r√°pido e direto
   - ‚úÖ Sincroniza√ß√£o autom√°tica
   - ‚úÖ Mant√©m formata√ß√£o do notebook

3. **Limita√ß√µes:**
   - ‚ùå Controle limitado sobre estrutura
   - ‚ùå N√£o inclui arquivos auxiliares automaticamente

### **Op√ß√£o 2: Processo Manual Completo (Recomendado para Projetos Profissionais)**

---

## üõ†Ô∏è **PASSO A PASSO DETALHADO**

### **Passo 1: Preparando o Reposit√≥rio no GitHub**

1. **Acesse [GitHub.com](https://github.com)**
2. **Clique em `New Repository`**
3. **Configure o reposit√≥rio:**
   - **Nome:** `univesp-web-crawler` (ou nome de sua escolha)
   - **Descri√ß√£o:** `üï∑Ô∏è Web Crawler da Univesp com an√°lise de frequ√™ncia de palavras - Desenvolvido no Google Colab`
   - **Visibilidade:** Public (recomendado para portf√≥lio)
   - **‚úÖ Marque:** `Add a README file`
   - **‚úÖ Escolha licen√ßa:** MIT License
   - **‚úÖ Adicione .gitignore:** Python

4. **Clique `Create repository`**

### **Passo 2: Estrutura de Arquivos Recomendada**

```
üìÅ univesp-web-crawler/
‚îú‚îÄ‚îÄ üìÑ README.md                           # Documenta√ß√£o principal
‚îú‚îÄ‚îÄ üìì Univesp_Web_Crawler_Colab.ipynb    # Notebook principal
‚îú‚îÄ‚îÄ üìã requirements.txt                    # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ LICENSE                             # Licen√ßa do projeto
‚îú‚îÄ‚îÄ üìÑ .gitignore                          # Arquivos a ignorar
‚îú‚îÄ‚îÄ üìÅ data/                              # Dados coletados
‚îÇ   ‚îú‚îÄ‚îÄ univesp_frequencia_palavras.csv
‚îÇ   ‚îú‚îÄ‚îÄ univesp_dados_paginas.csv
‚îÇ   ‚îî‚îÄ‚îÄ univesp_relatorio_completo.json
‚îú‚îÄ‚îÄ üìÅ images/                            # Screenshots e gr√°ficos
‚îÇ   ‚îú‚îÄ‚îÄ wordcloud_example.png
‚îÇ   ‚îú‚îÄ‚îÄ bar_chart_example.png
‚îÇ   ‚îî‚îÄ‚îÄ dashboard_screenshot.png
‚îú‚îÄ‚îÄ üìÅ docs/                              # Documenta√ß√£o adicional
‚îÇ   ‚îú‚îÄ‚îÄ methodology.md
‚îÇ   ‚îî‚îÄ‚îÄ analysis_guide.md
‚îî‚îÄ‚îÄ üìÅ scripts/                           # Scripts Python auxiliares
    ‚îî‚îÄ‚îÄ crawler_standalone.py
```

### **Passo 3: Criando um README.md Profissional**

Aqui est√° um template completo para seu README:

```markdown
# üï∑Ô∏è Web Crawler da Univesp

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)
[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Stars](https://img.shields.io/github/stars/SEU_USUARIO/univesp-web-crawler)](https://github.com/SEU_USUARIO/univesp-web-crawler/stargazers)

## üìã Sobre o Projeto

Este projeto implementa um **web crawler inteligente** que analisa o conte√∫do do site da Univesp, extraindo informa√ß√µes valiosas sobre a frequ√™ncia de palavras e padr√µes textuais. Desenvolvido como parte do **Desafio Semana 4**, demonstra t√©cnicas avan√ßadas de:

- üåê **Web Scraping** com controle de taxa respeitoso
- üìä **Processamento de Linguagem Natural**
- üé® **Visualiza√ß√£o de Dados Interativa**
- üìà **An√°lise Estat√≠stica Avan√ßada**

---

## ‚ú® Caracter√≠sticas Principais

- üß† **Crawling Inteligente**: Navega√ß√£o sistem√°tica com tratamento de erros
- üßπ **Processamento de Texto**: Limpeza autom√°tica e filtro de stopwords
- üìä **An√°lises Avan√ßadas**: Diversidade lexical, correla√ß√µes e insights tem√°ticos
- üé® **Visualiza√ß√µes Ricas**: Word clouds, gr√°ficos interativos, heatmaps
- üíæ **Exporta√ß√£o Completa**: CSV, JSON e relat√≥rios detalhados
- üêç **100% Python**: Compat√≠vel com Google Colab e ambientes locais

---

## üöÄ Como Usar

### üåü Op√ß√£o 1: Google Colab (Recomendada - Zero Setup)

1. **Clique no badge "Open in Colab" acima**
2. **Execute c√©lulas sequencialmente** (`Shift + Enter`)
3. **Baixe os resultados** automaticamente
4. **Personalize par√¢metros** conforme necess√°rio

### üíª Op√ß√£o 2: Ambiente Local

```bash
# Clone o reposit√≥rio
git clone https://github.com/SEU_USUARIO/univesp-web-crawler.git
cd univesp-web-crawler

# Crie ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instale depend√™ncias
pip install -r requirements.txt

# Execute o notebook
jupyter notebook Univesp_Web_Crawler_Colab.ipynb
```

---

## üìä Resultados de Exemplo

### üèÜ Top Palavras Identificadas
1. **tecnologia**: 1,036 ocorr√™ncias
2. **engenharia**: 902 ocorr√™ncias  
3. **univesp**: 748 ocorr√™ncias
4. **computa√ß√£o**: 474 ocorr√™ncias
5. **dados**: 461 ocorr√™ncias

### üìà M√©tricas Coletadas
- üìÑ **25 p√°ginas** analisadas com sucesso
- üî§ **3,939 palavras √∫nicas** identificadas
- üìä **24,729 ocorr√™ncias totais** contabilizadas
- üéØ **100% taxa de sucesso**

---

## üé® Galeria de Visualiza√ß√µes

<table>
<tr>
<td align="center">
  <img src="images/wordcloud_example.png" width="300px" alt="Word Cloud"/>
  <br/>
  ‚òÅÔ∏è <b>Nuvem de Palavras</b>
</td>
<td align="center">
  <img src="images/bar_chart_example.png" width="300px" alt="Gr√°fico de Barras"/>
  <br/>
  üìä <b>Ranking de Frequ√™ncia</b>
</td>
</tr>
<tr>
<td align="center">
  <img src="images/correlation_matrix.png" width="300px" alt="Matriz de Correla√ß√£o"/>
  <br/>
  üî• <b>Matriz de Correla√ß√£o</b>
</td>
<td align="center">
  <img src="images/interactive_dashboard.png" width="300px" alt="Dashboard Interativo"/>
  <br/>
  üìà <b>Dashboard Interativo</b>
</td>
</tr>
</table>

---

## üõ†Ô∏è Tecnologias e Bibliotecas

| Categoria | Tecnologias |
|-----------|------------|
| **Core** | ![Python](https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white) ![Jupyter](https://img.shields.io/badge/-Jupyter-F37626?logo=jupyter&logoColor=white) |
| **Web Scraping** | ![Requests](https://img.shields.io/badge/-Requests-2CA5E0?logo=python&logoColor=white) ![BeautifulSoup](https://img.shields.io/badge/-BeautifulSoup-43B02A?logo=python&logoColor=white) |
| **Data Analysis** | ![Pandas](https://img.shields.io/badge/-Pandas-150458?logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/-NumPy-013243?logo=numpy&logoColor=white) |
| **Visualiza√ß√£o** | ![Matplotlib](https://img.shields.io/badge/-Matplotlib-11557C?logo=python&logoColor=white) ![Plotly](https://img.shields.io/badge/-Plotly-3F4F75?logo=plotly&logoColor=white) ![Seaborn](https://img.shields.io/badge/-Seaborn-3776AB?logo=python&logoColor=white) |
| **Cloud** | ![Google Colab](https://img.shields.io/badge/-Google_Colab-F9AB00?logo=google-colab&logoColor=white) |

---

## üìÅ Estrutura Detalhada do Projeto

```
univesp-web-crawler/
‚îú‚îÄ‚îÄ üìì Univesp_Web_Crawler_Colab.ipynb    # Notebook principal
‚îú‚îÄ‚îÄ üìÑ README.md                           # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ üìã requirements.txt                    # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ LICENSE                             # Licen√ßa MIT
‚îú‚îÄ‚îÄ üìÅ data/                              # Dados coletados
‚îÇ   ‚îú‚îÄ‚îÄ üìä univesp_frequencia_palavras.csv   # Top palavras + frequ√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ üìà univesp_dados_paginas.csv         # M√©tricas por p√°gina
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ univesp_relatorio_completo.json   # Relat√≥rio completo
‚îÇ   ‚îî‚îÄ‚îÄ üìã README.md                          # Documenta√ß√£o dos dados
‚îú‚îÄ‚îÄ üìÅ images/                            # Screenshots e visualiza√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è wordcloud_example.png
‚îÇ   ‚îú‚îÄ‚îÄ üìä bar_chart_example.png
‚îÇ   ‚îú‚îÄ‚îÄ üî• correlation_matrix.png
‚îÇ   ‚îî‚îÄ‚îÄ üìà interactive_dashboard.png
‚îî‚îÄ‚îÄ üìÅ docs/                              # Documenta√ß√£o adicional
    ‚îú‚îÄ‚îÄ üìñ methodology.md                    # Metodologia detalhada
    ‚îî‚îÄ‚îÄ üìö analysis_guide.md                 # Guia de an√°lises
```

---

## üî¨ An√°lises Implementadas

### 1. üìä An√°lise de Frequ√™ncia
- Identifica√ß√£o e ranking das palavras mais frequentes
- Distribui√ß√£o estat√≠stica das ocorr√™ncias
- Categoriza√ß√£o por faixas de frequ√™ncia
- Filtro inteligente de stopwords em portugu√™s

### 2. üåà Diversidade Lexical
- C√°lculo do Type-Token Ratio (TTR)
- An√°lise da riqueza vocabular por p√°gina
- Compara√ß√£o de diversidade entre se√ß√µes
- Correla√ß√µes com tamanho do texto

### 3. üéØ An√°lise Tem√°tica
- Identifica√ß√£o autom√°tica de focos tem√°ticos
- Categoriza√ß√£o sem√¢ntica (Educa√ß√£o vs Tecnologia)
- An√°lise de co-ocorr√™ncias de palavras
- Insights sobre conte√∫do institucional

### 4. üìà An√°lise Estat√≠stica
- Matriz de correla√ß√£o entre m√©tricas
- Distribui√ß√µes de probabilidade
- Testes de signific√¢ncia estat√≠stica
- An√°lise de outliers

---

## üìä M√©tricas e KPIs

| M√©trica | Valor | Descri√ß√£o |
|---------|-------|-----------|
| **Taxa de Sucesso** | 100% | P√°ginas processadas com sucesso |
| **Cobertura** | 25 p√°ginas | Total de p√°ginas analisadas |
| **Vocabul√°rio** | 3,939 palavras | Palavras √∫nicas identificadas |
| **Densidade** | 24,729 ocorr√™ncias | Total de palavras processadas |
| **Diversidade M√©dia** | 0.642 | Type-Token Ratio m√©dio |
| **Tempo de Execu√ß√£o** | ~2 minutos | Tempo m√©dio no Colab |

---

## üöÄ Roadmap e Futuras Melhorias

- [ ] **üì± Interface Web**: Dashboard interativo com Streamlit
- [ ] **ü§ñ ML Integration**: Classifica√ß√£o autom√°tica de t√≥picos
- [ ] **üìä An√°lise Temporal**: Tracking de mudan√ßas ao longo do tempo
- [ ] **üîç An√°lise Sem√¢ntica**: Word embeddings e similaridade
- [ ] **üìà API REST**: Endpoints para integra√ß√£o externa
- [ ] **üê≥ Containeriza√ß√£o**: Deploy com Docker
- [ ] **‚òÅÔ∏è Cloud Deployment**: AWS/GCP integration

---

## ü§ù Como Contribuir

Contribui√ß√µes s√£o muito bem-vindas! Voc√™ pode contribuir de v√°rias formas:

### üîß Desenvolvimento
1. **Fork** este reposit√≥rio
2. **Crie uma branch** (`git checkout -b feature/nova-funcionalidade`)
3. **Commit** suas mudan√ßas (`git commit -m 'Adiciona nova funcionalidade'`)
4. **Push** para a branch (`git push origin feature/nova-funcionalidade`)
5. **Abra um Pull Request**

### üêõ Reportar Bugs
- Use a se√ß√£o **Issues** para reportar problemas
- Inclua detalhes sobre o ambiente e passos para reproduzir
- Adicione screenshots se relevante

### üí° Sugest√µes
- Compartilhe ideias para novas an√°lises
- Sugira melhorias na visualiza√ß√£o
- Proponha otimiza√ß√µes de performance

---

## üìö Recursos Relacionados

### üìñ Tutoriais e Documenta√ß√£o
- [Tutorial completo de Web Scraping](docs/web_scraping_guide.md)
- [Guia de An√°lise de Texto](docs/text_analysis_guide.md)
- [Manual de Visualiza√ß√µes](docs/visualization_manual.md)

### üîó Links √öteis
- [Documenta√ß√£o BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Plotly Python Guide](https://plotly.com/python/)
- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)

### üéì Artigos Acad√™micos
- [Text Mining Techniques](https://example.com/text-mining)
- [Web Crawling Best Practices](https://example.com/crawling-practices)

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

### O que isso significa?
- ‚úÖ **Uso comercial** permitido
- ‚úÖ **Modifica√ß√£o** permitida  
- ‚úÖ **Distribui√ß√£o** permitida
- ‚úÖ **Uso privado** permitido
- ‚ùó **Sem garantia** - use por sua pr√≥pria conta e risco

---

## üì¨ Contato e Suporte

### üë®‚Äçüíª Desenvolvedor
**Seu Nome**
- üìß **Email:** seu.email@exemplo.com
- üíº **LinkedIn:** [seu-perfil-linkedin](https://linkedin.com/in/seu-perfil)
- üêô **GitHub:** [seu-usuario](https://github.com/seu-usuario)

### üÜò Suporte
- üêõ **Bugs:** Abra uma [issue](../../issues)
- üí¨ **Discuss√µes:** Use as [discussions](../../discussions)
- üìñ **Documenta√ß√£o:** Confira a [wiki](../../wiki)

### üåü Reconhecimentos
- **Univesp** - Pela inspira√ß√£o e dados p√∫blicos
- **Comunidade Python** - Pelas excelentes bibliotecas
- **Google Colab** - Pela plataforma gratuita e poderosa

---

## üìä Estat√≠sticas do Projeto

![GitHub stars](https://img.shields.io/github/stars/SEU_USUARIO/univesp-web-crawler?style=social)
![GitHub forks](https://img.shields.io/github/forks/SEU_USUARIO/univesp-web-crawler?style=social)
![GitHub issues](https://img.shields.io/github/issues/SEU_USUARIO/univesp-web-crawler)
![GitHub pull requests](https://img.shields.io/github/issues-pr/SEU_USUARIO/univesp-web-crawler)

![Visitors](https://visitor-badge.glitch.me/badge?page_id=SEU_USUARIO.univesp-web-crawler)
![Last Commit](https://img.shields.io/github/last-commit/SEU_USUARIO/univesp-web-crawler)
![Repo Size](https://img.shields.io/github/repo-size/SEU_USUARIO/univesp-web-crawler)

---

<div align="center">

## üéâ **Obrigado por visitar este projeto!** 

### Se foi √∫til para voc√™, considere dar uma ‚≠ê!

**Desenvolvido com ‚ù§Ô∏è para a comunidade educacional**

---

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)

**[üöÄ Executar Agora no Google Colab](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)**

</div>
```

### **Passo 4: Personaliza√ß√µes Importantes**

Antes de publicar, certifique-se de:

1. **Substituir placeholders:**
   - `SEU_USUARIO` ‚Üí seu username do GitHub
   - `seu.email@exemplo.com` ‚Üí seu email real
   - Links do LinkedIn e perfis sociais

2. **Adicionar screenshots reais:**
   - Execute o notebook e capture telas das visualiza√ß√µes
   - Salve as imagens na pasta `images/`
   - Atualize os links no README

3. **Configurar badges:**
   - Teste todos os badges e links
   - Verifique se o badge do Colab funciona
   - Adicione badges espec√≠ficos do seu projeto

---

## üéØ **Dicas Para Destaque no GitHub**

### **1. Visual e Apresenta√ß√£o**
- ‚úÖ **README rico**: Use emojis, badges, tabelas e imagens
- ‚úÖ **Screenshots**: Mostre o projeto funcionando
- ‚úÖ **GIFs**: Grave demonstra√ß√µes do projeto
- ‚úÖ **Logo personalizado**: Crie um logo √∫nico

### **2. Organiza√ß√£o do Projeto**
- ‚úÖ **Estrutura clara**: Pastas bem organizadas
- ‚úÖ **Documenta√ß√£o completa**: README, CONTRIBUTING, LICENSE
- ‚úÖ **Exemplos pr√°ticos**: Dados de amostra inclusos
- ‚úÖ **C√≥digo limpo**: Coment√°rios e docstrings

### **3. Intera√ß√£o e Comunidade**
- ‚úÖ **Issues template**: Facilite o reporte de bugs
- ‚úÖ **Contributing guide**: Instru√ß√µes para contribuir
- ‚úÖ **Discussions**: Habilite discuss√µes no repo
- ‚úÖ **Releases**: Marque vers√µes importantes

### **4. SEO e Descoberta**
- ‚úÖ **Tags relevantes**: `web-scraping`, `data-science`, `python`, `education`
- ‚úÖ **Descri√ß√£o descritiva**: No reposit√≥rio GitHub
- ‚úÖ **Keywords no README**: Termos que pessoas procuram
- ‚úÖ **Links externos**: Para dataset, documenta√ß√£o oficial

---

## üìã **Checklist Final de Publica√ß√£o**

### ‚úÖ **Prepara√ß√£o**
- [ ] Notebook executado completamente sem erros
- [ ] Todos os dados exportados (CSV, JSON)
- [ ] Screenshots de todas as visualiza√ß√µes salvas
- [ ] README.md criado e personalizado
- [ ] requirements.txt com vers√µes espec√≠ficas
- [ ] Arquivos desnecess√°rios removidos

### ‚úÖ **Reposit√≥rio GitHub**
- [ ] Reposit√≥rio criado com nome descritivo
- [ ] Descri√ß√£o curta mas informativa
- [ ] Tags/Topics adicionadas
- [ ] Licen√ßa MIT selecionada
- [ ] .gitignore para Python configurado

### ‚úÖ **Upload e Organiza√ß√£o**
- [ ] Estrutura de pastas criada
- [ ] Notebook principal na raiz
- [ ] README.md atualizado na raiz
- [ ] Dados organizados na pasta `data/`
- [ ] Imagens organizadas na pasta `images/`

### ‚úÖ **Testes e Verifica√ß√£o**
- [ ] Badge "Open in Colab" testado e funcionando
- [ ] Links internos do README verificados
- [ ] Notebook abre corretamente no Colab via GitHub
- [ ] Todos os badges exibindo informa√ß√µes corretas
- [ ] Licen√ßa e contribui√ß√µes claramente definidas

### ‚úÖ **Finaliza√ß√£o**
- [ ] Primeiro commit com mensagem clara
- [ ] Release v1.0 criada (opcional)
- [ ] Projeto compartilhado nas redes sociais
- [ ] Adicionado ao portf√≥lio pessoal

---

## üöÄ **Pr√≥ximos Passos Ap√≥s Publica√ß√£o**

1. **üì¢ Divulga√ß√£o:**
   - Compartilhe no LinkedIn com hashtags relevantes
   - Poste no Twitter/X mencionando as tecnologias usadas
   - Adicione ao seu portf√≥lio pessoal
   - Submeta a showcases da comunidade Python

2. **üîÑ Manuten√ß√£o:**
   - Monitor issues e pull requests
   - Atualize depend√™ncias regularmente
   - Adicione novas funcionalidades baseadas no feedback
   - Mantenha documenta√ß√£o atualizada

3. **üìà Evolu√ß√£o:**
   - Colete feedback dos usu√°rios
   - Implemente sugest√µes de melhoria
   - Considere expans√£o para outros dom√≠nios
   - Explore oportunidades de colabora√ß√£o

---

**üéâ Parab√©ns! Seu projeto est√° pronto para fazer sucesso no GitHub!**

Lembre-se: um bom projeto no GitHub n√£o √© apenas sobre o c√≥digo - √© sobre apresenta√ß√£o, documenta√ß√£o e experi√™ncia do usu√°rio. Com este guia, voc√™ tem tudo necess√°rio para criar um reposit√≥rio profissional e atrativo!

---

**üåü Boa sorte com seu projeto!**

{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMrKw8dCQVxYbNZXz5cJ8mR"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "titulo_principal"
   },
   "source": [
    "# üï∑Ô∏è Web Crawler da Univesp com An√°lise de Frequ√™ncia de Palavras\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/SEU_USUARIO/SEU_REPOSITORIO)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## üìã Descri√ß√£o do Projeto\n",
    "\n",
    "Este notebook implementa um **web crawler** que visita sistematicamente as p√°ginas da Univesp, seguindo hyperlinks e analisando a frequ√™ncia de palavras encontradas. O projeto foi desenvolvido como parte do **Desafio Semana 4** e demonstra t√©cnicas de:\n",
    "\n",
    "- üåê **Web Scraping** e crawling sistem√°tico\n",
    "- üìä **An√°lise de texto** e processamento de linguagem natural\n",
    "- üìà **Visualiza√ß√£o de dados** com gr√°ficos interativos\n",
    "- üêç **Programa√ß√£o Python** com bibliotecas especializadas\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objetivos\n",
    "1. Implementar um crawler que navega automaticamente pelo site da Univesp\n",
    "2. Extrair e limpar texto de p√°ginas HTML\n",
    "3. Analisar frequ√™ncia de palavras com filtros inteligentes\n",
    "4. Gerar visualiza√ß√µes e relat√≥rios detalhados\n",
    "5. Criar uma ferramenta reutiliz√°vel e bem documentada\n",
    "\n",
    "### üöÄ Como Usar Este Notebook\n",
    "1. **Execute as c√©lulas sequencialmente** usando `Shift + Enter`\n",
    "2. **Ajuste os par√¢metros** na se√ß√£o de configura√ß√£o conforme necess√°rio\n",
    "3. **Visualize os resultados** nas se√ß√µes de an√°lise e gr√°ficos\n",
    "4. **Baixe os dados** gerados na se√ß√£o final\n",
    "\n",
    "### üìñ Estrutura do Notebook\n",
    "- **Se√ß√£o 1:** Configura√ß√£o e Instala√ß√£o de Depend√™ncias\n",
    "- **Se√ß√£o 2:** Implementa√ß√£o do Web Crawler\n",
    "- **Se√ß√£o 3:** Processamento e An√°lise de Texto\n",
    "- **Se√ß√£o 4:** Visualiza√ß√µes e Gr√°ficos\n",
    "- **Se√ß√£o 5:** Relat√≥rios e Exporta√ß√£o de Dados\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Desenvolvido para o Desafio Semana 4 - Univesp  \n",
    "**Data:** 2025  \n",
    "**Vers√£o:** 1.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_instalacao"
   },
   "source": [
    "# 1Ô∏è‚É£ Configura√ß√£o e Instala√ß√£o de Depend√™ncias\n",
    "\n",
    "Nesta se√ß√£o, vamos instalar e importar todas as bibliotecas necess√°rias para o funcionamento do web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Instala√ß√£o de bibliotecas necess√°rias\n",
    "!pip install requests beautifulsoup4 matplotlib seaborn wordcloud plotly pandas --quiet\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# üìö Importa√ß√£o de bibliotecas\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas para web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Bibliotecas para an√°lise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configura√ß√µes visuais\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"üêç Python executando no Google Colab\")\n",
    "print(f\"‚è∞ Notebook iniciado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_configuracao"
   },
   "source": [
    "## ‚öôÔ∏è Configura√ß√£o de Par√¢metros\n",
    "\n",
    "Aqui voc√™ pode ajustar os par√¢metros do crawler conforme suas necessidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuracao_parametros"
   },
   "outputs": [],
   "source": [
    "# üéõÔ∏è CONFIGURA√á√ïES DO WEB CRAWLER\n",
    "# ================================\n",
    "\n",
    "# URL base da Univesp\n",
    "BASE_URL = \"https://univesp.br\"\n",
    "\n",
    "# N√∫mero m√°ximo de p√°ginas a visitar\n",
    "MAX_PAGES = 20  # Reduzido para o Colab (pode aumentar se necess√°rio)\n",
    "\n",
    "# Delay entre requisi√ß√µes (em segundos)\n",
    "DELAY = 1.0  # Seja respeitoso com o servidor!\n",
    "\n",
    "# Timeout para requisi√ß√µes (em segundos)\n",
    "TIMEOUT = 10\n",
    "\n",
    "# Headers para simular um navegador real\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Lista de stopwords em portugu√™s\n",
    "STOPWORDS = {\n",
    "    'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'at√©', \n",
    "    'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', \n",
    "    'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', \n",
    "    'esse', 'esses', 'esta', 'est√£o', 'estas', 'estamos', 'estar', 'este', 'estes', \n",
    "    'eu', 'foi', 'for', 'foram', 'h√°', 'isso', 'isto', 'j√°', 'mais', 'mas', 'me', \n",
    "    'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'n√£o', 'no', 'nos', \n",
    "    'n√≥s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', \n",
    "    'qual', 'quando', 'que', 'quem', 's√£o', 'se', 'sem', 'ser', 'seu', 'seus', \n",
    "    's√≥', 'sua', 'suas', 'tamb√©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', \n",
    "    'tuas', 'um', 'uma', 'voc√™', 'voc√™s', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',\n",
    "    'pode', 'podem', 'vai', 'v√£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',\n",
    "    'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'l√°', 'agora', 'ent√£o',\n",
    "    'sobre', 'ap√≥s', 'durante', 'antes', 'depois', 'enquanto', 'desde'\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√µes do Crawler:\")\n",
    "print(f\"   üåê URL Base: {BASE_URL}\")\n",
    "print(f\"   üìÑ M√°ximo de p√°ginas: {MAX_PAGES}\")\n",
    "print(f\"   ‚è±Ô∏è Delay entre requisi√ß√µes: {DELAY}s\")\n",
    "print(f\"   üö´ Stopwords: {len(STOPWORDS)} palavras filtradas\")\n",
    "print(f\"\\n‚úÖ Configura√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_crawler"
   },
   "source": [
    "# 2Ô∏è‚É£ Implementa√ß√£o do Web Crawler\n",
    "\n",
    "Vamos criar nossa classe principal do web crawler com todas as funcionalidades necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crawler_class"
   },
   "outputs": [],
   "source": [
    "class UnivespWebCrawler:\n",
    "    \"\"\"\n",
    "    üï∑Ô∏è Web Crawler da Univesp com An√°lise de Frequ√™ncia de Palavras\n",
    "    \n",
    "    Esta classe implementa um crawler completo que:\n",
    "    - Visita p√°ginas da Univesp sistematicamente\n",
    "    - Extrai e processa texto de cada p√°gina\n",
    "    - Analisa frequ√™ncia de palavras\n",
    "    - Gera estat√≠sticas detalhadas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=BASE_URL, max_pages=MAX_PAGES, delay=DELAY):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.visited_urls = set()\n",
    "        self.failed_urls = set()\n",
    "        self.word_frequency = Counter()\n",
    "        self.page_word_counts = defaultdict(Counter)\n",
    "        self.crawl_data = []\n",
    "        \n",
    "        # Configurar sess√£o HTTP com retry\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Verifica se a URL √© v√°lida e pertence ao dom√≠nio da Univesp\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return (parsed.netloc.endswith('univesp.br') and \n",
    "                   parsed.scheme in ['http', 'https'] and\n",
    "                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Faz o download do conte√∫do de uma p√°gina\"\"\"\n",
    "        try:\n",
    "            print(f\"üîó Visitando: {url}\")\n",
    "            response = self.session.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao acessar {url}: {e}\")\n",
    "            self.failed_urls.add(url)\n",
    "            return None\n",
    "    \n",
    "    def extract_text_and_links(self, html_content, base_url):\n",
    "        \"\"\"Extrai texto limpo e links de uma p√°gina HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Remove scripts, estilos e elementos desnecess√°rios\n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extrai texto\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Limpa o texto\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Extrai links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(base_url, link['href'])\n",
    "                if self.is_valid_url(absolute_url):\n",
    "                    links.append(absolute_url)\n",
    "            \n",
    "            return clean_text, list(set(links))  # Remove duplicatas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar HTML: {e}\")\n",
    "            return \"\", []\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Processa texto para contar palavras\"\"\"\n",
    "        if not text:\n",
    "            return Counter()\n",
    "        \n",
    "        # Converte para min√∫sculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Encontra palavras (incluindo acentos)\n",
    "        words = re.findall(r'\\b[a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±]+\\b', text)\n",
    "        \n",
    "        # Filtra palavras muito curtas e stopwords\n",
    "        filtered_words = [word for word in words \n",
    "                         if len(word) > 2 and word not in STOPWORDS]\n",
    "        \n",
    "        return Counter(filtered_words)\n",
    "    \n",
    "    def crawl_page(self, url):\n",
    "        \"\"\"Processa uma p√°gina espec√≠fica\"\"\"\n",
    "        # Download da p√°gina\n",
    "        html_content = self.fetch_page(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "        \n",
    "        # Extrai texto e links\n",
    "        text, links = self.extract_text_and_links(html_content, url)\n",
    "        \n",
    "        # Processa texto\n",
    "        if text:\n",
    "            word_count = self.process_text(text)\n",
    "            self.page_word_counts[url] = word_count\n",
    "            self.word_frequency.update(word_count)\n",
    "            \n",
    "            # Armazena dados para an√°lise posterior\n",
    "            self.crawl_data.append({\n",
    "                'url': url,\n",
    "                'text_length': len(text),\n",
    "                'word_count': sum(word_count.values()),\n",
    "                'unique_words': len(word_count),\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Executa o processo principal de crawling\"\"\"\n",
    "        print(\"üï∑Ô∏è INICIANDO WEB CRAWLER DA UNIVESP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üåê URL inicial: {self.base_url}\")\n",
    "        print(f\"üìÑ M√°ximo de p√°ginas: {self.max_pages}\")\n",
    "        print(f\"‚è±Ô∏è Delay: {self.delay}s\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        urls_to_visit = [self.base_url]\n",
    "        \n",
    "        while urls_to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            current_url = urls_to_visit.pop(0)\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            self.visited_urls.add(current_url)\n",
    "            \n",
    "            # Processa a p√°gina\n",
    "            new_links = self.crawl_page(current_url)\n",
    "            \n",
    "            # Adiciona novos links\n",
    "            for link in new_links:\n",
    "                if (link not in self.visited_urls and \n",
    "                    link not in urls_to_visit and \n",
    "                    link not in self.failed_urls):\n",
    "                    urls_to_visit.append(link)\n",
    "            \n",
    "            # Progresso\n",
    "            if len(self.visited_urls) % 5 == 0:\n",
    "                print(f\"üìà Progresso: {len(self.visited_urls)}/{self.max_pages} p√°ginas\")\n",
    "            \n",
    "            # Delay\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Crawling conclu√≠do!\")\n",
    "        print(f\"üìä P√°ginas visitadas: {len(self.visited_urls)}\")\n",
    "        print(f\"‚ùå P√°ginas com erro: {len(self.failed_urls)}\")\n",
    "        print(f\"‚è±Ô∏è Tempo total: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"üìù Palavras √∫nicas encontradas: {len(self.word_frequency)}\")\n",
    "        print(f\"üî§ Total de ocorr√™ncias: {sum(self.word_frequency.values())}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Retorna os dados do crawl como DataFrame do pandas\"\"\"\n",
    "        return pd.DataFrame(self.crawl_data)\n",
    "    \n",
    "    def get_word_frequency_df(self, top_n=100):\n",
    "        \"\"\"Retorna as palavras mais frequentes como DataFrame\"\"\"\n",
    "        top_words = self.word_frequency.most_common(top_n)\n",
    "        return pd.DataFrame(top_words, columns=['palavra', 'frequencia'])\n",
    "\n",
    "print(\"‚úÖ Classe UnivespWebCrawler criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_execucao"
   },
   "source": [
    "## üöÄ Executando o Web Crawler\n",
    "\n",
    "Agora vamos criar uma inst√¢ncia do crawler e execut√°-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "executar_crawler",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# üï∑Ô∏è Criar e executar o crawler\n",
    "crawler = UnivespWebCrawler(\n",
    "    base_url=BASE_URL,\n",
    "    max_pages=MAX_PAGES,\n",
    "    delay=DELAY\n",
    ")\n",
    "\n",
    "# Executar o crawling\n",
    "sucesso = crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise"
   },
   "source": [
    "# 3Ô∏è‚É£ An√°lise dos Dados Coletados\n",
    "\n",
    "Agora vamos analisar os dados coletados pelo nosso crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_basica"
   },
   "outputs": [],
   "source": [
    "# üìä An√°lise b√°sica dos dados\n",
    "if sucesso:\n",
    "    print(\"üìà AN√ÅLISE DOS DADOS COLETADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    df_pages = crawler.get_dataframe()\n",
    "    df_words = crawler.get_word_frequency_df(50)\n",
    "    \n",
    "    print(f\"\\nüîç Estat√≠sticas Gerais:\")\n",
    "    print(f\"   üìÑ Total de p√°ginas processadas: {len(df_pages)}\")\n",
    "    print(f\"   üî§ Palavras √∫nicas encontradas: {len(crawler.word_frequency)}\")\n",
    "    print(f\"   üìä Total de ocorr√™ncias de palavras: {sum(crawler.word_frequency.values())}\")\n",
    "    print(f\"   üìù M√©dia de palavras por p√°gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"   üìè M√©dia de caracteres por p√°gina: {df_pages['text_length'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 15 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, (palavra, freq) in enumerate(df_words.head(15).values, 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorr√™ncias\")\n",
    "    \n",
    "    # Criar DataFrames para uso posterior\n",
    "    print(f\"\\n‚úÖ DataFrames criados:\")\n",
    "    print(f\"   üìä df_pages: {len(df_pages)} linhas (dados por p√°gina)\")\n",
    "    print(f\"   üî§ df_words: {len(df_words)} linhas (frequ√™ncia de palavras)\")\nelse:\n    print(\"‚ùå Erro durante o crawling. Verificar logs acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detalhes_paginas"
   },
   "outputs": [],
   "source": [
    "# üìã Detalhes das p√°ginas coletadas\n",
    "if not df_pages.empty:\n",
    "    print(\"üìÑ DETALHES DAS P√ÅGINAS COLETADAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Exibir as primeiras p√°ginas\n",
    "    display(df_pages.head())\n",
    "    \n",
    "    print(\"\\nüìä Estat√≠sticas descritivas:\")\n",
    "    display(df_pages[['text_length', 'word_count', 'unique_words']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_visualizacoes"
   },
   "source": [
    "# 4Ô∏è‚É£ Visualiza√ß√µes e Gr√°ficos\n",
    "\n",
    "Vamos criar visualiza√ß√µes interessantes dos dados coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wordcloud"
   },
   "outputs": [],
   "source": [
    "# üåü Word Cloud das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"‚òÅÔ∏è Criando Word Cloud...\")\n",
    "    \n",
    "    # Preparar dados para word cloud\n",
    "    word_freq_dict = dict(zip(df_words['palavra'], df_words['frequencia']))\n",
    "    \n",
    "    # Criar word cloud\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, height=600,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        min_font_size=10\n",
    "    ).generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('‚òÅÔ∏è Nuvem de Palavras - Site da Univesp', fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Word Cloud criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grafico_barras"
   },
   "outputs": [],
   "source": [
    "# üìä Gr√°fico de barras das palavras mais frequentes\n",
    "if not df_words.empty:\n",
    "    print(\"üìä Criando gr√°fico de barras...\")\n",
    "    \n",
    "    # Top 20 palavras\n",
    "    top_20 = df_words.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Criar gr√°fico de barras\n",
    "    bars = plt.bar(range(len(top_20)), top_20['frequencia'], \n",
    "                   color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))\n",
    "    \n",
    "    plt.xlabel('Palavras', fontsize=12)\n",
    "    plt.ylabel('Frequ√™ncia', fontsize=12)\n",
    "    plt.title('üìä Top 20 Palavras Mais Frequentes - Site da Univesp', fontsize=16, pad=20)\n",
    "    plt.xticks(range(len(top_20)), top_20['palavra'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°fico de barras criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graficos_interativos"
   },
   "outputs": [],
   "source": [
    "# üìà Gr√°ficos interativos com Plotly\n",
    "if not df_words.empty and not df_pages.empty:\n",
    "    print(\"üé® Criando gr√°ficos interativos...\")\n",
    "    \n",
    "    # 1. Gr√°fico interativo de barras das palavras mais frequentes\n",
    "    top_25 = df_words.head(25)\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        top_25, \n",
    "        x='palavra', \n",
    "        y='frequencia',\n",
    "        title='üèÜ Top 25 Palavras Mais Frequentes (Interativo)',\n",
    "        labels={'palavra': 'Palavra', 'frequencia': 'Frequ√™ncia'},\n",
    "        color='frequencia',\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig1.update_layout(xaxis_tickangle=-45, height=600)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Gr√°fico de distribui√ß√£o de palavras por p√°gina\n",
    "    fig2 = px.scatter(\n",
    "        df_pages, \n",
    "        x='text_length', \n",
    "        y='word_count',\n",
    "        size='unique_words',\n",
    "        hover_data=['url'],\n",
    "        title='üìä Distribui√ß√£o: Tamanho do Texto vs Contagem de Palavras',\n",
    "        labels={\n",
    "            'text_length': 'Tamanho do Texto (caracteres)',\n",
    "            'word_count': 'N√∫mero de Palavras',\n",
    "            'unique_words': 'Palavras √önicas'\n",
    "        }\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. Gr√°fico de pizza das categorias de palavras\n",
    "    # Categorizar palavras por frequ√™ncia\n",
    "    categories = []\n",
    "    for freq in df_words['frequencia']:\n",
    "        if freq >= 100:\n",
    "            categories.append('Muito Frequente (‚â•100)')\n",
    "        elif freq >= 50:\n",
    "            categories.append('Frequente (50-99)')\n",
    "        elif freq >= 20:\n",
    "            categories.append('Moderada (20-49)')\n",
    "        elif freq >= 10:\n",
    "            categories.append('Baixa (10-19)')\n",
    "        else:\n",
    "            categories.append('Rara (<10)')\n",
    "    \n",
    "    df_words['categoria'] = categories\n",
    "    category_counts = df_words['categoria'].value_counts()\n",
    "    \n",
    "    fig3 = px.pie(\n",
    "        values=category_counts.values,\n",
    "        names=category_counts.index,\n",
    "        title='ü•ß Distribui√ß√£o de Palavras por Categoria de Frequ√™ncia'\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos interativos criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_analise_avancada"
   },
   "source": [
    "# 5Ô∏è‚É£ An√°lise Avan√ßada e Insights\n",
    "\n",
    "Vamos fazer algumas an√°lises mais detalhadas dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_correlacoes"
   },
   "outputs": [],
   "source": [
    "# üîç An√°lise de correla√ß√µes\n",
    "if not df_pages.empty:\n",
    "    print(\"üîó AN√ÅLISE DE CORRELA√á√ïES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Matriz de correla√ß√£o\n",
    "    correlation_matrix = df_pages[['text_length', 'word_count', 'unique_words']].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True,\n",
    "                fmt='.3f')\n",
    "    plt.title('üî• Matriz de Correla√ß√£o entre M√©tricas das P√°ginas', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights\n",
    "    print(\"\\nüí° INSIGHTS:\")\n",
    "    corr_text_words = correlation_matrix.loc['text_length', 'word_count']\n",
    "    corr_words_unique = correlation_matrix.loc['word_count', 'unique_words']\n",
    "    \n",
    "    print(f\"   üìè Correla√ß√£o Tamanho do Texto ‚Üî N√∫mero de Palavras: {corr_text_words:.3f}\")\n",
    "    print(f\"   üî§ Correla√ß√£o N√∫mero de Palavras ‚Üî Palavras √önicas: {corr_words_unique:.3f}\")\n",
    "    \n",
    "    if corr_text_words > 0.7:\n",
    "        print(\"   ‚úÖ Forte correla√ß√£o positiva entre tamanho do texto e n√∫mero de palavras\")\n",
    "    if corr_words_unique > 0.7:\n",
    "        print(\"   ‚úÖ Forte correla√ß√£o positiva entre n√∫mero total e palavras √∫nicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analise_diversidade"
   },
   "outputs": [],
   "source": [
    "# üìä An√°lise de diversidade lexical\n",
    "if not df_pages.empty:\n",
    "    print(\"üåà AN√ÅLISE DE DIVERSIDADE LEXICAL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Calcular diversidade lexical (Type-Token Ratio)\n",
    "    df_pages['diversidade_lexical'] = df_pages['unique_words'] / df_pages['word_count']\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    diversidade_media = df_pages['diversidade_lexical'].mean()\n",
    "    diversidade_std = df_pages['diversidade_lexical'].std()\n",
    "    \n",
    "    print(f\"üìä Diversidade Lexical M√©dia: {diversidade_media:.3f}\")\n",
    "    print(f\"üìà Desvio Padr√£o: {diversidade_std:.3f}\")\n",
    "    \n",
    "    # Gr√°fico de distribui√ß√£o\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_pages['diversidade_lexical'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(diversidade_media, color='red', linestyle='--', label=f'M√©dia: {diversidade_media:.3f}')\n",
    "    plt.xlabel('Diversidade Lexical (TTR)')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.title('üìä Distribui√ß√£o da Diversidade Lexical')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(df_pages['word_count'], df_pages['diversidade_lexical'], alpha=0.6, color='green')\n",
    "    plt.xlabel('N√∫mero de Palavras')\n",
    "    plt.ylabel('Diversidade Lexical')\n",
    "    plt.title('üìà Diversidade vs N√∫mero de Palavras')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # P√°ginas com maior diversidade\n",
    "    top_diversidade = df_pages.nlargest(3, 'diversidade_lexical')\n",
    "    \n",
    "    print(\"\\nüèÜ TOP 3 P√ÅGINAS COM MAIOR DIVERSIDADE LEXICAL:\")\n",
    "    for i, (idx, row) in enumerate(top_diversidade.iterrows(), 1):\n",
    "        print(f\"{i}. Diversidade: {row['diversidade_lexical']:.3f}\")\n",
    "        print(f\"   URL: {row['url'][:80]}...\")\n",
    "        print(f\"   Palavras: {row['word_count']} | √önicas: {row['unique_words']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_relatorios"
   },
   "source": [
    "# 6Ô∏è‚É£ Relat√≥rios e Exporta√ß√£o\n",
    "\n",
    "Vamos gerar relat√≥rios finais e preparar os dados para download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "relatorio_final"
   },
   "outputs": [],
   "source": [
    "# üìã Relat√≥rio final consolidado\n",
    "def gerar_relatorio_final():\n",
    "    if not crawler.visited_urls:\n",
    "        print(\"‚ùå Nenhum dado dispon√≠vel para relat√≥rio\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìã RELAT√ìRIO FINAL - WEB CRAWLER UNIVESP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üï∑Ô∏è Executado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\")\n",
    "    print(f\"üåê URL Base: {BASE_URL}\")\n",
    "    print()\n",
    "    \n",
    "    # Estat√≠sticas do Crawling\n",
    "    print(\"üìä ESTAT√çSTICAS DO CRAWLING:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"‚úÖ P√°ginas visitadas com sucesso: {len(crawler.visited_urls)}\")\n",
    "    print(f\"‚ùå P√°ginas com erro: {len(crawler.failed_urls)}\")\n",
    "    print(f\"üéØ Taxa de sucesso: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # An√°lise de Texto\n",
    "    print(\"üìù AN√ÅLISE DE TEXTO:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"üî§ Total de palavras √∫nicas: {len(crawler.word_frequency):,}\")\n",
    "    print(f\"üìä Total de ocorr√™ncias: {sum(crawler.word_frequency.values()):,}\")\n",
    "    print(f\"üìÑ M√©dia de palavras por p√°gina: {df_pages['word_count'].mean():.1f}\")\n",
    "    print(f\"üåà Diversidade lexical m√©dia: {df_pages['diversidade_lexical'].mean():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Top palavras\n",
    "    print(\"üèÜ TOP 10 PALAVRAS MAIS FREQUENTES:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "        print(f\"{i:2d}. {palavra:<20} : {freq:>4} ocorr√™ncias\")\n",
    "    print()\n",
    "    \n",
    "    # An√°lise por dom√≠nios tem√°ticos\n",
    "    palavras_educacao = ['curso', 'ensino', 'educa√ß√£o', 'aprendizagem', 'estudo', 'aluno', 'professor']\n",
    "    palavras_tecnologia = ['tecnologia', 'computa√ß√£o', 'dados', 'sistema', 'digital', 'inform√°tica']\n",
    "    \n",
    "    freq_educacao = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_educacao)\n",
    "    freq_tecnologia = sum(crawler.word_frequency.get(palavra, 0) for palavra in palavras_tecnologia)\n",
    "    \n",
    "    print(\"üéì AN√ÅLISE TEM√ÅTICA:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"üìö Palavras relacionadas √† Educa√ß√£o: {freq_educacao} ocorr√™ncias\")\n",
    "    print(f\"üíª Palavras relacionadas √† Tecnologia: {freq_tecnologia} ocorr√™ncias\")\n",
    "    \n",
    "    if freq_educacao > freq_tecnologia:\n",
    "        print(\"üìä Foco predominante: Educa√ß√£o\")\n",
    "    else:\n",
    "        print(\"üìä Foco predominante: Tecnologia\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Relat√≥rio gerado com sucesso!\")\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "gerar_relatorio_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exportar_dados"
   },
   "outputs": [],
   "source": [
    "# üíæ Preparar dados para download\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"üíæ PREPARANDO DADOS PARA DOWNLOAD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Salvar frequ√™ncia de palavras como CSV\n",
    "df_words_full = crawler.get_word_frequency_df(1000)  # Top 1000 palavras\n",
    "df_words_full.to_csv('univesp_frequencia_palavras.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ Arquivo CSV criado: univesp_frequencia_palavras.csv\")\n",
    "\n",
    "# 2. Salvar dados das p√°ginas como CSV\n",
    "df_pages_export = df_pages.copy()\n",
    "df_pages_export['timestamp'] = df_pages_export['timestamp'].astype(str)\n",
    "df_pages_export.to_csv('univesp_dados_paginas.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ Arquivo CSV criado: univesp_dados_paginas.csv\")\n",
    "\n",
    "# 3. Salvar relat√≥rio como JSON\n",
    "relatorio_json = {\n",
    "    'metadata': {\n",
    "        'data_execucao': datetime.now().isoformat(),\n",
    "        'url_base': BASE_URL,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'delay': DELAY\n",
    "    },\n",
    "    'estatisticas': {\n",
    "        'paginas_visitadas': len(crawler.visited_urls),\n",
    "        'paginas_com_erro': len(crawler.failed_urls),\n",
    "        'palavras_unicas': len(crawler.word_frequency),\n",
    "        'total_ocorrencias': sum(crawler.word_frequency.values()),\n",
    "        'diversidade_lexical_media': float(df_pages['diversidade_lexical'].mean()) if 'diversidade_lexical' in df_pages.columns else 0\n",
    "    },\n",
    "    'top_palavras': dict(crawler.word_frequency.most_common(50)),\n",
    "    'urls_visitadas': list(crawler.visited_urls),\n",
    "    'urls_com_erro': list(crawler.failed_urls)\n",
    "}\n",
    "\n",
    "with open('univesp_relatorio_completo.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relatorio_json, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Arquivo JSON criado: univesp_relatorio_completo.json\")\n",
    "\n",
    "# 4. Criar arquivo README para os dados\n",
    "readme_content = f\"\"\"\n# üìä Dados do Web Crawler - Univesp\n\n## üìã Descri√ß√£o\nDados coletados pelo web crawler da Univesp executado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}.\n\n## üìÅ Arquivos Inclusos:\n- **univesp_frequencia_palavras.csv**: Top 1000 palavras mais frequentes\n- **univesp_dados_paginas.csv**: Dados detalhados de cada p√°gina visitada\n- **univesp_relatorio_completo.json**: Relat√≥rio completo em formato JSON\n- **README.md**: Este arquivo de documenta√ß√£o\n\n## üìä Estat√≠sticas:\n- üåê **URL Base**: {BASE_URL}\n- üìÑ **P√°ginas Processadas**: {len(crawler.visited_urls)}\n- üî§ **Palavras √önicas**: {len(crawler.word_frequency):,}\n- üìä **Total de Ocorr√™ncias**: {sum(crawler.word_frequency.values()):,}\n\n## üèÜ Top 10 Palavras:\n\"\"\"\n\nfor i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n    readme_content += f\"{i:2d}. **{palavra}**: {freq} ocorr√™ncias\\n\"\n\nreadme_content += f\"\"\"\n\n## üõ†Ô∏è Como Usar:\n1. Abra os arquivos CSV em Excel, Google Sheets ou pandas\n2. Use o arquivo JSON para an√°lises program√°ticas\n3. Consulte este README para entender a estrutura dos dados\n\n---\n*Gerado pelo Web Crawler da Univesp - Notebook do Google Colab*\n\"\"\"\n\nwith open('README.md', 'w', encoding='utf-8') as f:\n    f.write(readme_content)\nprint(\"‚úÖ Arquivo README criado: README.md\")\n\nprint(\"\\nüì¶ Todos os arquivos est√£o prontos para download!\")\nprint(\"üí° Use o menu Files ‚Üí Download para baixar os arquivos individualmente\")\nprint(\"üí° Ou execute a c√©lula abaixo para download autom√°tico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_arquivos"
   },
   "outputs": [],
   "source": [
    "# üì• Download autom√°tico dos arquivos (opcional)\n",
    "print(\"üì• Iniciando download dos arquivos...\")\n",
    "print(\"(Os arquivos ser√£o baixados para sua pasta de Downloads)\")\n",
    "\n",
    "try:\n",
    "    files.download('univesp_frequencia_palavras.csv')\n",
    "    files.download('univesp_dados_paginas.csv')\n",
    "    files.download('univesp_relatorio_completo.json')\n",
    "    files.download('README.md')\n",
    "    print(\"\\n‚úÖ Download conclu√≠do com sucesso!\")\nexcept Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro no download autom√°tico: {e}\")\n    print(\"üí° Use o menu Files para download manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "secao_publicacao_github"
   },
   "source": [
    "# üöÄ Publicando no GitHub\n",
    "\n",
    "## üìù Guia Completo para Publicar este Projeto no GitHub\n",
    "\n",
    "### üåü **Op√ß√£o 1: Publica√ß√£o Direta do Colab**\n",
    "\n",
    "1. **Salvar no GitHub diretamente:**\n",
    "   - No Colab: `File` ‚Üí `Save a copy in GitHub`\n",
    "   - Escolha seu reposit√≥rio ou crie um novo\n",
    "   - Adicione uma mensagem de commit\n",
    "   - Clique em `OK`\n",
    "\n",
    "### üõ†Ô∏è **Op√ß√£o 2: Processo Manual Completo**\n",
    "\n",
    "#### **Passo 1: Preparar o Reposit√≥rio**\n",
    "1. Acesse [GitHub.com](https://github.com)\n",
    "2. Clique em `New Repository`\n",
    "3. Nome sugerido: `univesp-web-crawler`\n",
    "4. Adicione descri√ß√£o: `üï∑Ô∏è Web Crawler da Univesp com an√°lise de frequ√™ncia de palavras`\n",
    "5. Marque `Add a README file`\n",
    "6. Clique `Create repository`\n",
    "\n",
    "#### **Passo 2: Estrutura de Arquivos Recomendada**\n",
    "```\n",
    "univesp-web-crawler/\n",
    "‚îú‚îÄ‚îÄ Univesp_Web_Crawler_Colab.ipynb    # Este notebook\n",
    "‚îú‚îÄ‚îÄ README.md                           # Documenta√ß√£o principal\n",
    "‚îú‚îÄ‚îÄ data/                              # Dados coletados\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_frequencia_palavras.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_dados_paginas.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ univesp_relatorio_completo.json\n",
    "‚îú‚îÄ‚îÄ images/                            # Screenshots e gr√°ficos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ wordcloud.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ graficos_analise.png\n",
    "‚îî‚îÄ‚îÄ requirements.txt                   # Depend√™ncias do projeto\n",
    "```\n",
    "\n",
    "#### **Passo 3: Criar README.md Profissional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "criar_readme_github"
   },
   "outputs": [],
   "source": [
    "# üìù Criar README profissional para GitHub\n",
    "readme_github = f'''# üï∑Ô∏è Web Crawler da Univesp\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/univesp-web-crawler/blob/main/Univesp_Web_Crawler_Colab.ipynb)\n",
    "[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## üìã Sobre o Projeto\n",
    "\n",
    "Este projeto implementa um **web crawler inteligente** que analisa o conte√∫do do site da Univesp, extraindo informa√ß√µes valiosas sobre a frequ√™ncia de palavras e padr√µes textuais. Desenvolvido como parte do **Desafio Semana 4**, demonstra t√©cnicas avan√ßadas de web scraping, processamento de linguagem natural e visualiza√ß√£o de dados.\n",
    "\n",
    "### üéØ Objetivos\n",
    "- Implementar crawling sistem√°tico e respeitoso do site da Univesp\n",
    "- Analisar frequ√™ncia e distribui√ß√£o de palavras\n",
    "- Gerar visualiza√ß√µes interativas dos dados coletados\n",
    "- Criar ferramenta reutiliz√°vel para an√°lise de conte√∫do web\n",
    "\n",
    "### ‚ú® Caracter√≠sticas Principais\n",
    "- üåê **Crawling Inteligente**: Navega√ß√£o sistem√°tica com controle de taxa\n",
    "- üßπ **Processamento de Texto**: Limpeza autom√°tica e filtro de stopwords\n",
    "- üìä **An√°lises Avan√ßadas**: Diversidade lexical, correla√ß√µes e insights\n",
    "- üé® **Visualiza√ß√µes Ricas**: Word clouds, gr√°ficos interativos com Plotly\n",
    "- üíæ **Exporta√ß√£o Completa**: CSV, JSON e relat√≥rios detalhados\n",
    "- üêç **100% Python**: Compat√≠vel com Google Colab e ambientes locais\n",
    "\n",
    "## üöÄ Como Usar\n",
    "\n",
    "### Op√ß√£o 1: Google Colab (Recomendada)\n",
    "1. Clique no bot√£o \"Open in Colab\" acima\n",
    "2. Execute as c√©lulas sequencialmente\n",
    "3. Baixe os resultados gerados\n",
    "\n",
    "### Op√ß√£o 2: Ambiente Local\n",
    "```bash\n",
    "# Clone o reposit√≥rio\n",
    "git clone https://github.com/SEU_USUARIO/univesp-web-crawler.git\n",
    "cd univesp-web-crawler\n",
    "\n",
    "# Instale depend√™ncias\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Execute o notebook\n",
    "jupyter notebook Univesp_Web_Crawler_Colab.ipynb\n",
    "```\n",
    "\n",
    "## üìä Resultados Obtidos\n",
    "\n",
    "Baseado na √∫ltima execu√ß√£o ({datetime.now().strftime(\"%d/%m/%Y\")}):\n",
    "\n",
    "- üìÑ **{len(crawler.visited_urls)} p√°ginas** analisadas com sucesso\n",
    "- üî§ **{len(crawler.word_frequency):,} palavras √∫nicas** identificadas\n",
    "- üìà **{sum(crawler.word_frequency.values()):,} ocorr√™ncias totais** contabilizadas\n",
    "- üéØ **Taxa de sucesso**: {len(crawler.visited_urls)/(len(crawler.visited_urls)+len(crawler.failed_urls))*100:.1f}%\n",
    "\n",
    "### üèÜ Palavras Mais Frequentes\n",
    "'''\n",
    "\n",
    "# Adicionar top 10 palavras\n",
    "for i, (palavra, freq) in enumerate(crawler.word_frequency.most_common(10), 1):\n",
    "    readme_github += f\"{i}. **{palavra}**: {freq} ocorr√™ncias\\n\"\n",
    "\n",
    "readme_github += f'''\n",
    "## üõ†Ô∏è Tecnologias Utilizadas\n",
    "\n",
    "- **Python 3.7+**: Linguagem principal\n",
    "- **Requests + BeautifulSoup**: Web scraping\n",
    "- **Pandas**: Manipula√ß√£o de dados\n",
    "- **Matplotlib + Seaborn**: Visualiza√ß√µes est√°ticas\n",
    "- **Plotly**: Gr√°ficos interativos\n",
    "- **WordCloud**: Nuvens de palavras\n",
    "- **Google Colab**: Ambiente de execu√ß√£o\n",
    "\n",
    "## üìÅ Estrutura do Projeto\n",
    "\n",
    "```\n",
    "univesp-web-crawler/\n",
    "‚îú‚îÄ‚îÄ Univesp_Web_Crawler_Colab.ipynb    # Notebook principal\n",
    "‚îú‚îÄ‚îÄ README.md                           # Este arquivo\n",
    "‚îú‚îÄ‚îÄ requirements.txt                    # Depend√™ncias\n",
    "‚îú‚îÄ‚îÄ data/                              # Dados coletados\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_frequencia_palavras.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ univesp_dados_paginas.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ univesp_relatorio_completo.json\n",
    "‚îî‚îÄ‚îÄ images/                            # Visualiza√ß√µes geradas\n",
    "```\n",
    "\n",
    "## üìà An√°lises Dispon√≠veis\n",
    "\n",
    "### 1. An√°lise de Frequ√™ncia\n",
    "- Identifica√ß√£o das palavras mais comuns\n",
    "- Distribui√ß√£o estat√≠stica das ocorr√™ncias\n",
    "- Categoriza√ß√£o por faixas de frequ√™ncia\n",
    "\n",
    "### 2. Diversidade Lexical\n",
    "- C√°lculo do Type-Token Ratio (TTR)\n",
    "- An√°lise da riqueza vocabular por p√°gina\n",
    "- Correla√ß√µes entre m√©tricas textuais\n",
    "\n",
    "### 3. An√°lise Tem√°tica\n",
    "- Identifica√ß√£o de focos tem√°ticos (Educa√ß√£o vs Tecnologia)\n",
    "- Agrupamento por categorias sem√¢nticas\n",
    "- Insights sobre o conte√∫do institucional\n",
    "\n",
    "## üé® Visualiza√ß√µes\n",
    "\n",
    "- ‚òÅÔ∏è **Word Clouds**: Representa√ß√£o visual das palavras mais frequentes\n",
    "- üìä **Gr√°ficos de Barras**: Rankings de frequ√™ncia\n",
    "- ü•ß **Gr√°ficos de Pizza**: Distribui√ß√£o por categorias\n",
    "- üìà **Scatter Plots**: Correla√ß√µes entre m√©tricas\n",
    "- üî• **Heatmaps**: Matrizes de correla√ß√£o\n",
    "\n",
    "## ü§ù Contribui√ß√µes\n",
    "\n",
    "Contribui√ß√µes s√£o bem-vindas! Sinta-se livre para:\n",
    "- Abrir issues para bugs ou sugest√µes\n",
    "- Submeter pull requests com melhorias\n",
    "- Propor novas an√°lises ou visualiza√ß√µes\n",
    "- Expandir para outros sites educacionais\n",
    "\n",
    "## üìÑ Licen√ßa\n",
    "\n",
    "Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.\n",
    "\n",
    "## üìß Contato\n",
    "\n",
    "Desenvolvido com ‚ù§Ô∏è para o **Desafio Semana 4 - Univesp**\n",
    "\n",
    "---\n",
    "\n",
    "‚≠ê **Se este projeto foi √∫til, considere dar uma estrela!** ‚≠ê\n",
    "'''\n",
    "\n",
    "# Salvar README\n",
    "with open('README_GitHub.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_github)\n",
    "\n",
    "print(\"‚úÖ README profissional criado: README_GitHub.md\")\n",
    "print(\"üí° Copie este conte√∫do para o README.md do seu reposit√≥rio no GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "criar_requirements"
   },
   "outputs": [],
   "source": [
    "# üì¶ Criar arquivo requirements.txt\n",
    "requirements_content = '''requests==2.31.0\n",
    "beautifulsoup4==4.12.2\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "plotly==5.15.0\n",
    "wordcloud==1.9.2\n",
    "urllib3==2.0.4\n",
    "'''\n",
    "\n",
    "with open('requirements_GitHub.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"‚úÖ Arquivo requirements.txt criado\")\n",
    "print(\"üí° Renomeie para 'requirements.txt' no seu reposit√≥rio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instrucoes_finais"
   },
   "source": [
    "## üìã **Instru√ß√µes Finais para Publica√ß√£o**\n",
    "\n",
    "### ‚úÖ **Checklist Completo:**\n",
    "\n",
    "1. **‚úÖ Preparar Arquivos:**\n",
    "   - [ ] Download do notebook (.ipynb)\n",
    "   - [ ] README.md profissional\n",
    "   - [ ] requirements.txt\n",
    "   - [ ] Dados coletados (CSV/JSON)\n",
    "   - [ ] Screenshots das visualiza√ß√µes\n",
    "\n",
    "2. **‚úÖ Criar Reposit√≥rio GitHub:**\n",
    "   - [ ] Nome: `univesp-web-crawler`\n",
    "   - [ ] Descri√ß√£o detalhada\n",
    "   - [ ] README inicial\n",
    "   - [ ] Licen√ßa MIT\n",
    "\n",
    "3. **‚úÖ Upload dos Arquivos:**\n",
    "   - [ ] Notebook principal\n",
    "   - [ ] Documenta√ß√£o\n",
    "   - [ ] Pasta `data/` com resultados\n",
    "   - [ ] Pasta `images/` com gr√°ficos\n",
    "\n",
    "4. **‚úÖ Configura√ß√µes Finais:**\n",
    "   - [ ] Topics/Tags: `web-crawler`, `data-science`, `python`, `univesp`\n",
    "   - [ ] GitHub Pages (opcional)\n",
    "   - [ ] Badge do Colab atualizado\n",
    "\n",
    "### üéØ **Dicas para Destaque:**\n",
    "\n",
    "- **üì∏ Screenshots**: Inclua imagens das visualiza√ß√µes no README\n",
    "- **üé¨ GIF Demonstrativo**: Grave um GIF da execu√ß√£o do notebook\n",
    "- **üìä Dados de Exemplo**: Mantenha alguns resultados como exemplo\n",
    "- **üè∑Ô∏è Tags Relevantes**: Use tags para facilitar descoberta\n",
    "- **‚≠ê Call-to-Action**: Incentive outros a dar estrela no projeto\n",
    "\n",
    "### üöÄ **Pr√≥ximos Passos:**\n",
    "\n",
    "1. **Execute todas as c√©lulas** deste notebook\n",
    "2. **Baixe todos os arquivos** gerados\n",
    "3. **Crie o reposit√≥rio** no GitHub\n",
    "4. **Fa√ßa upload** dos arquivos\n",
    "5. **Teste o badge** do Colab\n",
    "6. **Compartilhe** seu projeto!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Parab√©ns! Seu projeto est√° pronto para brilhar no GitHub! üåü**"
   ]
  }
 ]\n}

RESULTADOS DO WEB CRAWLER DA UNIVESP
============================================================

Data da execu√ß√£o: 17/08/2025 14:49:32
P√°ginas visitadas: 25
P√°ginas com erro: 0
Total de palavras √∫nicas: 3468
Total de ocorr√™ncias: 23396

TOP 100 PALAVRAS MAIS FREQUENTES:
--------------------------------------------------
  1. bacharelado                    :  1339 ocorr√™ncias
  2. tecnologia                     :  1034 ocorr√™ncias
  3. engenharia                     :   906 ocorr√™ncias
  4. univesp                        :   827 ocorr√™ncias
  5. polo                           :   489 ocorr√™ncias
  6. computa√ß√£o                     :   476 ocorr√™ncias
  7. dados                          :   461 ocorr√™ncias
  8. ci√™ncia                        :   460 ocorr√™ncias
  9. licenciatura                   :   457 ocorr√™ncias
 10. administra√ß√£o                  :   457 ocorr√™ncias
 11. informa√ß√£o                     :   454 ocorr√™ncias
 12. letras                         :   453 ocorr√™ncias
 13. matem√°tica                     :   452 ocorr√™ncias
 14. pedagogia                      :   449 ocorr√™ncias
 15. processos                      :   448 ocorr√™ncias
 16. produ√ß√£o                       :   446 ocorr√™ncias
 17. habilita√ß√£o                    :   444 ocorr√™ncias
 18. gerenciais                     :   444 ocorr√™ncias
 19. l√≠ngua                         :   443 ocorr√™ncias
 20. portuguesa                     :   443 ocorr√™ncias
 21. cep                            :   441 ocorr√™ncias
 22. rua                            :   271 ocorr√™ncias
 23. curso                          :   190 ocorr√™ncias
 24. centro                         :   188 ocorr√™ncias
 25. gest√£o                         :   149 ocorr√™ncias
 26. p√∫blica                        :   148 ocorr√™ncias
 27. oferta                         :   144 ocorr√™ncias
 28. superior                       :   135 ocorr√™ncias
 29. cps                            :   133 ocorr√™ncias
 30. √∫ltima                         :   131 ocorr√™ncias
 31. paulo                          :   129 ocorr√™ncias
 32. avenida                        :    95 ocorr√™ncias
 33. cursos                         :    82 ocorr√™ncias
 34. jardim                         :    81 ocorr√™ncias
 35. universidade                   :    74 ocorr√™ncias
 36. virtual                        :    65 ocorr√™ncias
 37. inclus√£o                       :    64 ocorr√™ncias
 38. vila                           :    64 ocorr√™ncias
 39. estado                         :    62 ocorr√™ncias
 40. aluno                          :    61 ocorr√™ncias
 41. escola                         :    57 ocorr√™ncias
 42. atendimento                    :    57 ocorr√™ncias
 43. paulista                       :    56 ocorr√™ncias
 44. vestibular                     :    53 ocorr√™ncias
 45. uniceu                         :    52 ocorr√™ncias
 46. polos                          :    45 ocorr√™ncias
 47. https                          :    40 ocorr√™ncias
 48. disciplinas                    :    40 ocorr√™ncias
 49. jos√©                           :    40 ocorr√™ncias
 50. gradua√ß√£o                      :    39 ocorr√™ncias
 51. alunos                         :    39 ocorr√™ncias
 52. atividades                     :    35 ocorr√™ncias
 53. acesso                         :    34 ocorr√™ncias
 54. matr√≠cula                      :    34 ocorr√™ncias
 55. sobre                          :    33 ocorr√™ncias
 56. aprendizagem                   :    32 ocorr√™ncias
 57. libras                         :    31 ocorr√™ncias
 58. canais                         :    30 ocorr√™ncias
 59. jo√£o                           :    30 ocorr√™ncias
 60. aulas                          :    29 ocorr√™ncias
 61. sedpcd                         :    28 ocorr√™ncias
 62. inscri√ß√µes                     :    28 ocorr√™ncias
 63. pra√ßa                          :    28 ocorr√™ncias
 64. matriz                         :    28 ocorr√™ncias
 65. est√°gio                        :    28 ocorr√™ncias
 66. castro                         :    27 ocorr√™ncias
 67. nesta                          :    27 ocorr√™ncias
 68. vista                          :    27 ocorr√™ncias
 69. introdu√ß√£o                     :    26 ocorr√™ncias
 70. educa√ß√£o                       :    26 ocorr√™ncias
 71. institucional                  :    25 ocorr√™ncias
 72. nove                           :    25 ocorr√™ncias
 73. quarta                         :    25 ocorr√™ncias
 74. bairro                         :    25 ocorr√™ncias
 75. santa                          :    25 ocorr√™ncias
 76. bela                           :    24 ocorr√™ncias
 77. ambiente                       :    24 ocorr√™ncias
 78. projeto                        :    24 ocorr√™ncias
 79. transpar√™ncia                  :    23 ocorr√™ncias
 80. carreira                       :    23 ocorr√™ncias
 81. leonardo                       :    23 ocorr√™ncias
 82. chefia                         :    23 ocorr√™ncias
 83. gabinete                       :    23 ocorr√™ncias
 84. abre                           :    23 ocorr√™ncias
 85. gravadas                       :    23 ocorr√™ncias
 86. andar                          :    23 ocorr√™ncias
 87. not√≠cias                       :    23 ocorr√™ncias
 88. estudantes                     :    23 ocorr√™ncias
 89. ava                            :    23 ocorr√™ncias
 90. assume                         :    22 ocorr√™ncias
 91. inaugura                       :    22 ocorr√™ncias
 92. presencial                     :    20 ocorr√™ncias
 93. oferece                        :    20 ocorr√™ncias
 94. portaria                       :    20 ocorr√™ncias
 95. prova                          :    20 ocorr√™ncias
 96. novo                           :    19 ocorr√™ncias
 97. possui                         :    19 ocorr√™ncias
 98. partir                         :    19 ocorr√™ncias
 99. novos                          :    19 ocorr√™ncias
100. d√∫vidas                        :    19 ocorr√™ncias


P√ÅGINAS VISITADAS:
--------------------------------------------------
  1. https://univesp.br/institucional/marca
  2. https://univesp.br/institucional/parceiros
  3. https://univesp.br/noticias/escola-da-inclusao-da-sedpcd-abre-inscricoes-para-curso-de-introducao-a-libras-com-aulas-gravadas-nesta-quarta-16
  4. https://univesp.br/noticias/resposta-em-relacao-ao-novo-marco-da-ead
  5. https://univesp.br/institucional
  6. https://univesp.br/
  7. https://univesp.br/carreiras
  8. https://apps.univesp.br/manual-do-aluno/
  9. https://univesp.br/noticias/univesp-e-reconhecida-com-o-selo-o-cliente-recomenda
 10. https://univesp.br
 11. https://intranet.univesp.br/
 12. https://univesp.br/transparencia
 13. https://univesp.br/noticias/univesp-inaugura-polo-na-escola-da-inclusao
 14. https://univesp.br/noticias/aluno-da-univesp-vence-hackathon-de-inteligencia-artificial-da-aws-e-insper
 15. https://univesp.br/noticias/univesp-promove-integracao-entre-novos-docentes-e-supervisores
 16. https://univesp.br/noticias
 17. https://univesp.br/acesso_aluno.html
 18. https://univesp.br/noticias/leonardo-castro-assume-a-chefia-de-gabinete-da-univesp
 19. https://univesp.br/cursos
 20. https://univesp.br/polos
 21. http://apps.univesp.br/repositorio/
 22. https://univesp.br/noticias/polo-mirassol-estacao-crianca-leva-oficinas-ludicas-a-praca-da-matriz-a-partir-de-sabado
 23. https://univesp.br/vestibular
 24. https://univesp.br/noticias/univesp-realiza-aula-magna-com-marcelo-tas
 25. https://atendimento.univesp.br

# PowerShell Deployment Script for Univesp Web Crawler
# Usage: .\Deploy-UnivespCrawler.ps1 -Mode simple|full|docker|status

param(
    [Parameter(Mandatory=$true)]
    [ValidateSet("simple", "full", "docker", "venv", "status", "schedule")]
    [string]$Mode,
    
    [switch]$Force,
    [switch]$Quiet
)

# Set error action preference
$ErrorActionPreference = "Stop"

# Define colors
$Colors = @{
    Success = "Green"
    Error   = "Red"
    Warning = "Yellow"
    Info    = "Cyan"
}

function Write-ColoredOutput {
    param(
        [string]$Message,
        [string]$Color = "White",
        [string]$Emoji = ""
    )
    
    if (-not $Quiet) {
        Write-Host "$Emoji $Message" -ForegroundColor $Color
    }
}

function Test-Requirements {
    Write-ColoredOutput "üîç Checking requirements..." $Colors.Info
    
    # Check Python
    try {
        $pythonVersion = python --version 2>&1
        if ($pythonVersion -match "Python (\d+)\.(\d+)") {
            $major = [int]$matches[1]
            $minor = [int]$matches[2]
            
            if ($major -ge 3 -and $minor -ge 6) {
                Write-ColoredOutput "‚úÖ Python $major.$minor detected" $Colors.Success
            } else {
                Write-ColoredOutput "‚ùå Python 3.6+ required. Found: $pythonVersion" $Colors.Error
                return $false
            }
        }
    }
    catch {
        Write-ColoredOutput "‚ùå Python not found in PATH" $Colors.Error
        return $false
    }
    
    # Check required files
    $requiredFiles = @(
        "univesp_crawler_simple.py",
        "requirements.txt"
    )
    
    foreach ($file in $requiredFiles) {
        if (-not (Test-Path $file)) {
            Write-ColoredOutput "‚ùå Required file missing: $file" $Colors.Error
            return $false
        }
    }
    
    Write-ColoredOutput "‚úÖ All requirements met" $Colors.Success
    return $true
}

function Install-Dependencies {
    Write-ColoredOutput "üì¶ Installing dependencies..." $Colors.Info
    
    try {
        $result = pip install -r requirements.txt 2>&1
        if ($LASTEXITCODE -eq 0) {
            Write-ColoredOutput "‚úÖ Dependencies installed successfully" $Colors.Success
            return $true
        } else {
            Write-ColoredOutput "‚ùå Failed to install dependencies: $result" $Colors.Error
            return $false
        }
    }
    catch {
        Write-ColoredOutput "‚ùå Error installing dependencies: $_" $Colors.Error
        return $false
    }
}

function Deploy-Simple {
    Write-ColoredOutput "üöÄ Deploying simple version..." $Colors.Info
    
    try {
        python univesp_crawler_simple.py
        Write-ColoredOutput "‚úÖ Simple deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "‚ùå Simple deployment failed: $_" $Colors.Error
        return $false
    }
}

function Deploy-Full {
    Write-ColoredOutput "üöÄ Deploying full version..." $Colors.Info
    
    if (-not (Install-Dependencies)) {
        return $false
    }
    
    try {
        python univesp_crawler.py
        Write-ColoredOutput "‚úÖ Full deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "‚ùå Full deployment failed: $_" $Colors.Error
        return $false
    }
}

function Deploy-Docker {
    Write-ColoredOutput "üê≥ Deploying with Docker..." $Colors.Info
    
    # Check if Docker is available
    try {
        docker --version | Out-Null
        if ($LASTEXITCODE -ne 0) {
            throw "Docker not available"
        }
    }
    catch {
        Write-ColoredOutput "‚ùå Docker not found. Please install Docker Desktop" $Colors.Error
        return $false
    }
    
    try {
        # Build Docker image
        Write-ColoredOutput "üî® Building Docker image..." $Colors.Info
        docker build -t univesp-crawler .
        if ($LASTEXITCODE -ne 0) {
            throw "Docker build failed"
        }
        
        # Create results directory if it doesn't exist
        if (-not (Test-Path "results")) {
            New-Item -ItemType Directory -Name "results" | Out-Null
        }
        
        # Run Docker container
        Write-ColoredOutput "üèÉ Running Docker container..." $Colors.Info
        docker run --rm -v "${PWD}\results:/app/results" univesp-crawler
        
        Write-ColoredOutput "‚úÖ Docker deployment completed" $Colors.Success
        return $true
    }
    catch {
        Write-ColoredOutput "‚ùå Docker deployment failed: $_" $Colors.Error
        return $false
    }
}

function New-VirtualEnvironment {
    Write-ColoredOutput "üèóÔ∏è Creating virtual environment..." $Colors.Info
    
    $venvPath = "venv"
    
    try {
        # Create virtual environment
        python -m venv $venvPath
        if ($LASTEXITCODE -ne 0) {
            throw "Failed to create virtual environment"
        }
        
        # Activate virtual environment and install dependencies
        $activateScript = Join-Path $venvPath "Scripts\Activate.ps1"
        
        Write-ColoredOutput "üîß Installing dependencies in virtual environment..." $Colors.Info
        
        # Run pip install in the virtual environment
        & "$venvPath\Scripts\pip.exe" install -r requirements.txt
        if ($LASTEXITCODE -ne 0) {
            throw "Failed to install dependencies in virtual environment"
        }
        
        Write-ColoredOutput "‚úÖ Virtual environment created successfully" $Colors.Success
        Write-ColoredOutput "üí° To activate: $activateScript" $Colors.Info
        Write-ColoredOutput "üí° To run crawler: & venv\Scripts\python.exe univesp_crawler.py" $Colors.Info
        
        return $true
    }
    catch {
        Write-ColoredOutput "‚ùå Failed to create virtual environment: $_" $Colors.Error
        return $false
    }
}

function Show-Status {
    Write-ColoredOutput "" 
    Write-ColoredOutput "=" * 60
    Write-ColoredOutput "üìä UNIVESP CRAWLER DEPLOYMENT STATUS" $Colors.Info
    Write-ColoredOutput "=" * 60
    
    # Check files
    $files = @(
        @{Path = "univesp_crawler_simple.py"; Description = "Simple Version"},
        @{Path = "univesp_crawler.py"; Description = "Full Version"},
        @{Path = "requirements.txt"; Description = "Dependencies"},
        @{Path = "Dockerfile"; Description = "Docker Support"},
        @{Path = "docker-compose.yml"; Description = "Docker Compose"},
        @{Path = "README.md"; Description = "Documentation"}
    )
    
    Write-ColoredOutput ""
    Write-ColoredOutput "üìÅ Files:" $Colors.Info
    foreach ($file in $files) {
        $exists = Test-Path $file.Path
        $status = if ($exists) { "‚úÖ" } else { "‚ùå" }
        Write-Host "   $status $($file.Description.PadRight(20)) ($($file.Path))"
    }
    
    # Check tools
    Write-ColoredOutput ""
    Write-ColoredOutput "üîß Tools:" $Colors.Info
    
    $tools = @("python", "pip", "docker", "docker-compose")
    foreach ($tool in $tools) {
        try {
            & $tool --version | Out-Null
            $available = $LASTEXITCODE -eq 0
        }
        catch {
            $available = $false
        }
        
        $status = if ($available) { "‚úÖ" } else { "‚ùå" }
        Write-Host "   $status $tool"
    }
    
    Write-ColoredOutput ""
    Write-ColoredOutput "üíª System: $([System.Environment]::OSVersion.Platform)" $Colors.Info
    Write-ColoredOutput "üêç Python: $((python --version 2>&1))" $Colors.Info
    Write-ColoredOutput "üìÇ Project: $PWD" $Colors.Info
}

function New-ScheduledTask {
    Write-ColoredOutput "‚è∞ Setting up scheduled task..." $Colors.Info
    
    $scriptPath = Join-Path $PWD "univesp_crawler_simple.py"
    $pythonPath = (Get-Command python).Source
    
    Write-ColoredOutput ""
    Write-ColoredOutput "üí° To create a scheduled task in Windows:" $Colors.Info
    Write-ColoredOutput "   1. Open Task Scheduler" $Colors.Info
    Write-ColoredOutput "   2. Create Basic Task" $Colors.Info
    Write-ColoredOutput "   3. Set your desired schedule" $Colors.Info
    Write-ColoredOutput "   4. Action: Start a program" $Colors.Info
    Write-ColoredOutput "   5. Program/script: $pythonPath" $Colors.Info
    Write-ColoredOutput "   6. Add arguments: $scriptPath" $Colors.Info
    Write-ColoredOutput "   7. Start in: $PWD" $Colors.Info
    Write-ColoredOutput ""
    
    return $true
}

# Main execution
try {
    Write-ColoredOutput "üöÄ UNIVESP WEB CRAWLER DEPLOYMENT" $Colors.Info
    Write-ColoredOutput "=" * 50
    Write-ColoredOutput ""
    
    # Check requirements (except for status mode)
    if ($Mode -ne "status") {
        if (-not (Test-Requirements)) {
            Write-ColoredOutput "‚ùå Requirements check failed" $Colors.Error
            exit 1
        }
    }
    
    # Execute based on mode
    $success = $false
    
    switch ($Mode) {
        "simple" {
            $success = Deploy-Simple
        }
        "full" {
            $success = Deploy-Full
        }
        "docker" {
            $success = Deploy-Docker
        }
        "venv" {
            $success = New-VirtualEnvironment
        }
        "status" {
            Show-Status
            $success = $true
        }
        "schedule" {
            $success = New-ScheduledTask
        }
    }
    
    if ($success) {
        Write-ColoredOutput ""
        Write-ColoredOutput "üéâ Deployment completed successfully!" $Colors.Success
    } else {
        Write-ColoredOutput ""
        Write-ColoredOutput "üí• Deployment failed!" $Colors.Error
        exit 1
    }
}
catch {
    Write-ColoredOutput "‚ùå Unexpected error: $_" $Colors.Error
    exit 1
}

@echo off
echo.
echo ========================================
echo  UNIVESP WEB CRAWLER - QUICK START
echo ========================================
echo.

REM Check if Python is installed
python --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Python is not installed or not in PATH
    echo Please install Python 3.6+ from https://python.org
    pause
    exit /b 1
)

echo ‚úÖ Python detected
echo.

REM Check if the crawler script exists
if not exist "univesp_crawler_simple.py" (
    echo ‚ùå Crawler script not found in current directory
    echo Please ensure univesp_crawler_simple.py is in the same folder
    pause
    exit /b 1
)

echo ‚úÖ Crawler script found
echo.

REM Ask user which version to run
echo Choose deployment option:
echo 1. Simple version (no dependencies)
echo 2. Full version (install dependencies)
echo 3. Create virtual environment
echo 4. Exit
echo.
set /p choice=Enter your choice (1-4): 

if "%choice%"=="1" goto simple
if "%choice%"=="2" goto full
if "%choice%"=="3" goto venv
if "%choice%"=="4" goto end
goto invalid

:simple
echo.
echo üöÄ Running simple version...
echo.
python univesp_crawler_simple.py
goto end

:full
echo.
echo üì¶ Installing dependencies...
pip install -r requirements.txt
if errorlevel 1 (
    echo ‚ùå Failed to install dependencies
    pause
    exit /b 1
)
echo.
echo üöÄ Running full version...
echo.
python univesp_crawler.py
goto end

:venv
echo.
echo üèóÔ∏è Creating virtual environment...
python -m venv venv
if errorlevel 1 (
    echo ‚ùå Failed to create virtual environment
    pause
    exit /b 1
)

echo Activating virtual environment...
call venv\Scripts\activate.bat

echo Installing dependencies...
pip install -r requirements.txt
if errorlevel 1 (
    echo ‚ùå Failed to install dependencies
    pause
    exit /b 1
)

echo.
echo üöÄ Running crawler in virtual environment...
echo.
python univesp_crawler.py
goto end

:invalid
echo.
echo ‚ùå Invalid choice. Please run the script again.
pause
exit /b 1

:end
echo.
echo üìä Check the results in the generated report file!
echo.
pause

#!/usr/bin/env python3
"""
üöÄ One-Click Deployment Script for Univesp Web Crawler
=====================================================

This script automates the deployment process with multiple options.
"""

import os
import sys
import subprocess
import argparse
import platform
from pathlib import Path

class UnivespDeployer:
    """Handles deployment of the Univesp crawler in different environments"""
    
    def __init__(self):
        self.project_dir = Path(__file__).parent
        self.python_executable = sys.executable
        self.system = platform.system()
        
    def check_requirements(self):
        """Check if all requirements are met"""
        print("üîç Checking requirements...")
        
        # Check Python version
        if sys.version_info < (3, 6):
            print("‚ùå Python 3.6 or higher is required")
            return False
            
        print(f"‚úÖ Python {sys.version_info.major}.{sys.version_info.minor} detected")
        
        # Check if required files exist
        required_files = [
            'univesp_crawler_simple.py',
            'requirements.txt'
        ]
        
        for file in required_files:
            if not (self.project_dir / file).exists():
                print(f"‚ùå Required file missing: {file}")
                return False
                
        print("‚úÖ All required files found")
        return True
    
    def install_dependencies(self):
        """Install Python dependencies"""
        print("üì¶ Installing dependencies...")
        
        try:
            subprocess.run([
                self.python_executable, "-m", "pip", "install", 
                "-r", str(self.project_dir / "requirements.txt")
            ], check=True, capture_output=True, text=True)
            print("‚úÖ Dependencies installed successfully")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install dependencies: {e}")
            return False
    
    def create_virtual_environment(self):
        """Create and setup virtual environment"""
        print("üèóÔ∏è Creating virtual environment...")
        
        venv_path = self.project_dir / "venv"
        
        try:
            # Create virtual environment
            subprocess.run([
                self.python_executable, "-m", "venv", str(venv_path)
            ], check=True)
            
            # Determine activation script based on OS
            if self.system == "Windows":
                activate_script = venv_path / "Scripts" / "activate.bat"
                pip_executable = venv_path / "Scripts" / "pip.exe"
            else:
                activate_script = venv_path / "bin" / "activate"
                pip_executable = venv_path / "bin" / "pip"
            
            # Install dependencies in virtual environment
            subprocess.run([
                str(pip_executable), "install", "-r", "requirements.txt"
            ], check=True, cwd=str(self.project_dir))
            
            print(f"‚úÖ Virtual environment created at: {venv_path}")
            print(f"üí° To activate: {activate_script}")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to create virtual environment: {e}")
            return False
    
    def deploy_simple(self):
        """Deploy using the simple version (no external dependencies)"""
        print("üöÄ Deploying simple version...")
        
        try:
            subprocess.run([
                self.python_executable, 
                str(self.project_dir / "univesp_crawler_simple.py")
            ], cwd=str(self.project_dir))
            print("‚úÖ Simple deployment completed")
            return True
        except KeyboardInterrupt:
            print("‚ö†Ô∏è Deployment interrupted by user")
            return True
        except Exception as e:
            print(f"‚ùå Deployment failed: {e}")
            return False
    
    def deploy_full(self):
        """Deploy the full version with dependencies"""
        print("üöÄ Deploying full version...")
        
        if not self.install_dependencies():
            return False
            
        try:
            subprocess.run([
                self.python_executable, 
                str(self.project_dir / "univesp_crawler.py")
            ], cwd=str(self.project_dir))
            print("‚úÖ Full deployment completed")
            return True
        except KeyboardInterrupt:
            print("‚ö†Ô∏è Deployment interrupted by user")
            return True
        except Exception as e:
            print(f"‚ùå Deployment failed: {e}")
            return False
    
    def deploy_docker(self):
        """Deploy using Docker"""
        print("üê≥ Deploying with Docker...")
        
        try:
            # Check if Docker is available
            subprocess.run(["docker", "--version"], 
                          check=True, capture_output=True)
            
            # Build Docker image
            print("üî® Building Docker image...")
            subprocess.run([
                "docker", "build", "-t", "univesp-crawler", "."
            ], check=True, cwd=str(self.project_dir))
            
            # Run Docker container
            print("üèÉ Running Docker container...")
            subprocess.run([
                "docker", "run", "--rm", 
                "-v", f"{self.project_dir}/results:/app/results",
                "univesp-crawler"
            ], check=True)
            
            print("‚úÖ Docker deployment completed")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Docker deployment failed: {e}")
            print("üí° Make sure Docker is installed and running")
            return False
    
    def deploy_docker_compose(self):
        """Deploy using Docker Compose"""
        print("üê≥ Deploying with Docker Compose...")
        
        try:
            # Check if Docker Compose is available
            subprocess.run(["docker-compose", "--version"], 
                          check=True, capture_output=True)
            
            # Run with Docker Compose
            subprocess.run([
                "docker-compose", "up", "--build"
            ], check=True, cwd=str(self.project_dir))
            
            print("‚úÖ Docker Compose deployment completed")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Docker Compose deployment failed: {e}")
            print("üí° Make sure Docker Compose is installed")
            return False
    
    def create_scheduled_task(self):
        """Create a scheduled task for automatic runs"""
        print("‚è∞ Setting up scheduled execution...")
        
        script_path = self.project_dir / "univesp_crawler_simple.py"
        
        if self.system == "Windows":
            print("üí° For Windows Task Scheduler:")
            print(f"   Program: {self.python_executable}")
            print(f"   Arguments: {script_path}")
            print(f"   Start in: {self.project_dir}")
        else:
            print("üí° For Linux/Mac cron job, add this line to crontab:")
            print(f"   0 2 * * * cd {self.project_dir} && {self.python_executable} {script_path}")
        
        return True
    
    def show_status(self):
        """Show deployment status and options"""
        print("\n" + "="*60)
        print("üìä UNIVESP CRAWLER DEPLOYMENT STATUS")
        print("="*60)
        
        # Check files
        files_status = [
            ("univesp_crawler_simple.py", "Simple Version"),
            ("univesp_crawler.py", "Full Version"),
            ("requirements.txt", "Dependencies"),
            ("Dockerfile", "Docker Support"),
            ("docker-compose.yml", "Docker Compose"),
            ("README.md", "Documentation")
        ]
        
        print("\nüìÅ Files:")
        for file, description in files_status:
            exists = (self.project_dir / file).exists()
            status = "‚úÖ" if exists else "‚ùå"
            print(f"   {status} {description:<20} ({file})")
        
        # Check tools
        tools_status = []
        for tool in ["python", "pip", "docker", "docker-compose"]:
            try:
                subprocess.run([tool, "--version"], 
                              capture_output=True, check=True)
                tools_status.append((tool, True))
            except:
                tools_status.append((tool, False))
        
        print("\nüîß Tools:")
        for tool, available in tools_status:
            status = "‚úÖ" if available else "‚ùå"
            print(f"   {status} {tool}")
        
        print(f"\nüíª System: {self.system}")
        print(f"üêç Python: {sys.version}")
        print(f"üìÇ Project: {self.project_dir}")


def main():
    """Main deployment function"""
    parser = argparse.ArgumentParser(
        description="üöÄ Univesp Web Crawler Deployment Tool"
    )
    
    parser.add_argument(
        "mode",
        choices=["simple", "full", "venv", "docker", "compose", "schedule", "status"],
        help="Deployment mode"
    )
    
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="Only check requirements, don't deploy"
    )
    
    args = parser.parse_args()
    
    deployer = UnivespDeployer()
    
    print("üöÄ UNIVESP WEB CRAWLER DEPLOYMENT")
    print("="*50)
    
    # Check requirements first
    if not deployer.check_requirements():
        print("‚ùå Requirements check failed")
        sys.exit(1)
    
    if args.check_only:
        print("‚úÖ Requirements check passed")
        sys.exit(0)
    
    # Execute deployment based on mode
    success = False
    
    if args.mode == "simple":
        success = deployer.deploy_simple()
    elif args.mode == "full":
        success = deployer.deploy_full()
    elif args.mode == "venv":
        success = deployer.create_virtual_environment()
    elif args.mode == "docker":
        success = deployer.deploy_docker()
    elif args.mode == "compose":
        success = deployer.deploy_docker_compose()
    elif args.mode == "schedule":
        success = deployer.create_scheduled_task()
    elif args.mode == "status":
        deployer.show_status()
        success = True
    
    if success:
        print("\nüéâ Deployment completed successfully!")
    else:
        print("\nüí• Deployment failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()

version: '3.8'

services:
  univesp-crawler:
    build: .
    container_name: univesp-crawler
    environment:
      - MAX_PAGES=30
      - DELAY=2.0
    volumes:
      - ./results:/app/results
    restart: unless-stopped
    
  # Optional: Add a web interface service
  # univesp-web:
  #   build: 
  #     context: .
  #     dockerfile: Dockerfile.web
  #   container_name: univesp-web
  #   ports:
  #     - "8000:8000"
  #   depends_on:
  #     - univesp-crawler
  #   restart: unless-stopped

volumes:
  results:

# Use Python 3.9 slim image
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV MAX_PAGES=25
ENV DELAY=1.5

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first (for better Docker layer caching)
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY univesp_crawler.py .
COPY univesp_crawler_simple.py .

# Create results directory
RUN mkdir -p /app/results

# Set the results directory as a volume
VOLUME ["/app/results"]

# Expose port (if you add a web interface later)
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import sys; sys.exit(0)"

# Default command - run the simple version
CMD ["python", "univesp_crawler_simple.py"]

# üöÄ DEPLOYMENT GUIDE - Univesp Web Crawler

This guide shows you how to deploy and run the Univesp web crawler in different environments.

## üìã Quick Start (Local Deployment)

### Option 1: Immediate Local Run (Easiest)
```powershell
# Navigate to the project directory
cd C:\Users\dell

# Run the simple version (no dependencies needed)
python univesp_crawler_simple.py
```

### Option 2: Full Version with Dependencies
```powershell
# Install dependencies
pip install -r requirements.txt

# Run the full version
python univesp_crawler.py
```

## üñ•Ô∏è Local Environment Setup

### Prerequisites Check
```powershell
# Check Python version (needs 3.6+)
python --version

# Check pip
pip --version

# List current packages
pip list
```

### Virtual Environment Setup (Recommended)
```powershell
# Create virtual environment
python -m venv univesp_crawler_env

# Activate virtual environment (Windows)
.\univesp_crawler_env\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Run the crawler
python univesp_crawler.py

# Deactivate when done
deactivate
```

## üê≥ Docker Deployment

### Create Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Copy requirements first (for better caching)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY univesp_crawler.py .
COPY univesp_crawler_simple.py .

# Create results directory
RUN mkdir -p /app/results

# Run the simple version by default
CMD ["python", "univesp_crawler_simple.py"]
```

### Docker Commands
```bash
# Build Docker image
docker build -t univesp-crawler .

# Run container
docker run -v $(pwd)/results:/app/results univesp-crawler

# Run with custom settings
docker run -e MAX_PAGES=50 -v $(pwd)/results:/app/results univesp-crawler
```

## ‚òÅÔ∏è Cloud Deployment Options

### 1. Heroku Deployment

#### Create required files:

**Procfile:**
```
worker: python univesp_crawler_simple.py
```

**runtime.txt:**
```
python-3.9.16
```

#### Deploy to Heroku:
```bash
# Install Heroku CLI first
# Login to Heroku
heroku login

# Create Heroku app
heroku create your-univesp-crawler

# Push to Heroku
git init
git add .
git commit -m "Initial commit"
git push heroku main

# Run the worker
heroku ps:scale worker=1
```

### 2. AWS EC2 Deployment

```bash
# Connect to EC2 instance
ssh -i your-key.pem ec2-user@your-ec2-ip

# Install Python and dependencies
sudo yum update -y
sudo yum install python3 python3-pip -y

# Clone or upload your code
# Upload files using scp or git clone

# Install dependencies
pip3 install -r requirements.txt

# Run the crawler
python3 univesp_crawler_simple.py

# Run in background
nohup python3 univesp_crawler_simple.py > output.log 2>&1 &
```

### 3. Google Cloud Run

#### Create cloudbuild.yaml:
```yaml
steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/univesp-crawler', '.']
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/univesp-crawler']
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['run', 'deploy', 'univesp-crawler', '--image', 'gcr.io/$PROJECT_ID/univesp-crawler', '--platform', 'managed', '--region', 'us-central1']
```

#### Deploy:
```bash
gcloud builds submit --config cloudbuild.yaml
```

## üìä Scheduled Deployment (Automation)

### Windows Task Scheduler
1. Open Task Scheduler
2. Create Basic Task
3. Set trigger (daily/weekly)
4. Action: Start a program
5. Program: `python`
6. Arguments: `C:\Users\dell\univesp_crawler_simple.py`
7. Start in: `C:\Users\dell`

### Linux Cron Job
```bash
# Edit crontab
crontab -e

# Run daily at 2 AM
0 2 * * * cd /path/to/crawler && python3 univesp_crawler_simple.py

# Run weekly on Sunday at 1 AM
0 1 * * 0 cd /path/to/crawler && python3 univesp_crawler_simple.py
```

### GitHub Actions (CI/CD)

#### Create .github/workflows/crawler.yml:
```yaml
name: Run Univesp Crawler

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:  # Manual trigger

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run crawler
      run: python univesp_crawler_simple.py
      
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: crawler-results
        path: univesp_crawler_resultados.txt
```

## üîß Configuration for Production

### Environment Variables Setup
```powershell
# Set environment variables (Windows)
$env:MAX_PAGES = "50"
$env:DELAY = "2.0"
$env:OUTPUT_DIR = "C:\crawler_results"

# Run with environment variables
python univesp_crawler.py
```

### Configuration File (config.py)
```python
import os

# Configuration settings
MAX_PAGES = int(os.getenv('MAX_PAGES', 25))
DELAY = float(os.getenv('DELAY', 1.5))
OUTPUT_DIR = os.getenv('OUTPUT_DIR', '.')
BASE_URL = os.getenv('BASE_URL', 'https://univesp.br')
```

## üêç Advanced Deployment with Python Scripts

### Batch Deployment Script (deploy.py)
```python
import subprocess
import sys
import os

def deploy_crawler():
    """Deploy the Univesp crawler with all dependencies"""
    
    print("üöÄ Starting Univesp Crawler Deployment...")
    
    # Check Python version
    if sys.version_info < (3, 6):
        print("‚ùå Python 3.6+ required")
        return
    
    # Install dependencies
    print("üì¶ Installing dependencies...")
    subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])
    
    # Run the crawler
    print("üï∑Ô∏è Starting crawler...")
    subprocess.run([sys.executable, "univesp_crawler.py"])
    
    print("‚úÖ Deployment completed!")

if __name__ == "__main__":
    deploy_crawler()
```

### Usage:
```powershell
python deploy.py
```

## üì± Monitoring and Alerts

### Simple Monitoring Script
```python
import logging
import smtplib
from email.mime.text import MIMEText
from datetime import datetime

def setup_monitoring():
    """Setup basic monitoring and alerts"""
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('crawler.log'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def send_alert(message):
    """Send email alert when crawler completes or fails"""
    # Configure your email settings here
    pass

# Usage in your crawler
logger = setup_monitoring()
logger.info("Crawler started")
```

## üõ°Ô∏è Security Considerations

### Production Security Settings
```python
# Add to your crawler configuration
SECURITY_SETTINGS = {
    'USER_AGENT': 'UnivespCrawler/1.0 (Educational Purpose)',
    'RESPECT_ROBOTS_TXT': True,
    'MAX_CONCURRENT_REQUESTS': 1,
    'DOWNLOAD_DELAY': 2,
    'RANDOMIZE_DOWNLOAD_DELAY': True,
}
```

## üéØ Deployment Recommendations

### For Development:
```powershell
python univesp_crawler_simple.py
```

### For Production:
1. Use Docker containers
2. Set up monitoring
3. Use environment variables
4. Schedule regular runs
5. Implement error handling

### For Scale:
1. Use cloud services (AWS, GCP, Azure)
2. Implement distributed crawling
3. Add database storage
4. Set up API endpoints

---

## üöÄ Quick Deploy Commands

### Immediate Local Run:
```powershell
cd C:\Users\dell
python univesp_crawler_simple.py
```

### With Virtual Environment:
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python univesp_crawler.py
```

### Docker Run:
```powershell
docker build -t univesp-crawler .
docker run univesp-crawler
```

Choose the deployment method that best fits your needs! üéØ

# Web Crawler da Univesp üï∑Ô∏è

Este projeto implementa um web crawler b√°sico que visita sistematicamente as p√°ginas Web da Univesp, seguindo hyperlinks da p√°gina principal e contabilizando a frequ√™ncia de cada palavra encontrada nas p√°ginas visitadas.

## üìã Funcionalidades

- ‚úÖ Crawling sistem√°tico do site da Univesp
- ‚úÖ Seguimento de hyperlinks internos
- ‚úÖ Extra√ß√£o e limpeza de texto HTML
- ‚úÖ Contagem de frequ√™ncia de palavras
- ‚úÖ Filtro de stopwords em portugu√™s
- ‚úÖ Relat√≥rios detalhados de an√°lise
- ‚úÖ Salvamento de resultados em arquivo
- ‚úÖ Interface com progresso em tempo real

## üìÅ Arquivos

### Vers√µes Dispon√≠veis:

1. **`univesp_crawler.py`** - Vers√£o completa com bibliotecas externas (requests, BeautifulSoup)
2. **`univesp_crawler_simple.py`** - Vers√£o simplificada usando apenas bibliotecas padr√£o do Python
3. **`Desafio_Semana_4.ipynb`** - Notebook Jupyter original com c√≥digo base

### Arquivos Auxiliares:

- **`requirements.txt`** - Depend√™ncias para a vers√£o completa
- **`README.md`** - Este arquivo com instru√ß√µes

## üöÄ Como Executar

### Op√ß√£o 1: Vers√£o Simplificada (Recomendada)

Esta vers√£o usa apenas bibliotecas padr√£o do Python, n√£o requer instala√ß√£o de depend√™ncias:

```bash
python univesp_crawler_simple.py
```

### Op√ß√£o 2: Vers√£o Completa

1. Instale as depend√™ncias:
```bash
pip install -r requirements.txt
```

2. Execute o crawler:
```bash
python univesp_crawler.py
```

## ‚öôÔ∏è Configura√ß√µes

Voc√™ pode ajustar as configura√ß√µes do crawler editando as vari√°veis na fun√ß√£o `main()`:

```python
MAX_PAGES = 25  # N√∫mero m√°ximo de p√°ginas a visitar
DELAY = 1.5     # Delay entre requisi√ß√µes (segundos)
```

## üìä Resultados

O crawler gera:

1. **Relat√≥rio no terminal** com:
   - Estat√≠sticas gerais
   - Top 25 palavras mais frequentes
   - An√°lise por p√°gina (top 5)
   - URLs com erro

2. **Arquivo de resultados** (`univesp_crawler_resultados.txt`) com:
   - Data e hora da execu√ß√£o
   - Top 100 palavras mais frequentes
   - Lista completa de p√°ginas visitadas
   - URLs com erro (se houver)

## üîß Caracter√≠sticas T√©cnicas

### Processamento de Texto:
- Remove tags HTML, scripts e estilos
- Decodifica entidades HTML
- Normaliza texto para an√°lise
- Filtra palavras com menos de 3 caracteres
- Remove stopwords comuns em portugu√™s

### Controle de Crawling:
- Respeita apenas URLs do dom√≠nio `univesp.br`
- Ignora arquivos n√£o-HTML (PDF, imagens, etc.)
- Implementa delay entre requisi√ß√µes
- Controle de p√°ginas visitadas
- Tratamento de erros robusto

### An√°lise de Dados:
- Contagem global de frequ√™ncia de palavras
- Contagem individual por p√°gina
- Estat√≠sticas detalhadas
- Exporta√ß√£o de resultados

## ‚ö†Ô∏è Considera√ß√µes Importantes

1. **Uso √âtico**: O crawler implementa delays entre requisi√ß√µes para n√£o sobrecarregar o servidor
2. **Limita√ß√µes**: Por padr√£o, visita at√© 25 p√°ginas para demonstra√ß√£o
3. **Conectividade**: Requer conex√£o com internet ativa
4. **Tempo**: Dependendo da configura√ß√£o, pode levar alguns minutos para concluir

## üõë Interrompendo a Execu√ß√£o

Para parar o crawler durante a execu√ß√£o:
- Pressione `Ctrl+C` (Windows/Linux) ou `Cmd+C` (Mac)
- O programa ir√° gerar um relat√≥rio parcial com os dados coletados at√© ent√£o

## üìà Exemplo de Sa√≠da

```
üï∑Ô∏è  INICIANDO WEB CRAWLER DA UNIVESP
==================================================
URL inicial: https://univesp.br
M√°ximo de p√°ginas: 25
Delay entre requisi√ß√µes: 1.5s

Visitando: https://univesp.br
Visitando: https://univesp.br/cursos
Progresso: 5/25 p√°ginas visitadas
...

================================================================================
RELAT√ìRIO DO WEB CRAWLER DA UNIVESP
================================================================================

Estat√≠sticas Gerais:
  ‚Ä¢ P√°ginas visitadas: 25
  ‚Ä¢ P√°ginas com erro: 0
  ‚Ä¢ Total de palavras √∫nicas: 1247
  ‚Ä¢ Total de ocorr√™ncias: 8934

üìä TOP 25 PALAVRAS MAIS FREQUENTES:
------------------------------------------------------------
 1. univesp               : 156 ocorr√™ncias
 2. curso                 : 89 ocorr√™ncias
 3. ensino                : 67 ocorr√™ncias
...
```

## üÜò Solu√ß√£o de Problemas

### Erro de conex√£o:
- Verifique sua conex√£o com a internet
- O site da Univesp pode estar temporariamente indispon√≠vel

### Erro de importa√ß√£o (vers√£o completa):
- Execute: `pip install -r requirements.txt`
- Use a vers√£o simplificada como alternativa

### Permiss√£o de escrita:
- Verifique se voc√™ tem permiss√£o para escrever na pasta atual
- Os resultados s√£o salvos em `C:\Users\dell\`

---

**Desenvolvido para o Desafio Semana 4 - An√°lise de Frequ√™ncia de Palavras em Web Crawling**

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Crawler da Univesp - Vers√£o Simplificada
===========================================

Vers√£o que usa apenas bibliotecas padr√£o do Python.
Este programa desenvolve um web crawler b√°sico que visita sistematicamente 
as p√°ginas da Univesp, seguindo hyperlinks da p√°gina principal. Para cada 
p√°gina visitada, o programa contabiliza a frequ√™ncia de cada palavra.

Autor: Desenvolvido para o Desafio Semana 4
Data: 2025
"""

import re
import time
import html
from urllib.parse import urljoin, urlparse
from html.parser import HTMLParser
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from collections import Counter, defaultdict


class TextExtractor(HTMLParser):
    """
    Extrator de texto de HTML usando HTMLParser
    """
    
    def __init__(self):
        super().__init__()
        self.text_data = []
        self.skip_tags = {'script', 'style', 'nav', 'footer', 'header'}
        self.current_tag = None
    
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag.lower()
    
    def handle_endtag(self, tag):
        self.current_tag = None
    
    def handle_data(self, data):
        if self.current_tag not in self.skip_tags:
            # Decodifica entidades HTML e limpa o texto
            cleaned_data = html.unescape(data.strip())
            if cleaned_data:
                self.text_data.append(cleaned_data)
    
    def get_text(self):
        return ' '.join(self.text_data)


class LinkCollector(HTMLParser):
    """
    Coletor de hyperlinks usando HTMLParser
    """
    
    def __init__(self, base_url):
        super().__init__()
        self.base_url = base_url
        self.links = []
    
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr_name, attr_value in attrs:
                if attr_name == 'href' and attr_value:
                    # Constr√≥i URL absoluto
                    absolute_url = urljoin(self.base_url, attr_value)
                    if self._is_valid_url(absolute_url):
                        self.links.append(absolute_url)
    
    def _is_valid_url(self, url):
        """Verifica se a URL √© v√°lida para crawling"""
        try:
            parsed = urlparse(url)
            return (parsed.netloc.endswith('univesp.br') and 
                   parsed.scheme in ['http', 'https'] and
                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc', '.docx']))
        except:
            return False
    
    def get_links(self):
        return list(set(self.links))  # Remove duplicatas


class UnivespCrawlerSimple:
    """
    Classe principal do crawler da Univesp - Vers√£o Simplificada
    """
    
    def __init__(self, base_url="https://univesp.br", max_pages=20, delay=1):
        """
        Inicializa o crawler
        
        Args:
            base_url (str): URL base da Univesp
            max_pages (int): N√∫mero m√°ximo de p√°ginas a visitar
            delay (float): Delay entre requisi√ß√µes em segundos
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls = set()
        self.word_frequency = Counter()
        self.page_word_counts = defaultdict(Counter)
        self.failed_urls = set()
        
        # Palavras comuns portuguesas (stopwords) para filtrar
        self.stopwords = {
            'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'at√©', 
            'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 
            'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', 
            'esse', 'esses', 'esta', 'est√£o', 'estas', 'estamos', 'estar', 'este', 'estes', 
            'eu', 'foi', 'for', 'foram', 'h√°', 'isso', 'isto', 'j√°', 'mais', 'mas', 'me', 
            'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'n√£o', 'no', 'nos', 
            'n√≥s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 
            'qual', 'quando', 'que', 'quem', 's√£o', 'se', 'sem', 'ser', 'seu', 'seus', 
            's√≥', 'sua', 'suas', 'tamb√©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', 
            'tuas', 'um', 'uma', 'voc√™', 'voc√™s', 'vos', 'ver', 'anos', 'ano', 'dia', 'dias',
            'pode', 'podem', 'vai', 'v√£o', 'fazer', 'feito', 'bem', 'muito', 'toda', 'todo',
            'todos', 'todas', 'vez', 'vezes', 'onde', 'aqui', 'ali', 'l√°', 'agora', 'ent√£o'
        }
    
    def fetch_page(self, url):
        """
        Faz o download do conte√∫do de uma p√°gina
        
        Args:
            url (str): URL da p√°gina
            
        Returns:
            str: Conte√∫do HTML da p√°gina ou None se houver erro
        """
        try:
            print(f"Visitando: {url}")
            
            # Configura headers para simular navegador
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            req = Request(url, headers=headers)
            response = urlopen(req, timeout=10)
            
            # L√™ o conte√∫do
            content = response.read()
            
            # Tenta decodificar usando diferentes encodings
            try:
                return content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    return content.decode('latin-1')
                except UnicodeDecodeError:
                    return content.decode('utf-8', errors='ignore')
            
        except Exception as e:
            print(f"Erro ao acessar {url}: {e}")
            self.failed_urls.add(url)
            return None
    
    def extract_text(self, html_content):
        """
        Extrai texto limpo do HTML
        
        Args:
            html_content (str): Conte√∫do HTML
            
        Returns:
            str: Texto extra√≠do
        """
        try:
            extractor = TextExtractor()
            extractor.feed(html_content)
            return extractor.get_text()
        except Exception as e:
            print(f"Erro ao extrair texto: {e}")
            return ""
    
    def extract_links(self, html_content, base_url):
        """
        Extrai links do HTML
        
        Args:
            html_content (str): Conte√∫do HTML
            base_url (str): URL base
            
        Returns:
            list: Lista de URLs encontradas
        """
        try:
            collector = LinkCollector(base_url)
            collector.feed(html_content)
            return collector.get_links()
        except Exception as e:
            print(f"Erro ao extrair links: {e}")
            return []
    
    def process_text(self, text):
        """
        Processa texto para contar palavras
        
        Args:
            text (str): Texto a ser processado
            
        Returns:
            Counter: Contador de frequ√™ncia de palavras
        """
        # Converte para min√∫sculas
        text = text.lower()
        
        # Encontra todas as palavras (incluindo acentos)
        words = re.findall(r'\b[a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±]+\b', text)
        
        # Filtra palavras muito curtas e stopwords
        filtered_words = [word for word in words 
                         if len(word) > 2 and word not in self.stopwords]
        
        return Counter(filtered_words)
    
    def crawl_page(self, url):
        """
        Processa uma p√°gina espec√≠fica
        
        Args:
            url (str): URL da p√°gina
            
        Returns:
            list: Lista de novos links encontrados
        """
        # Faz download da p√°gina
        html_content = self.fetch_page(url)
        if not html_content:
            return []
        
        # Extrai e processa texto
        text = self.extract_text(html_content)
        if text:
            word_count = self.process_text(text)
            self.page_word_counts[url] = word_count
            self.word_frequency.update(word_count)
        
        # Extrai links
        links = self.extract_links(html_content, url)
        return links
    
    def crawl(self):
        """
        Executa o processo principal de crawling
        """
        print("üï∑Ô∏è  INICIANDO WEB CRAWLER DA UNIVESP")
        print("="*50)
        print(f"URL inicial: {self.base_url}")
        print(f"M√°ximo de p√°ginas: {self.max_pages}")
        print(f"Delay entre requisi√ß√µes: {self.delay}s")
        print("")
        
        urls_to_visit = [self.base_url]
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            current_url = urls_to_visit.pop(0)
            
            # Evita URLs j√° visitadas
            if current_url in self.visited_urls:
                continue
            
            self.visited_urls.add(current_url)
            
            # Processa a p√°gina atual
            new_links = self.crawl_page(current_url)
            
            # Adiciona novos links √† fila
            for link in new_links:
                if (link not in self.visited_urls and 
                    link not in urls_to_visit and 
                    link not in self.failed_urls):
                    urls_to_visit.append(link)
            
            # Status do progresso
            if len(self.visited_urls) % 5 == 0:
                print(f"Progresso: {len(self.visited_urls)}/{self.max_pages} p√°ginas visitadas")
            
            # Delay entre requisi√ß√µes
            time.sleep(self.delay)
        
        print(f"\nCrawling conclu√≠do!")
        print(f"P√°ginas visitadas: {len(self.visited_urls)}")
        print(f"P√°ginas com erro: {len(self.failed_urls)}")
    
    def generate_report(self):
        """
        Gera e exibe relat√≥rio completo
        """
        print("\n" + "="*80)
        print("RELAT√ìRIO DO WEB CRAWLER DA UNIVESP")
        print("="*80)
        
        print(f"\nEstat√≠sticas Gerais:")
        print(f"  ‚Ä¢ P√°ginas visitadas: {len(self.visited_urls)}")
        print(f"  ‚Ä¢ P√°ginas com erro: {len(self.failed_urls)}")
        print(f"  ‚Ä¢ Total de palavras √∫nicas: {len(self.word_frequency)}")
        print(f"  ‚Ä¢ Total de ocorr√™ncias: {sum(self.word_frequency.values())}")
        
        if self.word_frequency:
            print(f"\nüìä TOP 25 PALAVRAS MAIS FREQUENTES:")
            print("-" * 60)
            for i, (word, count) in enumerate(self.word_frequency.most_common(25), 1):
                print(f"{i:2d}. {word:<25} : {count:>4} ocorr√™ncias")
            
            print(f"\nüìà AN√ÅLISE POR P√ÅGINA (Top 5):")
            print("-" * 60)
            for i, url in enumerate(list(self.page_word_counts.keys())[:5], 1):
                word_count = self.page_word_counts[url]
                total_words = sum(word_count.values())
                top_word = word_count.most_common(1)[0] if word_count else ("N/A", 0)
                print(f"{i}. {url}")
                print(f"   ‚Ä¢ Total de palavras: {total_words}")
                print(f"   ‚Ä¢ Palavra mais frequente: '{top_word[0]}' ({top_word[1]} vezes)")
                print()
        
        if self.failed_urls:
            print(f"‚ùå URLs COM ERRO:")
            print("-" * 60)
            for i, url in enumerate(list(self.failed_urls)[:10], 1):
                print(f"{i:2d}. {url}")
    
    def save_results(self, filename="univesp_crawler_resultados.txt"):
        """
        Salva os resultados em arquivo
        
        Args:
            filename (str): Nome do arquivo para salvar
        """
        try:
            filepath = f"C:\\Users\\dell\\{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("RESULTADOS DO WEB CRAWLER DA UNIVESP\n")
                f.write("="*60 + "\n\n")
                
                f.write(f"Data da execu√ß√£o: {time.strftime('%d/%m/%Y %H:%M:%S')}\n")
                f.write(f"P√°ginas visitadas: {len(self.visited_urls)}\n")
                f.write(f"P√°ginas com erro: {len(self.failed_urls)}\n")
                f.write(f"Total de palavras √∫nicas: {len(self.word_frequency)}\n")
                f.write(f"Total de ocorr√™ncias: {sum(self.word_frequency.values())}\n\n")
                
                f.write("TOP 100 PALAVRAS MAIS FREQUENTES:\n")
                f.write("-" * 50 + "\n")
                for i, (word, count) in enumerate(self.word_frequency.most_common(100), 1):
                    f.write(f"{i:3d}. {word:<30} : {count:>5} ocorr√™ncias\n")
                
                f.write(f"\n\nP√ÅGINAS VISITADAS:\n")
                f.write("-" * 50 + "\n")
                for i, url in enumerate(self.visited_urls, 1):
                    f.write(f"{i:3d}. {url}\n")
                
                if self.failed_urls:
                    f.write(f"\n\nP√ÅGINAS COM ERRO:\n")
                    f.write("-" * 50 + "\n")
                    for i, url in enumerate(self.failed_urls, 1):
                        f.write(f"{i:3d}. {url}\n")
            
            print(f"\nüíæ Resultados salvos em: {filepath}")
        except Exception as e:
            print(f"Erro ao salvar resultados: {e}")


def main():
    """
    Fun√ß√£o principal do programa
    """
    # Configura√ß√µes do crawler
    MAX_PAGES = 25  # Limite de p√°ginas
    DELAY = 1.5     # Delay entre requisi√ß√µes
    
    # Cria o crawler
    crawler = UnivespCrawlerSimple(max_pages=MAX_PAGES, delay=DELAY)
    
    try:
        # Executa o crawling
        start_time = time.time()
        crawler.crawl()
        end_time = time.time()
        
        # Gera relat√≥rio
        crawler.generate_report()
        
        # Salva resultados
        crawler.save_results()
        
        print(f"\n‚è±Ô∏è  Tempo total de execu√ß√£o: {end_time - start_time:.2f} segundos")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Processo interrompido pelo usu√°rio")
        if len(crawler.visited_urls) > 0:
            print("Gerando relat√≥rio parcial...")
            crawler.generate_report()
            crawler.save_results("univesp_crawler_resultados_parciais.txt")
    
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
        if len(crawler.visited_urls) > 0:
            crawler.generate_report()


if __name__ == "__main__":
    main()

requests==2.31.0
beautifulsoup4==4.12.2
urllib3==2.0.4

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Crawler da Univesp com An√°lise de Frequ√™ncia de Palavras
===========================================================

Este programa desenvolve um web crawler b√°sico que visita sistematicamente 
as p√°ginas da Univesp, seguindo hyperlinks da p√°gina principal. Para cada 
p√°gina visitada, o programa contabiliza a frequ√™ncia de cada palavra.

Autor: Desenvolvido para o Desafio Semana 4
Data: 2025
"""

import re
import time
from urllib.parse import urljoin, urlparse
from html.parser import HTMLParser
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from collections import Counter, defaultdict
from bs4 import BeautifulSoup
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UnivespCrawler:
    """
    Classe principal do crawler da Univesp que coleta links e analisa texto
    """
    
    def __init__(self, base_url="https://univesp.br", max_pages=50, delay=1):
        """
        Inicializa o crawler
        
        Args:
            base_url (str): URL base da Univesp
            max_pages (int): N√∫mero m√°ximo de p√°ginas a visitar
            delay (float): Delay entre requisi√ß√µes em segundos
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls = set()
        self.word_frequency = Counter()
        self.page_word_counts = defaultdict(Counter)
        self.failed_urls = set()
        
        # Configura√ß√£o da sess√£o HTTP com retry
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers para simular um navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # Palavras comuns portuguesas (stopwords) para filtrar
        self.stopwords = {
            'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'at√©', 
            'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 
            'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'essa', 'essas', 
            'esse', 'esses', 'esta', 'est√£o', 'estas', 'estamos', 'estar', 'este', 'estes', 
            'eu', 'foi', 'for', 'foram', 'h√°', 'isso', 'isto', 'j√°', 'mais', 'mas', 'me', 
            'mesmo', 'meu', 'meus', 'minha', 'minhas', 'na', 'nas', 'n√£o', 'no', 'nos', 
            'n√≥s', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 
            'qual', 'quando', 'que', 'quem', 's√£o', 'se', 'sem', 'ser', 'seu', 'seus', 
            's√≥', 'sua', 'suas', 'tamb√©m', 'te', 'tem', 'teu', 'teus', 'tu', 'tua', 
            'tuas', 'um', 'uma', 'voc√™', 'voc√™s', 'vos'
        }
    
    def is_valid_url(self, url):
        """
        Verifica se a URL √© v√°lida e pertence ao dom√≠nio da Univesp
        
        Args:
            url (str): URL a ser verificada
            
        Returns:
            bool: True se a URL √© v√°lida
        """
        try:
            parsed = urlparse(url)
            return (parsed.netloc.endswith('univesp.br') and 
                   parsed.scheme in ['http', 'https'] and
                   not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.doc']))
        except:
            return False
    
    def extract_text_from_html(self, html_content):
        """
        Extrai texto limpo do HTML
        
        Args:
            html_content (str): Conte√∫do HTML da p√°gina
            
        Returns:
            str: Texto limpo extra√≠do
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove scripts e estilos
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # Extrai texto
            text = soup.get_text()
            
            # Limpa o texto
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
        except Exception as e:
            logger.error(f"Erro ao extrair texto do HTML: {e}")
            return ""
    
    def extract_links(self, html_content, base_url):
        """
        Extrai todos os links de uma p√°gina HTML
        
        Args:
            html_content (str): Conte√∫do HTML da p√°gina
            base_url (str): URL base para resolver links relativos
            
        Returns:
            list: Lista de URLs absolutas encontradas
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(base_url, href)
                
                if self.is_valid_url(absolute_url):
                    links.append(absolute_url)
            
            return links
        except Exception as e:
            logger.error(f"Erro ao extrair links: {e}")
            return []
    
    def process_text(self, text):
        """
        Processa o texto extra√≠do para contar palavras
        
        Args:
            text (str): Texto a ser processado
            
        Returns:
            Counter: Contador de frequ√™ncia de palavras
        """
        # Converte para min√∫sculas e remove pontua√ß√£o
        text = text.lower()
        words = re.findall(r'\b[a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß]+\b', text)
        
        # Filtra palavras muito curtas e stopwords
        filtered_words = [word for word in words 
                         if len(word) > 2 and word not in self.stopwords]
        
        return Counter(filtered_words)
    
    def crawl_page(self, url):
        """
        Faz o crawl de uma p√°gina espec√≠fica
        
        Args:
            url (str): URL da p√°gina a ser processada
            
        Returns:
            list: Lista de links encontrados na p√°gina
        """
        try:
            logger.info(f"Visitando: {url}")
            
            response = self.session.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # Extrai texto e conta palavras
            text = self.extract_text_from_html(response.text)
            if text:
                word_count = self.process_text(text)
                self.page_word_counts[url] = word_count
                self.word_frequency.update(word_count)
            
            # Extrai links
            links = self.extract_links(response.text, url)
            
            return links
            
        except Exception as e:
            logger.error(f"Erro ao processar {url}: {e}")
            self.failed_urls.add(url)
            return []
    
    def crawl(self):
        """
        Executa o processo principal de crawling
        """
        logger.info(f"Iniciando crawler da Univesp - M√°ximo {self.max_pages} p√°ginas")
        
        urls_to_visit = [self.base_url]
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
            
            self.visited_urls.add(current_url)
            
            # Processa a p√°gina atual
            new_links = self.crawl_page(current_url)
            
            # Adiciona novos links √† lista de URLs a visitar
            for link in new_links:
                if (link not in self.visited_urls and 
                    link not in urls_to_visit and 
                    link not in self.failed_urls):
                    urls_to_visit.append(link)
            
            # Delay entre requisi√ß√µes
            time.sleep(self.delay)
        
        logger.info(f"Crawling conclu√≠do. Visitadas {len(self.visited_urls)} p√°ginas")
    
    def generate_report(self):
        """
        Gera relat√≥rio com as estat√≠sticas coletadas
        """
        print("\n" + "="*80)
        print("RELAT√ìRIO DO WEB CRAWLER DA UNIVESP")
        print("="*80)
        
        print(f"\nEstat√≠sticas Gerais:")
        print(f"  ‚Ä¢ P√°ginas visitadas: {len(self.visited_urls)}")
        print(f"  ‚Ä¢ P√°ginas com erro: {len(self.failed_urls)}")
        print(f"  ‚Ä¢ Total de palavras √∫nicas: {len(self.word_frequency)}")
        print(f"  ‚Ä¢ Total de ocorr√™ncias: {sum(self.word_frequency.values())}")
        
        print(f"\nüìä TOP 20 PALAVRAS MAIS FREQUENTES:")
        print("-" * 50)
        for i, (word, count) in enumerate(self.word_frequency.most_common(20), 1):
            print(f"{i:2d}. {word:<20} : {count:>4} ocorr√™ncias")
        
        print(f"\nüìà AN√ÅLISE POR P√ÅGINA (Top 5):")
        print("-" * 50)
        for url in list(self.page_word_counts.keys())[:5]:
            word_count = self.page_word_counts[url]
            total_words = sum(word_count.values())
            top_word = word_count.most_common(1)[0] if word_count else ("N/A", 0)
            print(f"URL: {url}")
            print(f"  ‚Ä¢ Total de palavras: {total_words}")
            print(f"  ‚Ä¢ Palavra mais frequente: '{top_word[0]}' ({top_word[1]} vezes)")
            print()
        
        if self.failed_urls:
            print(f"\n‚ùå P√ÅGINAS COM ERRO:")
            print("-" * 50)
            for url in list(self.failed_urls)[:10]:  # Mostra apenas as primeiras 10
                print(f"  ‚Ä¢ {url}")
    
    def save_results(self, filename="univesp_crawler_results.txt"):
        """
        Salva os resultados em um arquivo de texto
        
        Args:
            filename (str): Nome do arquivo para salvar
        """
        try:
            filepath = f"C:\\Users\\dell\\{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("RESULTADOS DO WEB CRAWLER DA UNIVESP\n")
                f.write("="*50 + "\n\n")
                
                f.write(f"P√°ginas visitadas: {len(self.visited_urls)}\n")
                f.write(f"Total de palavras √∫nicas: {len(self.word_frequency)}\n")
                f.write(f"Total de ocorr√™ncias: {sum(self.word_frequency.values())}\n\n")
                
                f.write("TOP 50 PALAVRAS MAIS FREQUENTES:\n")
                f.write("-" * 40 + "\n")
                for i, (word, count) in enumerate(self.word_frequency.most_common(50), 1):
                    f.write(f"{i:2d}. {word:<25} : {count:>5} ocorr√™ncias\n")
                
                f.write(f"\n\nP√ÅGINAS VISITADAS:\n")
                f.write("-" * 40 + "\n")
                for i, url in enumerate(self.visited_urls, 1):
                    f.write(f"{i:2d}. {url}\n")
            
            logger.info(f"Resultados salvos em: {filepath}")
        except Exception as e:
            logger.error(f"Erro ao salvar resultados: {e}")


def main():
    """
    Fun√ß√£o principal do programa
    """
    print("üï∑Ô∏è  INICIANDO WEB CRAWLER DA UNIVESP")
    print("="*50)
    
    # Configura√ß√µes do crawler
    MAX_PAGES = 30  # Limite de p√°ginas para evitar sobrecarga
    DELAY = 1       # Delay de 1 segundo entre requisi√ß√µes
    
    # Cria e executa o crawler
    crawler = UnivespCrawler(max_pages=MAX_PAGES, delay=DELAY)
    
    try:
        # Executa o crawling
        crawler.crawl()
        
        # Gera relat√≥rio
        crawler.generate_report()
        
        # Salva resultados
        crawler.save_results()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Processo interrompido pelo usu√°rio")
        if len(crawler.visited_urls) > 0:
            print("Gerando relat√≥rio com dados coletados at√© agora...")
            crawler.generate_report()
            crawler.save_results("univesp_crawler_partial_results.txt")
    
    except Exception as e:
        logger.error(f"Erro durante execu√ß√£o: {e}")


if __name__ == "__main__":
    main()
